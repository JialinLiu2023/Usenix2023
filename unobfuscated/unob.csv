fumitoh-modelx-blob-0180da34d052c44fb94dab9e115e218bbebfc9c3-devnotes-cpydep.py,thebigmunch-gmusicapi-wrapper-blob-8708683cd33955def1378fc28319ef37805b851d-gmusicapi_wrapper-musicmanager.py,koalalorenzo-python-digitalocean-blob-d0221b57856fb1e131cafecf99d826f7b07a947c-digitalocean-Account.py,multiformats-py-multicodec-blob-23213b8b40b21e17e2e1844224498cbd8e359bfa-multicodec-multicodec.py,nephics-mat4py-blob-6c1a2ad903937437cc5f24f3c3f5aa2c5a77a1c1-mat4py-loadmat.py,koalalorenzo-python-digitalocean-blob-d0221b57856fb1e131cafecf99d826f7b07a947c-digitalocean-LoadBalancer.py,mikeboers-PyAV-blob-9414187088b9b8dbaa180cfe1db6ceba243184ea-docs-includes.py,shoebot-shoebot-blob-d554c1765c1899fa25727c9fc6805d221585562b-shoebot-data-variable.py,Erotemic-ubelt-blob-db802f3ad8abba025db74b54f86e6892b8927325-ubelt-util_list.py,shoebot-shoebot-blob-d554c1765c1899fa25727c9fc6805d221585562b-lib-tuio-profiles.py,EmbodiedCognition-pagoda-blob-8892f847026d98aba8646ecbc4589397e6dec7bd-pagoda-physics.py,okfn-brasil-serenata-toolbox-blob-47b14725e8ed3a53fb52190a2ba5f29182a16959-serenata_toolbox-chamber_of_deputies-deputies_dataset.py,pinterest-thrift-tools-blob-64e74aec89e2491c781fc62d1c45944dc15aba28-thrift_tools-sniffer.py,rainwoodman-sharedmem-blob-b23e59c1ed0e28f7b6c96c17a04d55c700e06e3a-contrib-savetxt.py,saimn-sigal-blob-912ca39991355d358dc85fd55c7aeabdd7acc386-sigal-gallery.py,openstack-quark-blob-1112e6a66917d3e98e44cb7b33b107fd5a74bb2e-quark-tools-null_routes.py,inspirehep-harvesting-kit-blob-33a7f8aa9dade1d863110c6d8b27dfd955cb471f-harvestingkit-world_scientific_package.py,opencobra-cobrapy-blob-9d1987cdb3a395cf4125a3439c3b002ff2be2009-cobra-flux_analysis-room.py,bmcfee-muda-blob-ff82efdfaeb98da0a9f9124845826eb20536a9ba-muda-deformers-pitch.py,opencobra-cobrapy-blob-9d1987cdb3a395cf4125a3439c3b002ff2be2009-cobra-util-array.py,ibelie-typy-blob-3616845fb91459aacd8df6bf82c5d91f4542bee7-typy-google-protobuf-text_format.py,Diviyan-Kalainathan-CausalDiscoveryToolbox-blob-be228b078ba9eb76c01b3ccba9a1c0ad9e9e5ed1-cdt-causality-graph-bnlearn.py,quantopian-pyfolio-blob-712716ab0cdebbec9fabb25eea3bf40e4354749d-pyfolio-risk.py,Jammy2211-PyAutoLens-blob-91e50369c7a9c048c83d217625578b72423cd5a7-autolens-lens-plane.py,RediSearch-redisearch-py-blob-f65d1dd078713cbe9b83584e86655a254d0531ab-redisearch-auto_complete.py,numenta-nupic-blob-5922fafffdccc8812e72b3324965ad2f7d4bbdad-src-nupic-algorithms-connections.py,Netflix-Skunkworks-cloudaux-blob-c4b0870c3ac68b1c69e71d33cf78b6a8bdf437ea-cloudaux-orchestration-aws-vpc.py,cpenv-cpenv-blob-afbb569ae04002743db041d3629a5be8c290bd89-cpenv-packages-click-termui.py,edx-edx-search-blob-476cf02b71ceba34ae7d8b798f36d60692317c55-search-result_processor.py,numenta-nupic-blob-5922fafffdccc8812e72b3324965ad2f7d4bbdad-src-nupic-frameworks-viz-network_visualization.py,shoebot-shoebot-blob-d554c1765c1899fa25727c9fc6805d221585562b-lib-web-yahoo.py,numenta-nupic-blob-5922fafffdccc8812e72b3324965ad2f7d4bbdad-src-nupic-data-generators-sequence_machine.py,waqasbhatti-astrobase-blob-2922a14619d183fb28005fa7d02027ac436f2265-astrobase-timeutils.py,chrislit-abydos-blob-165466b3ff6afd8024a4c8660421b0c4e7773db9-abydos-stemmer-_schinke.py,CxAalto-gtfspy-blob-bddba4b74faae6c1b91202f19184811e326547e5-gtfspy-spreading-spreading_stop.py,armet-python-armet-blob-d61eca9082256cb1e7f7f3c7f2fbc4b697157de7-armet-resources-resource-options.py,Jajcus-pyxmpp2-blob-14a40a3950910a9cd008b55f0d8905aa0186ce18-pyxmpp2-jid.py,vrtsystems-hszinc-blob-d52a7c6b5bc466f3c1a77b71814c8c0776aba995-hszinc-grid.py,numenta-nupic-blob-5922fafffdccc8812e72b3324965ad2f7d4bbdad-examples-sp-hello_sp.py,peri-source-peri-blob-61beed5deaaf978ab31ed716e8470d86ba639867-peri-viz-plots.py,Infinidat-infi.gevent_utils-blob-7eb3c1601b8f2c9aaa3a83154ee7dfce8e5e5a5a-src-infi-gevent_utils-silent_greenlets.py,Jajcus-pyxmpp2-blob-14a40a3950910a9cd008b55f0d8905aa0186ce18-pyxmpp2-sasl-scram.py,apache-incubator-heron-blob-ad10325a0febe89ad337e561ebcbe37ec5d9a5ac-heron-instance-src-python-utils-tuple.py,rabitt-pysox-blob-eae89bde74567136ec3f723c3e6b369916d9b837-sox-combine.py,waqasbhatti-astrobase-blob-2922a14619d183fb28005fa7d02027ac436f2265-astrobase-magnitudes.py,openstack-quark-blob-1112e6a66917d3e98e44cb7b33b107fd5a74bb2e-quark-plugin_modules-segment_allocation_ranges.py,pybel-pybel-tools-blob-3491adea0ac4ee60f57275ef72f9b73da6dbfe0c-src-pybel_tools-summary-edge_summary.py,saimn-sigal-blob-912ca39991355d358dc85fd55c7aeabdd7acc386-sigal-plugins-media_page.py,tensorforce-tensorforce-blob-520a8d992230e382f08e315ede5fc477f5e26bfb-tensorforce-core-memories-deprecated-deprecated_prioritized_replay.py,pilosus-ForgeryPy3-blob-e15f2e59538deb4cbfceaac314f5ea897f2d5450-forgery_py-forgery-russian_tax.py,jhermann-rituals-blob-1534f50d81e19bbbe799e2eba0acdefbce047c06-src-rituals-acts-documentation.py,dereneaton-ipyrad-blob-5eeb8a178160f45faf71bf47cec4abe998a575d1-ipyrad-analysis-sratools.py,agsimeonov-cbexchange-blob-e3762f77583f89cf7b4f501ab3c7675fc7d30ab3-cbexchange-orderbook.py,ARMmbed-yotta-blob-56bc1e56c602fa20307b23fe27518e9cd6c11af1-yotta-lib-git_access.py,quantopian-pyfolio-blob-712716ab0cdebbec9fabb25eea3bf40e4354749d-pyfolio-timeseries.py,edx-edx-lint-blob-d87ccb51a48984806b6442a36992c5b45c3d4d58-edx_lint-tamper_evident.py,tklovett-PyShirtsIO-blob-ff2f2d3b5e4ab2813abbce8545b27319c6af0def-interactive_console.py,DataBiosphere-dsub-blob-443ce31daa6023dc2fd65ef2051796e19d18d5a7-dsub-lib-providers_util.py,google-grumpy-blob-3ec87959189cfcdeae82eb68a47648ac25ceb10b-third_party-stdlib-difflib.py,ManiacalLabs-BiblioPixel-blob-fd97e6c651a4bbcade64733847f4eec8f7704b7c-bibliopixel-project-edit_queue.py,Devoxin-Lavalink.py-blob-63f55c3d726d24c4cfd3674d3cd6aab6f5be110d-lavalink-WebSocket.py,CalebBell-thermo-blob-3857ed023a3e64fd3039a32d53576c24990ef1c3-thermo-permittivity.py,tensorforce-tensorforce-blob-520a8d992230e382f08e315ede5fc477f5e26bfb-tensorforce-core-optimizers-solvers-solver.py,topic2k-pygcgen-blob-c41701815df2c8c3a57fd5f7b8babe702127c8a1-pygcgen-options_parser.py,numenta-nupic-blob-5922fafffdccc8812e72b3324965ad2f7d4bbdad-src-nupic-regions-tm_region.py,ManiacalLabs-BiblioPixel-blob-fd97e6c651a4bbcade64733847f4eec8f7704b7c-bibliopixel-drivers-driver_base.py,plivo-plivohelper-python-blob-a2f706d69e2138fbb973f792041341f662072d26-examples-phonemenu-phonemenu.py,tensorlayer-tensorlayer-blob-aa9e52e36c7058a7e6fd81d36563ca6850b21956-tensorlayer-layers-convolution-deformable_conv.py,mar10-wsgidav-blob-cec0d84222fc24bea01be1cea91729001963f172-wsgidav-server-ext_wsgiutils_server.py,Dirguis-ipfn-blob-0a896ea395664515c5a424b69043937aad1d5567-ipfn-ipfn.py,openai-baselines-blob-3301089b48c42b87b396e246ea3f56fa4bfc9678-baselines-common-vec_env-shmem_vec_env.py,jhermann-rituals-blob-1534f50d81e19bbbe799e2eba0acdefbce047c06-src-rituals-util-antglob.py,tensorforce-tensorforce-blob-520a8d992230e382f08e315ede5fc477f5e26bfb-tensorforce-execution-threaded_runner.py,FaradayRF-faradayio-blob-6cf3af88bb4a83e5d2036e5cbdfaf8f0f01500bb-faradayio-faraday.py,Jajcus-pyxmpp2-blob-14a40a3950910a9cd008b55f0d8905aa0186ce18-pyxmpp2-server-listener.py,shoebot-shoebot-blob-d554c1765c1899fa25727c9fc6805d221585562b-shoebot-core-canvas.py,rossengeorgiev-aprs-python-blob-94b89a6da47a322129484efcaf1e82f6a9932891-aprslib-base91.py,bethgelab-foolbox-blob-8ab54248c70e45d8580a7d9ee44c9c0fb5755c4a-foolbox-models-tensorflow.py,alphatwirl-alphatwirl-blob-5138eeba6cd8a334ba52d6c2c022b33c61e3ba38-alphatwirl-loop-MPEventLoopRunner.py,waqasbhatti-astrobase-blob-2922a14619d183fb28005fa7d02027ac436f2265-astrobase-lcproc-lcbin.py,pingali-dgit-blob-ecde01f40b98f0719dbcfb54452270ed2f86686d-dgitcore-datasets-transformation.py,Jajcus-pyxmpp2-blob-14a40a3950910a9cd008b55f0d8905aa0186ce18-pyxmpp2-clientstream.py,uw-it-cte-uw-restclients-wheniwork-blob-0d3ca09d5bbe808fec12e5f943596570d33a1731-uw_wheniwork-locations.py,aleju-imgaug-blob-786be74aa855513840113ea523c5df495dc6a8af-imgaug-augmentables-heatmaps.py,Jajcus-pyxmpp2-blob-14a40a3950910a9cd008b55f0d8905aa0186ce18-pyxmpp2-ext-disco.py,Neurita-boyle-blob-2dae7199849395a209c887d5f30506e1de8a9ad9-boyle-nifti-roi.py,bbusenius-Diablo-Python-blob-646ac5a6f1c79cf9b928a4e2a7979988698b6c82-convert_php-convert_php.py,knipknap-SpiffWorkflow-blob-f0af7f59a332e0619e4f3c00a7d4a3d230760e00-SpiffWorkflow-specs-Join.py,Jajcus-pyxmpp2-blob-14a40a3950910a9cd008b55f0d8905aa0186ce18-pyxmpp2-mainloop-glib.py,apache-incubator-heron-blob-ad10325a0febe89ad337e561ebcbe37ec5d9a5ac-heron-executor-src-python-heron_executor.py,waqasbhatti-astrobase-blob-2922a14619d183fb28005fa7d02027ac436f2265-astrobase-lcproc-lcvfeatures.py,undertheseanlp-languageflow-blob-1436e0bf72803e02ccf727f41e8fc85ba167d9fe-languageflow-model-sgd.py,crazy-canux-arguspy-blob-e9486b5df61978a990d56bf43de35f3a4cdefcc3-scripts-check_wmi_sh.py,cos-archives-modular-odm-blob-8a34891892b8af69b21fdc46701c91763a5c1cf9-modularodm-frozen.py,tensorlayer-tensorlayer-blob-aa9e52e36c7058a7e6fd81d36563ca6850b21956-tensorlayer-visualize.py,tensorforce-tensorforce-blob-520a8d992230e382f08e315ede5fc477f5e26bfb-tensorforce-core-networks-layer.py,qualisys-qualisys_python_sdk-blob-127d7eeebc2b38b5cafdfa5d1d0198437fedd274-examples-basic_example.py,google-grumpy-blob-3ec87959189cfcdeae82eb68a47648ac25ceb10b-benchmarks-concurrent.py,openstax-cnx-publishing-blob-f55b4a2c45d8618737288f1b74b4139d5ac74154-cnxpublishing-authnz.py
"

















import ctypes
import inspect
from types import FunctionType

def _alter_code(code, **attrs):
    


    PyCode_New = ctypes.pythonapi.PyCode_New

    PyCode_New.argtypes = (
        ctypes.c_int,
        ctypes.c_int,
        ctypes.c_int,
        ctypes.c_int,
        ctypes.c_int,
        ctypes.py_object,
        ctypes.py_object,
        ctypes.py_object,
        ctypes.py_object,
        ctypes.py_object,
        ctypes.py_object,
        ctypes.py_object,
        ctypes.py_object,
        ctypes.c_int,
        ctypes.py_object)

    PyCode_New.restype = ctypes.py_object

    args = [
        [code.co_argcount, 'co_argcount'],
        [code.co_kwonlyargcount, 'co_kwonlyargcount'],
        [code.co_nlocals, 'co_nlocals'],
        [code.co_stacksize, 'co_stacksize'],
        [code.co_flags, 'co_flags'],
        [code.co_code, 'co_code'],
        [code.co_consts, 'co_consts'],
        [code.co_names, 'co_names'],
        [code.co_varnames, 'co_varnames'],
        [code.co_freevars, 'co_freevars'],
        [code.co_cellvars, 'co_cellvars'],
        [code.co_filename, 'co_filename'],
        [code.co_name, 'co_name'],
        [code.co_firstlineno, 'co_firstlineno'],
        [code.co_lnotab, 'co_lnotab']]

    for arg in args:
        if arg[1] in attrs:
            arg[0] = attrs[arg[1]]

    return PyCode_New(
        args[0][0],  
        args[1][0],  
        args[2][0],  
        args[3][0],  
        args[4][0],  
        args[5][0],  
        args[6][0],  
        args[7][0],  
        args[8][0],  
        args[9][0],  
        args[10][0],  
        args[11][0],  
        args[12][0],  
        args[13][0],  
        args[14][0])  


def _create_cell(value):

    PyCell_New = ctypes.pythonapi.PyCell_New
    PyCell_New.argtypes = (ctypes.py_object,)
    PyCell_New.restype = ctypes.py_object

    return PyCell_New(value)


def _create_closure(*values):
    return tuple(_create_cell(val) for val in values)


def alter_freevars(func, globals_=None, **vars):
    


    if globals_ is None:
        globals_ = func.__globals__

    frees = tuple(vars.keys())
    oldlocs = func.__code__.co_names
    newlocs = tuple(name for name in oldlocs if name not in frees)

    code = _alter_code(func.__code__,
                       co_freevars=frees,
                       co_names=newlocs,
                       co_flags=func.__code__.co_flags | inspect.CO_NESTED)
    closure = _create_closure(*vars.values())

    return FunctionType(code, globals_, closure=closure)

","




import logging
import os
import shutil
import tempfile

import mutagen
from gmusicapi import CallFailure
from gmusicapi.clients import Musicmanager, OAUTH_FILEPATH

from .base import _BaseWrapper
from .constants import CYGPATH_RE, GM_ID_RE
from .decorators import cast_to_list
from .utils import convert_cygwin_path, filter_google_songs, template_to_filepath

logger = logging.getLogger(__name__)


class MusicManagerWrapper(_BaseWrapper):
	


	def __init__(self, enable_logging=False):
		super().__init__(Musicmanager, enable_logging=enable_logging)

	def login(self, oauth_filename=""oauth"", uploader_id=None):
		


		cls_name = type(self).__name__

		oauth_cred = os.path.join(os.path.dirname(OAUTH_FILEPATH), oauth_filename + '.cred')

		try:
			if not self.api.login(oauth_credentials=oauth_cred, uploader_id=uploader_id):
				try:
					self.api.perform_oauth(storage_filepath=oauth_cred)
				except OSError:
					logger.exception(""\nUnable to login with specified oauth code."")

				self.api.login(oauth_credentials=oauth_cred, uploader_id=uploader_id)
		except (OSError, ValueError):
			logger.exception(""{} authentication failed."".format(cls_name))

			return False

		if not self.is_authenticated:
			logger.warning(""{} authentication failed."".format(cls_name))

			return False

		logger.info(""{} authentication succeeded.\n"".format(cls_name))

		return True

	def logout(self, revoke_oauth=False):
		


		return self.api.logout(revoke_oauth=revoke_oauth)

	def get_google_songs(
		self, include_filters=None, exclude_filters=None, all_includes=False, all_excludes=False,
		uploaded=True, purchased=True):
		


		if not uploaded and not purchased:
			raise ValueError(""One or both of uploaded/purchased parameters must be True."")

		logger.info(""Loading Google Music songs..."")

		google_songs = []

		if uploaded:
			google_songs += self.api.get_uploaded_songs()

		if purchased:
			for song in self.api.get_purchased_songs():
				if song not in google_songs:
					google_songs.append(song)

		matched_songs, filtered_songs = filter_google_songs(
			google_songs, include_filters=include_filters, exclude_filters=exclude_filters,
			all_includes=all_includes, all_excludes=all_excludes
		)

		logger.info(""Filtered {0} Google Music songs"".format(len(filtered_songs)))
		logger.info(""Loaded {0} Google Music songs"".format(len(matched_songs)))

		return matched_songs, filtered_songs

	@cast_to_list(0)
	def _download(self, songs, template=None):
		if not template:
			template = os.getcwd()

		if os.name == 'nt' and CYGPATH_RE.match(template):
			template = convert_cygwin_path(template)

		for song in songs:
			song_id = song['id']

			title = song.get('title', ""<empty>"")
			artist = song.get('artist', ""<empty>"")
			album = song.get('album', ""<empty>"")

			logger.debug(
				""Downloading {title} -- {artist} -- {album} ({song_id})"".format(
					title=title, artist=artist, album=album, song_id=song_id
				)
			)

			try:
				_, audio = self.api.download_song(song_id)
			except CallFailure as e:
				result = ({}, {song_id: e})
			else:
				with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp:
					temp.write(audio)

				metadata = mutagen.File(temp.name, easy=True)
				filepath = template_to_filepath(template, metadata) + '.mp3'
				dirname = os.path.dirname(filepath)

				if dirname:
					try:
						os.makedirs(dirname)
					except OSError:
						if not os.path.isdir(dirname):
							raise

				shutil.move(temp.name, filepath)

				result = ({song_id: filepath}, {})

			yield result

	@cast_to_list(0)
	def download(self, songs, template=None):
		


		if not template:
			template = os.getcwd()

		songnum = 0
		total = len(songs)
		results = []
		errors = {}
		pad = len(str(total))

		for result in self._download(songs, template):
			song_id = songs[songnum]['id']
			songnum += 1

			downloaded, error = result

			if downloaded:
				logger.info(
					""({num:>{pad}}/{total}) Successfully downloaded -- {file} ({song_id})"".format(
						num=songnum, pad=pad, total=total, file=downloaded[song_id], song_id=song_id
					)
				)

				results.append({'result': 'downloaded', 'id': song_id, 'filepath': downloaded[song_id]})
			elif error:
				title = songs[songnum].get('title', ""<empty>"")
				artist = songs[songnum].get('artist', ""<empty>"")
				album = songs[songnum].get('album', ""<empty>"")

				logger.info(
					""({num:>{pad}}/{total}) Error on download -- {title} -- {artist} -- {album} ({song_id})"".format(
						num=songnum, pad=pad, total=total, title=title, artist=artist, album=album, song_id=song_id
					)
				)

				results.append({'result': 'error', 'id': song_id, 'message': error[song_id]})

		if errors:
			logger.info(""\n\nThe following errors occurred:\n"")
			for filepath, e in errors.items():
				logger.info(""{file} | {error}"".format(file=filepath, error=e))
			logger.info(""\nThese files may need to be synced again.\n"")

		return results

	@cast_to_list(0)
	def _upload(self, filepaths, enable_matching=False, transcode_quality='320k'):
		for filepath in filepaths:
			try:
				logger.debug(""Uploading -- {}"".format(filepath))
				uploaded, matched, not_uploaded = self.api.upload(
					filepath, enable_matching=enable_matching, transcode_quality=transcode_quality
				)
				result = (uploaded, matched, not_uploaded, {})
			except CallFailure as e:
				result = ({}, {}, {}, {filepath: e})

			yield result

	@cast_to_list(0)
	def upload(self, filepaths, enable_matching=False, transcode_quality='320k', delete_on_success=False):
		


		filenum = 0
		total = len(filepaths)
		results = []
		errors = {}
		pad = len(str(total))
		exist_strings = [""ALREADY_EXISTS"", ""this song is already uploaded""]

		for result in self._upload(filepaths, enable_matching=enable_matching, transcode_quality=transcode_quality):
			filepath = filepaths[filenum]
			filenum += 1

			uploaded, matched, not_uploaded, error = result

			if uploaded:
				logger.info(
					""({num:>{pad}}/{total}) Successfully uploaded -- {file} ({song_id})"".format(
						num=filenum, pad=pad, total=total, file=filepath, song_id=uploaded[filepath]
					)
				)

				results.append({'result': 'uploaded', 'filepath': filepath, 'id': uploaded[filepath]})
			elif matched:
				logger.info(
					""({num:>{pad}}/{total}) Successfully scanned and matched -- {file} ({song_id})"".format(
						num=filenum, pad=pad, total=total, file=filepath, song_id=matched[filepath]
					)
				)

				results.append({'result': 'matched', 'filepath': filepath, 'id': matched[filepath]})
			elif error:
				logger.warning(""({num:>{pad}}/{total}) Error on upload -- {file}"".format(num=filenum, pad=pad, total=total, file=filepath))

				results.append({'result': 'error', 'filepath': filepath, 'message': error[filepath]})
				errors.update(error)
			else:
				if any(exist_string in not_uploaded[filepath] for exist_string in exist_strings):
					response = ""ALREADY EXISTS""

					song_id = GM_ID_RE.search(not_uploaded[filepath]).group(0)

					logger.info(
						""({num:>{pad}}/{total}) Failed to upload -- {file} ({song_id}) | {response}"".format(
							num=filenum, pad=pad, total=total, file=filepath, response=response, song_id=song_id
						)
					)

					results.append({'result': 'not_uploaded', 'filepath': filepath, 'id': song_id, 'message': not_uploaded[filepath]})
				else:
					response = not_uploaded[filepath]

					logger.info(
						""({num:>{pad}}/{total}) Failed to upload -- {file} | {response}"".format(
							num=filenum, pad=pad, total=total, file=filepath, response=response
						)
					)

					results.append({'result': 'not_uploaded', 'filepath': filepath, 'message': not_uploaded[filepath]})

			success = (uploaded or matched) or (not_uploaded and 'ALREADY_EXISTS' in not_uploaded[filepath])

			if success and delete_on_success:
				try:
					os.remove(filepath)
				except (OSError, PermissionError):
					logger.warning(""Failed to remove {} after successful upload"".format(filepath))

		if errors:
			logger.info(""\n\nThe following errors occurred:\n"")

			for filepath, e in errors.items():
				logger.info(""{file} | {error}"".format(file=filepath, error=e))
			logger.info(""\nThese filepaths may need to be synced again.\n"")

		return results
","
from .baseapi import BaseAPI


class Account(BaseAPI):
    def __init__(self, *args, **kwargs):
        self.droplet_limit = None
        self.floating_ip_limit = None
        self.email = None
        self.uuid = None
        self.email_verified = None
        self.status = None
        self.status_message = None

        super(Account, self).__init__(*args, **kwargs)

    @classmethod
    def get_object(cls, api_token):
        

        acct = cls(token=api_token)
        acct.load()
        return acct

    def load(self):
        
        data = self.get_data(""account/"")
        account = data['account']

        for attr in account.keys():
            setattr(self, attr, account[attr])

    def __str__(self):
        return ""%s"" % (self.email)
","import varint

from .constants import NAME_TABLE, CODE_TABLE


def extract_prefix(bytes_):
    

    try:
        return varint.decode_bytes(bytes_)
    except TypeError:
        raise ValueError('incorrect varint provided')


def get_prefix(multicodec):
    

    try:
        prefix = varint.encode(NAME_TABLE[multicodec])
    except KeyError:
        raise ValueError('{} multicodec is not supported.'.format(multicodec))
    return prefix


def add_prefix(multicodec, bytes_):
    

    prefix = get_prefix(multicodec)
    return b''.join([prefix, bytes_])


def remove_prefix(bytes_):
    

    prefix_int = extract_prefix(bytes_)
    prefix = varint.encode(prefix_int)
    return bytes_[len(prefix):]


def get_codec(bytes_):
    

    prefix = extract_prefix(bytes_)
    try:
        return CODE_TABLE[prefix]
    except KeyError:
        raise ValueError('Prefix {} not present in the lookup table'.format(prefix))


def is_codec(name):
    

    return name in NAME_TABLE
","


__all__ = ['loadmat']


import struct
import sys
import zlib

from collections import Sequence
from itertools import tee
try:
    from itertools import izip
    ispy2 = True
except ImportError:
    izip = zip
    basestring = str
    ispy2 = False
from io import BytesIO



asbytes = lambda s: s.encode('latin1')
asstr = lambda b: b.decode('latin1')


etypes = {
    'miINT8': {'n': 1, 'fmt': 'b'},
    'miUINT8': {'n': 2, 'fmt': 'B'},
    'miINT16': {'n': 3, 'fmt': 'h'},
    'miUINT16': {'n': 4, 'fmt': 'H'},
    'miINT32': {'n': 5, 'fmt': 'i'},
    'miUINT32': {'n': 6, 'fmt': 'I'},
    'miSINGLE': {'n': 7, 'fmt': 'f'},
    'miDOUBLE': {'n': 9, 'fmt': 'd'},
    'miINT64': {'n': 12, 'fmt': 'q'},
    'miUINT64': {'n': 13, 'fmt': 'Q'},
    'miMATRIX': {'n': 14},
    'miCOMPRESSED': {'n': 15},
    'miUTF8': {'n': 16, 'fmt': 's'},
    'miUTF16': {'n': 17, 'fmt': 's'},
    'miUTF32': {'n': 18, 'fmt': 's'}
}


inv_etypes = dict((v['n'], k) for k, v in etypes.items())


mclasses = {
    'mxCELL_CLASS': 1,
    'mxSTRUCT_CLASS': 2,
    'mxOBJECT_CLASS': 3,
    'mxCHAR_CLASS': 4,
    'mxSPARSE_CLASS': 5,
    'mxDOUBLE_CLASS': 6,
    'mxSINGLE_CLASS': 7,
    'mxINT8_CLASS': 8,
    'mxUINT8_CLASS': 9,
    'mxINT16_CLASS': 10,
    'mxUINT16_CLASS': 11,
    'mxINT32_CLASS': 12,
    'mxUINT32_CLASS': 13,
    'mxINT64_CLASS': 14,
    'mxUINT64_CLASS': 15,
    'mxFUNCTION_CLASS': 16,
    'mxOPAQUE_CLASS': 17,
    'mxOBJECT_CLASS_FROM_MATRIX_H': 18
}


numeric_class_etypes = {
    'mxDOUBLE_CLASS': 'miDOUBLE',
    'mxSINGLE_CLASS': 'miSINGLE',
    'mxINT8_CLASS': 'miINT8',
    'mxUINT8_CLASS': 'miUINT8',
    'mxINT16_CLASS': 'miINT16',
    'mxUINT16_CLASS': 'miUINT16',
    'mxINT32_CLASS': 'miINT32',
    'mxUINT32_CLASS': 'miUINT32',
    'mxINT64_CLASS': 'miINT64',
    'mxUINT64_CLASS': 'miUINT64'
}

inv_mclasses = dict((v, k) for k, v in mclasses.items())


compressed_numeric = ['miINT32', 'miUINT16', 'miINT16', 'miUINT8']


def diff(iterable):
    

    a, b = tee(iterable)
    next(b, None)
    return (i - j for i, j in izip(a, b))






def unpack(endian, fmt, data):
    

    if fmt == 's':
        
        val = struct.unpack(''.join([endian, str(len(data)), 's']),
                            data)[0]
    else:
        
        num = len(data) // struct.calcsize(fmt)
        val = struct.unpack(''.join([endian, str(num), fmt]), data)
        if len(val) == 1:
            val = val[0]
    return val


def read_file_header(fd, endian):
    

    fields = [
        ('description', 's', 116),
        ('subsystem_offset', 's', 8),
        ('version', 'H', 2),
        ('endian_test', 's', 2)
    ]
    hdict = {}
    for name, fmt, num_bytes in fields:
        data = fd.read(num_bytes)
        hdict[name] = unpack(endian, fmt, data)
    hdict['description'] = hdict['description'].strip()
    v_major = hdict['version'] >> 8
    v_minor = hdict['version'] & 0xFF
    hdict['__version__'] = '%d.%d' % (v_major, v_minor)
    return hdict


def read_element_tag(fd, endian):
    

    data = fd.read(8)
    mtpn = unpack(endian, 'I', data[:4])
    
    
    num_bytes = mtpn >> 16
    if num_bytes > 0:
        
        mtpn = mtpn & 0xFFFF
        if num_bytes > 4:
            raise ParseError('Error parsing Small Data Element (SDE) '
                             'formatted data')
        data = data[4:4 + num_bytes]
    else:
        
        num_bytes = unpack(endian, 'I', data[4:])
        data = None
    return (mtpn, num_bytes, data)


def read_elements(fd, endian, mtps, is_name=False):
    

    mtpn, num_bytes, data = read_element_tag(fd, endian)
    if mtps and mtpn not in [etypes[mtp]['n'] for mtp in mtps]:
        raise ParseError('Got type {}, expected {}'.format(
            mtpn, ' / '.join('{} ({})'.format(
                etypes[mtp]['n'], mtp) for mtp in mtps)))
    if not data:
        
        data = fd.read(num_bytes)
        
        mod8 = num_bytes % 8
        if mod8:
            fd.seek(8 - mod8, 1)

    
    if is_name:
        
        fmt = 's'
        val = [unpack(endian, fmt, s)
               for s in data.split(b'\0') if s]
        if len(val) == 0:
            val = ''
        elif len(val) == 1:
            val = asstr(val[0])
        else:
            val = [asstr(s) for s in val]
    else:
        fmt = etypes[inv_etypes[mtpn]]['fmt']
        val = unpack(endian, fmt, data)
    return val


def read_header(fd, endian):
    

    flag_class, nzmax = read_elements(fd, endian, ['miUINT32'])
    header = {
        'mclass': flag_class & 0x0FF,
        'is_logical': (flag_class >> 9 & 1) == 1,
        'is_global': (flag_class >> 10 & 1) == 1,
        'is_complex': (flag_class >> 11 & 1) == 1,
        'nzmax': nzmax
    }
    header['dims'] = read_elements(fd, endian, ['miINT32'])
    header['n_dims'] = len(header['dims'])
    if header['n_dims'] != 2:
        raise ParseError('Only matrices with dimension 2 are supported.')
    header['name'] = read_elements(fd, endian, ['miINT8'], is_name=True)
    return header


def read_var_header(fd, endian):
    

    mtpn, num_bytes = unpack(endian, 'II', fd.read(8))
    next_pos = fd.tell() + num_bytes

    if mtpn == etypes['miCOMPRESSED']['n']:
        
        data = fd.read(num_bytes)
        dcor = zlib.decompressobj()
        
        fd_var = BytesIO(dcor.decompress(data))
        del data
        fd = fd_var
        
        if dcor.flush() != b'':
            raise ParseError('Error in compressed data.')
        
        mtpn, num_bytes = unpack(endian, 'II', fd.read(8))

    if mtpn != etypes['miMATRIX']['n']:
        raise ParseError('Expecting miMATRIX type number {}, '
                         'got {}'.format(etypes['miMATRIX']['n'], mtpn))
    
    header = read_header(fd, endian)
    return header, next_pos, fd


def squeeze(array):
    

    if len(array) == 1:
        array = array[0]
    return array


def read_numeric_array(fd, endian, header, data_etypes):
    

    if header['is_complex']:
        raise ParseError('Complex arrays are not supported')
    
    data = read_elements(fd, endian, data_etypes)
    if not isinstance(data, Sequence):
        
        return data
    
    
    rowcount = header['dims'][0]
    colcount = header['dims'][1]
    array = [list(data[c * rowcount + r] for c in range(colcount))
             for r in range(rowcount)]
    
    return squeeze(array)


def read_cell_array(fd, endian, header):
    

    array = [list() for i in range(header['dims'][0])]
    for row in range(header['dims'][0]):
        for col in range(header['dims'][1]):
            
            vheader, next_pos, fd_var = read_var_header(fd, endian)
            varray = read_var_array(fd_var, endian, vheader)
            array[row].append(varray)
            
            fd.seek(next_pos)
    
    if header['dims'][0] == 1:
        return squeeze(array[0])
    return squeeze(array)


def read_struct_array(fd, endian, header):
    

    
    field_name_length = read_elements(fd, endian, ['miINT32'])
    if field_name_length > 32:
        raise ParseError('Unexpected field name length: {}'.format(
                         field_name_length))

    
    fields = read_elements(fd, endian, ['miINT8'], is_name=True)
    if isinstance(fields, basestring):
        fields = [fields]

    
    empty = lambda: [list() for i in range(header['dims'][0])]
    array = {}
    for row in range(header['dims'][0]):
        for col in range(header['dims'][1]):
            for field in fields:
                
                vheader, next_pos, fd_var = read_var_header(fd, endian)
                data = read_var_array(fd_var, endian, vheader)
                if field not in array:
                    array[field] = empty()
                array[field][row].append(data)
                
                fd.seek(next_pos)
    
    for field in fields:
        rows = array[field]
        for i in range(header['dims'][0]):
            rows[i] = squeeze(rows[i])
        array[field] = squeeze(array[field])
    return array


def read_char_array(fd, endian, header):
    array = read_numeric_array(fd, endian, header, ['miUTF8'])
    if header['dims'][0] > 1:
        
        array = [asstr(bytearray(i)) for i in array]
    else:
        
        array = asstr(bytearray(array))
    return array


def read_var_array(fd, endian, header):
    

    mc = inv_mclasses[header['mclass']]

    if mc in numeric_class_etypes:
        return read_numeric_array(
            fd, endian, header,
            set(compressed_numeric).union([numeric_class_etypes[mc]])
        )
    elif mc == 'mxSPARSE_CLASS':
        raise ParseError('Sparse matrices not supported')
    elif mc == 'mxCHAR_CLASS':
        return read_char_array(fd, endian, header)
    elif mc == 'mxCELL_CLASS':
        return read_cell_array(fd, endian, header)
    elif mc == 'mxSTRUCT_CLASS':
        return read_struct_array(fd, endian, header)
    elif mc == 'mxOBJECT_CLASS':
        raise ParseError('Object classes not supported')
    elif mc == 'mxFUNCTION_CLASS':
        raise ParseError('Function classes not supported')
    elif mc == 'mxOPAQUE_CLASS':
        raise ParseError('Anonymous function classes not supported')


def eof(fd):
    

    b = fd.read(1)
    end = len(b) == 0
    if not end:
        curpos = fd.tell()
        fd.seek(curpos - 1)
    return end


class ParseError(Exception):
    pass







def loadmat(filename, meta=False):
    


    if isinstance(filename, basestring):
        fd = open(filename, 'rb')
    else:
        fd = filename

    
    
    
    
    fd.seek(124)
    tst_str = fd.read(4)
    little_endian = (tst_str[2:4] == b'IM')
    endian = ''
    if (sys.byteorder == 'little' and little_endian) or \
       (sys.byteorder == 'big' and not little_endian):
        
        pass
    elif sys.byteorder == 'little':
        
        endian = '>'
    else:
        
        endian = '<'
    maj_ind = int(little_endian)
    
    maj_val = ord(tst_str[maj_ind]) if ispy2 else tst_str[maj_ind]
    if maj_val != 1:
        raise ParseError('Can only read from Matlab level 5 MAT-files')
    
    

    mdict = {}
    if meta:
        
        fd.seek(0)
        mdict['__header__'] = read_file_header(fd, endian)
        mdict['__globals__'] = []

    
    while not eof(fd):
        hdr, next_position, fd_var = read_var_header(fd, endian)
        name = hdr['name']
        if name in mdict:
            raise ParseError('Duplicate variable name ""{}"" in mat file.'
                             .format(name))

        
        mdict[name] = read_var_array(fd_var, endian, hdr)
        if meta and hdr['is_global']:
            mdict['__globals__'].append(name)

        
        fd.seek(next_position)

    fd.close()
    return mdict
","
from .baseapi import BaseAPI, GET, POST, PUT, DELETE


class StickySesions(object):
    

    def __init__(self, type='none', cookie_name='', cookie_ttl_seconds=None):
        self.type = type
        if type is 'cookies':
            self.cookie_name = 'DO-LB'
            self.cookie_ttl_seconds = 300
        self.cookie_name = cookie_name
        self.cookie_ttl_seconds = cookie_ttl_seconds


class ForwardingRule(object):
    

    def __init__(self, entry_protocol=None, entry_port=None,
                 target_protocol=None, target_port=None, certificate_id="""",
                 tls_passthrough=False):
        self.entry_protocol = entry_protocol
        self.entry_port = entry_port
        self.target_protocol = target_protocol
        self.target_port = target_port
        self.certificate_id = certificate_id
        self.tls_passthrough = tls_passthrough


class HealthCheck(object):
    

    def __init__(self, protocol='http', port=80, path='/',
                 check_interval_seconds=10, response_timeout_seconds=5,
                 healthy_threshold=5, unhealthy_threshold=3):
        self.protocol = protocol
        self.port = port
        self.path = path
        self.check_interval_seconds = check_interval_seconds
        self.response_timeout_seconds = response_timeout_seconds
        self.healthy_threshold = healthy_threshold
        self.unhealthy_threshold = unhealthy_threshold


class LoadBalancer(BaseAPI):
    

    def __init__(self, *args, **kwargs):
        self.id = None
        self.name = None
        self.region = None
        self.algorithm = None
        self.forwarding_rules = []
        self.health_check = None
        self.sticky_sessions = None
        self.redirect_http_to_https = False
        self.droplet_ids = []
        self.tag = None
        self.status = None
        self.created_at = None

        super(LoadBalancer, self).__init__(*args, **kwargs)

    @classmethod
    def get_object(cls, api_token, id):
        

        load_balancer = cls(token=api_token, id=id)
        load_balancer.load()
        return load_balancer

    def load(self):
        

        data = self.get_data('load_balancers/%s' % self.id, type=GET)
        load_balancer = data['load_balancer']

        
        for attr in load_balancer.keys():
            if attr == 'health_check':
                health_check = HealthCheck(**load_balancer['health_check'])
                setattr(self, attr, health_check)
            elif attr == 'sticky_sessions':
                sticky_ses = StickySesions(**load_balancer['sticky_sessions'])
                setattr(self, attr, sticky_ses)
            elif attr == 'forwarding_rules':
                rules = list()
                for rule in load_balancer['forwarding_rules']:
                    rules.append(ForwardingRule(**rule))
                setattr(self, attr, rules)
            else:
                setattr(self, attr, load_balancer[attr])

        return self

    def create(self, *args, **kwargs):
        

        rules_dict = [rule.__dict__ for rule in self.forwarding_rules]

        params = {'name': self.name, 'region': self.region,
                  'forwarding_rules': rules_dict,
                  'redirect_http_to_https': self.redirect_http_to_https}

        if self.droplet_ids and self.tag:
            raise ValueError('droplet_ids and tag are mutually exclusive args')
        elif self.tag:
            params['tag'] = self.tag
        else:
            params['droplet_ids'] = self.droplet_ids

        if self.algorithm:
            params['algorithm'] = self.algorithm
        if self.health_check:
            params['health_check'] = self.health_check.__dict__
        if self.sticky_sessions:
            params['sticky_sessions'] = self.sticky_sessions.__dict__

        data = self.get_data('load_balancers/', type=POST, params=params)

        if data:
            self.id = data['load_balancer']['id']
            self.ip = data['load_balancer']['ip']
            self.algorithm = data['load_balancer']['algorithm']
            self.health_check = HealthCheck(
                **data['load_balancer']['health_check'])
            self.sticky_sessions = StickySesions(
                **data['load_balancer']['sticky_sessions'])
            self.droplet_ids = data['load_balancer']['droplet_ids']
            self.status = data['load_balancer']['status']
            self.created_at = data['load_balancer']['created_at']

        return self

    def save(self):
        

        forwarding_rules = [rule.__dict__ for rule in self.forwarding_rules]

        data = {
            'name': self.name,
            'region': self.region['slug'],
            'forwarding_rules': forwarding_rules,
            'redirect_http_to_https': self.redirect_http_to_https
        }

        if self.tag:
            data['tag'] = self.tag
        else:
            data['droplet_ids'] = self.droplet_ids

        if self.algorithm:
            data[""algorithm""] = self.algorithm
        if self.health_check:
            data['health_check'] = self.health_check.__dict__
        if self.sticky_sessions:
            data['sticky_sessions'] = self.sticky_sessions.__dict__

        return self.get_data(""load_balancers/%s/"" % self.id,
                             type=PUT,
                             params=data)

    def destroy(self):
        

        return self.get_data('load_balancers/%s/' % self.id, type=DELETE)

    def add_droplets(self, droplet_ids):
        

        return self.get_data(
            ""load_balancers/%s/droplets/"" % self.id,
            type=POST,
            params={""droplet_ids"": droplet_ids}
        )

    def remove_droplets(self, droplet_ids):
        

        return self.get_data(
            ""load_balancers/%s/droplets/"" % self.id,
            type=DELETE,
            params={""droplet_ids"": droplet_ids}
        )

    def add_forwarding_rules(self, forwarding_rules):
        

        rules_dict = [rule.__dict__ for rule in forwarding_rules]

        return self.get_data(
            ""load_balancers/%s/forwarding_rules/"" % self.id,
            type=POST,
            params={""forwarding_rules"": rules_dict}
        )

    def remove_forwarding_rules(self, forwarding_rules):
        

        rules_dict = [rule.__dict__ for rule in forwarding_rules]

        return self.get_data(
            ""load_balancers/%s/forwarding_rules/"" % self.id,
            type=DELETE,
            params={""forwarding_rules"": rules_dict}
        )

    def __str__(self):
        return ""%s"" % (self.id)
","from __future__ import print_function

import json
import os
import re
import sys

import xml.etree.ElementTree as etree

from Cython.Compiler.Main import compile_single, CompilationOptions
from Cython.Compiler.TreeFragment import parse_from_strings
from Cython.Compiler.Visitor import TreeVisitor
from Cython.Compiler import Nodes

os.chdir(os.path.abspath(os.path.join(__file__, '..', '..')))


class Visitor(TreeVisitor):

    def __init__(self, state=None):
        super(Visitor, self).__init__()
        self.state = dict(state or {})
        self.events = []

    def record_event(self, node, **kw):
        state = self.state.copy()
        state.update(**kw)
        state['node'] = node
        state['pos'] = node.pos
        state['end_pos'] = node.end_pos()
        self.events.append(state)

    def visit_Node(self, node):
        self.visitchildren(node)

    def visit_ModuleNode(self, node):
        self.state['module'] = node.full_module_name
        self.visitchildren(node)
        self.state.pop('module')

    def visit_CDefExternNode(self, node):
        self.state['extern_from'] = node.include_file
        self.visitchildren(node)
        self.state.pop('extern_from')

    def visit_CStructOrUnionDefNode(self, node):
        self.record_event(node, type='struct', name=node.name)
        self.state['struct'] = node.name
        self.visitchildren(node)
        self.state.pop('struct')

    def visit_CFuncDeclaratorNode(self, node):
        if isinstance(node.base, Nodes.CNameDeclaratorNode):
            self.record_event(node, type='function', name=node.base.name)
        else:
            self.visitchildren(node)

    def visit_CVarDefNode(self, node):

        if isinstance(node.declarators[0], Nodes.CNameDeclaratorNode):

            
            
            type_ = node.base_type
            if hasattr(type_, 'name'):
                type_name = type_.name
            elif hasattr(type_, 'base_type'):
                type_name = type_.base_type.name
            else:
                type_name = str(type_)

            self.record_event(node, type='variable', name=node.declarators[0].name,
                vartype=type_name)

        else:
            self.visitchildren(node)

    def visit_CClassDefNode(self, node):
        self.state['class'] = node.class_name
        self.visitchildren(node)
        self.state.pop('class')

    def visit_PropertyNode(self, node):
        self.state['property'] = node.name
        self.visitchildren(node)
        self.state.pop('property')

    def visit_DefNode(self, node):
        self.state['function'] = node.name
        self.visitchildren(node)
        self.state.pop('function')

    def visit_AttributeNode(self, node):
        if getattr(node.obj, 'name', None) == 'lib':
            self.record_event(node, type='use', name=node.attribute)
        else:
            self.visitchildren(node)


def extract(path, **kwargs):

    name = os.path.splitext(os.path.relpath(path))[0].replace('/', '.')

    options = CompilationOptions()
    options.include_path.append('include')
    options.language_level = 2
    options.compiler_directives = dict(
        c_string_type='str',
        c_string_encoding='ascii',
    )

    context = options.create_context()

    tree = parse_from_strings(name, open(path).read(), context,
        level='module_pxd' if path.endswith('.pxd') else None,
        **kwargs)

    extractor = Visitor({'file': path})
    extractor.visit(tree)
    return extractor.events


def iter_cython(path):
    

    for dir_path, dir_names, file_names in os.walk(path):
        for file_name in file_names:
            if file_name.startswith('.'):
                continue
            if os.path.splitext(file_name)[1] not in ('.pyx', '.pxd'):
                continue
            yield os.path.join(dir_path, file_name)


doxygen = {}
doxygen_base = 'https://ffmpeg.org/doxygen/trunk'
tagfile_path = 'tmp/tagfile.xml'

tagfile_json = tagfile_path + '.json'
if os.path.exists(tagfile_json):
    print('Loading pre-parsed Doxygen tagfile:', tagfile_json, file=sys.stderr)
    doxygen = json.load(open(tagfile_json))


if not doxygen:

    print('Parsing Doxygen tagfile:', tagfile_path, file=sys.stderr)
    if not os.path.exists(tagfile_path):
        print('    MISSING!', file=sys.stderr)
    else:

        root = etree.parse(tagfile_path)

        def inspect_member(node, name_prefix=''):
            name = name_prefix + node.find('name').text
            anchorfile = node.find('anchorfile').text
            anchor = node.find('anchor').text

            url = '%s/%s

            doxygen[name] = {'url': url}

            if node.attrib['kind'] == 'function':
                ret_type = node.find('type').text
                arglist = node.find('arglist').text
                sig = '%s %s%s' % (ret_type, name, arglist)
                doxygen[name]['sig'] = sig

        for struct in root.iter('compound'):
            if struct.attrib['kind'] != 'struct':
                continue
            name_prefix = struct.find('name').text + '.'
            for node in struct.iter('member'):
                inspect_member(node, name_prefix)

        for node in root.iter('member'):
            inspect_member(node)


        json.dump(doxygen, open(tagfile_json, 'w'), sort_keys=True, indent=4)


print('Parsing Cython source for references...', file=sys.stderr)
lib_references = {}
for path in iter_cython('av'):
    try:
        events = extract(path)
    except Exception as e:
        print(""    %s in %s"" % (e.__class__.__name__, path), file=sys.stderr)
        print(""        %s"" % e, file=sys.stderr)
        continue
    for event in events:
        if event['type'] == 'use':
            lib_references.setdefault(event['name'], []).append(event)







defs_by_extern = {}
for path in iter_cython('include'):

    
    
    if path == 'include/libav.pxd':
        continue

    
    comments_by_line = {}
    for i, line in enumerate(open(path)):
        m = re.match(r'^\s*
        if m:
            comment = line[m.end():].rstrip()
            comments_by_line[i + 1] = line[m.end():]

    
    for event in extract(path):

        extern = event.get('extern_from') or path.replace('include/', '')
        defs_by_extern.setdefault(extern, []).append(event)

        
        comments = event['_comments'] = []
        line = event['pos'][1] - 1
        while line in comments_by_line:
            comments.insert(0, comments_by_line.pop(line))
            line -= 1
        line = event['end_pos'][1] + 1
        while line in comments_by_line:
            comments.append(comments_by_line.pop(line))
            line += 1

        
        if event['type'] == 'function':
            event['_sort_key'] = 2
            sig = doxygen.get(event['name'], {}).get('sig')
            if sig:
                sig = re.sub(r'\).+', ')', sig) 
                event['_headline'] = '.. c:function:: %s' % sig
            else:
                event['_headline'] = '.. c:function:: %s()' % event['name']

        elif event['type'] == 'variable':
            struct = event.get('struct')
            if struct:
                event['_headline'] = '.. c:member:: %s %s' % (event['vartype'], event['name'])
                event['_sort_key'] = 1.1
            else:
                event['_headline'] = '.. c:var:: %s' % event['name']
                event['_sort_key'] = 3

        elif event['type'] == 'struct':
            event['_headline'] = '.. c:type:: struct %s' % event['name']
            event['_sort_key'] = 1
            event['_doxygen_url'] = '%s/struct%s.html' % (doxygen_base, event['name'])

        else:
            print('Unknown event type %s' % event['type'], file=sys.stderr)

        name = event['name']
        if event.get('struct'):
            name = '%s.%s' % (event['struct'], name)

        
        event.setdefault('_doxygen_url', doxygen.get(name, {}).get('url'))

        
        ref_events = lib_references.get(name, [])
        if ref_events:

            ref_pairs = []
            for ref in sorted(ref_events, key=lambda e: e['name']):

                chunks = [
                    ref.get('module'),
                    ref.get('class'),
                ]
                chunks = filter(None, chunks)
                prefix = '.'.join(chunks) + '.' if chunks else ''

                if ref.get('property'):
                    ref_pairs.append((ref['property'], ':attr:`%s%s`' % (prefix, ref['property'])))
                elif ref.get('function'):
                    name = ref['function']
                    if name in ('__init__', '__cinit__', '__dealloc__'):
                        ref_pairs.append((name, ':class:`%s%s <%s>`' % (prefix, name, prefix.rstrip('.'))))
                    else:
                        ref_pairs.append((name, ':func:`%s%s`' % (prefix, name)))
                else:
                    continue

            unique_refs = event['_references'] = []
            seen = set()
            for name, ref in sorted(ref_pairs):
                if name in seen:
                    continue
                seen.add(name)
                unique_refs.append(ref)




print(
)

for extern, events in sorted(defs_by_extern.items()):
    did_header = False

    for event in events:

        headline = event.get('_headline')
        comments = event.get('_comments')
        refs = event.get('_references', [])
        url = event.get('_doxygen_url')
        indent = '    ' if event.get('struct') else ''

        if not headline:
            continue
        if (
            not filter(None, (x.strip() for x in comments if x.strip())) and
            not refs and
            event['type'] not in ('struct', )
        ):
            pass

        if not did_header:
            print('``%s``' % extern)
            print('-' * (len(extern) + 4))
            print()
            did_header = True

        if url:
            print()
            print(indent + '.. rst-class:: ffmpeg-quicklink')
            print()
            print(indent + '    `FFmpeg Docs <%s>`_' % url)

        print(indent + headline)
        print()

        if comments:
            for line in comments:
                print(indent + '    ' + line)
            print()

        if refs:
            print(indent + '    Referenced by: ', end='')
            for i, ref in enumerate(refs):
                print((', ' if i else '') + ref, end='')
            print('.')

        print()
","NUMBER = 1
TEXT = 2
BOOLEAN = 3
BUTTON = 4

DEFAULT_STEPS = 256.0


def clamp(minvalue, value, maxvalue):
    return max(minvalue, min(value, maxvalue))


class Variable(object):
    


    def __init__(self, name, type, **kwargs):
        

        self.name = name
        if not isinstance(name, basestring):
            raise AttributeError(""Variable name must be a string"")
        if kwargs.get(""step"") and kwargs.get(""steps""):
            raise AttributeError(""Can only set step or steps"")  
        self.type = type or NUMBER
        self.min = None
        self.max = None
        self.step = None or kwargs.get(""step"")
        self.steps = kwargs.get(""steps"", DEFAULT_STEPS)
        if self.type == NUMBER:
            self.min = kwargs.get(""min"", 0.0)
            self.max = kwargs.get(""max"", 100.0)
            if self.step is None:
                diff = max(self.min, self.max) - min(self.min, self.max)
                self.step = (diff / float(self.steps))
            self.default = kwargs.get(""default"")
            if self.default is None:
                self.default = self.min
        elif self.type == TEXT:
            self.default = kwargs.get(""default"", ""bonjour"")
        elif self.type == BOOLEAN:
            self.default = kwargs.get(""default"", True)
        elif self.type == BUTTON:
            self.default = kwargs.get(""default"", self.name)
        else:
            raise AttributeError(""Variables must be of type NUMBER, TEXT, BOOLEAN or BUTTON"")
        self.value = kwargs.get(""value"", self.default)
        if self.value is None and self.default is not None:
            self.value = self.default

    def sanitize(self, val):
        

        if self.type == NUMBER:
            try:
                return clamp(self.min, self.max, float(val))
            except ValueError:
                return 0.0
        elif self.type == TEXT:
            try:
                return unicode(str(val), ""utf_8"", ""replace"")
            except:
                return """"
        elif self.type == BOOLEAN:
            if unicode(val).lower() in (""true"", ""1"", ""yes""):
                return True
            else:
                return False

    def compliesTo(self, v):
        

        if self.type == v.type:
            if self.type == NUMBER:
                if self.value < self.min or self.value > self.max:
                    return False
            return True
        return False

    def __repr__(self):
        return ""Variable(name=%s, type=%s, default=%s, min=%s, max=%s, value=%s)"" % \
               (self.name, self.type, self.default, self.min, self.max, self.value)
","


from __future__ import absolute_import, division, print_function, unicode_literals
import itertools as it
import six
import math
import operator
from six.moves import zip_longest
from six import next
if six.PY2:  
    import collections as collections_abc
else:
    from collections import abc as collections_abc


class chunks(object):
    

    def __init__(self, items, chunksize=None, nchunks=None, total=None,
                 bordermode='none'):
        if nchunks is not None and chunksize is not None:  
            raise ValueError('Cannot specify both chunksize and nchunks')
        if nchunks is None and chunksize is None:  
            raise ValueError('Must specify either chunksize or nchunks')
        if nchunks is not None:
            if total is None:
                total = len(items)
            chunksize = int(math.ceil(total / nchunks))

        self.bordermode = bordermode
        self.items = items
        self.chunksize = chunksize
        self.total = total

    def __len__(self):
        if self.total is None:
            self.total = len(self.items)
        nchunks = int(math.ceil(self.total / self.chunksize))
        return nchunks

    def __iter__(self):
        bordermode = self.bordermode
        items = self.items
        chunksize = self.chunksize
        if bordermode is None or bordermode == 'none':
            return self.noborder(items, chunksize)
        elif bordermode == 'cycle':
            return self.cycle(items, chunksize)
        elif bordermode == 'replicate':
            return self.replicate(items, chunksize)
        else:
            raise ValueError('unknown bordermode=%r' % (bordermode,))

    @staticmethod
    def noborder(items, chunksize):
        
        
        sentinal = object()
        copied_iters = [iter(items)] * chunksize
        chunks_with_sentinals = zip_longest(*copied_iters, fillvalue=sentinal)
        
        for chunk in chunks_with_sentinals:
            yield [item for item in chunk if item is not sentinal]

    @staticmethod
    def cycle(items, chunksize):
        sentinal = object()
        copied_iters = [iter(items)] * chunksize
        chunks_with_sentinals = zip_longest(*copied_iters, fillvalue=sentinal)
        
        bordervalues = it.cycle(iter(items))
        for chunk in chunks_with_sentinals:
            yield [item if item is not sentinal else next(bordervalues)
                   for item in chunk]

    @staticmethod
    def replicate(items, chunksize):
        sentinal = object()
        copied_iters = [iter(items)] * chunksize
        
        chunks_with_sentinals = zip_longest(*copied_iters, fillvalue=sentinal)
        for chunk in chunks_with_sentinals:
            filt_chunk = [item for item in chunk if item is not sentinal]
            if len(filt_chunk) == chunksize:
                yield filt_chunk
            else:
                sizediff = (chunksize - len(filt_chunk))
                padded_chunk = filt_chunk + [filt_chunk[-1]] * sizediff
                yield padded_chunk


def iterable(obj, strok=False):
    

    try:
        iter(obj)
    except Exception:
        return False
    else:
        return strok or not isinstance(obj, six.string_types)


def take(items, indices):
    

    return (items[index] for index in indices)


def compress(items, flags):
    

    return it.compress(items, flags)


def flatten(nested_list):
    

    return it.chain.from_iterable(nested_list)


def unique(items, key=None):
    

    seen = set()
    if key is None:
        for item in items:
            if item not in seen:
                seen.add(item)
                yield item
    else:
        for item in items:
            norm = key(item)
            if norm not in seen:
                seen.add(norm)
                yield item


def argunique(items, key=None):
    

    
    if key is None:
        return unique(range(len(items)), key=lambda i: items[i])
    else:
        return unique(range(len(items)), key=lambda i: key(items[i]))


def unique_flags(items, key=None):
    

    len_ = len(items)
    if key is None:
        item_to_index = dict(zip(reversed(items), reversed(range(len_))))
        indices = item_to_index.values()
    else:
        indices = argunique(items, key=key)
    flags = boolmask(indices, len_)
    return flags


def boolmask(indices, maxval=None):
    

    if maxval is None:
        indices = list(indices)
        maxval = max(indices) + 1
    mask = [False] * maxval
    for index in indices:
        mask[index] = True
    return mask


def iter_window(iterable, size=2, step=1, wrap=False):
    

    
    iter_list = it.tee(iterable, size)
    if wrap:
        
        iter_list = [iter_list[0]] + list(map(it.cycle, iter_list[1:]))
    
    try:
        for count, iter_ in enumerate(iter_list[1:], start=1):
            for _ in range(count):
                next(iter_)
    except StopIteration:
        return iter(())
    else:
        _window_iter = zip(*iter_list)
        
        window_iter = it.islice(_window_iter, 0, None, step)
        return window_iter


def allsame(iterable, eq=operator.eq):
    

    iter_ = iter(iterable)
    try:
        first = next(iter_)
    except StopIteration:
        return True
    return all(eq(first, item) for item in iter_)


def argsort(indexable, key=None, reverse=False):
    

    
    if isinstance(indexable, collections_abc.Mapping):
        vk_iter = ((v, k) for k, v in indexable.items())
    else:
        vk_iter = ((v, k) for k, v in enumerate(indexable))
    
    if key is None:
        indices = [k for v, k in sorted(vk_iter, reverse=reverse)]
    else:
        
        indices = [k for v, k in sorted(vk_iter, key=lambda vk: key(vk[0]),
                                        reverse=reverse)]
    return indices


def argmax(indexable, key=None):
    

    if key is None and isinstance(indexable, collections_abc.Mapping):
        return max(indexable.items(), key=operator.itemgetter(1))[0]
    elif hasattr(indexable, 'index'):
        if key is None:
            return indexable.index(max(indexable))
        else:
            return indexable.index(max(indexable, key=key))
    else:
        
        return argsort(indexable, key=key)[-1]


def argmin(indexable, key=None):
    

    if key is None and isinstance(indexable, collections_abc.Mapping):
        return min(indexable.items(), key=operator.itemgetter(1))[0]
    elif hasattr(indexable, 'index'):
        if key is None:
            return indexable.index(min(indexable))
        else:
            return indexable.index(min(indexable, key=key))
    else:
        
        return argsort(indexable, key=key)[0]


def peek(iterable):
    

    return next(iter(iterable))
","from objects import *

class TuioProfile(object):
    


    
    address = None

    
    list_label = None

    def __init__(self):
        self.objects = {}
        self.sessions = []

    def set(self, client, message):
        

        raise NotImplementedError

    def alive(self, client, message):
        

        raise NotImplementedError

    def fseq(self, client, message):
        

        client.last_frame = client.current_frame
        client.current_frame = message[3]

    def objs(self):
        

        for obj in self.objects.itervalues():
            if obj.sessionid in self.sessions:
                yield obj

class Tuio2DcurProfile(TuioProfile):
    

    address = ""/tuio/2Dcur""
    list_label = ""cursors""

    def __init__(self):
        super(Tuio2DcurProfile, self).__init__()

    def set(self, client, message):
        sessionid = message[3]
        if sessionid not in self.objects:
            self.objects[sessionid] = Tuio2DCursor(sessionid)
        self.objects[sessionid].update(sessionid, message[4:])

    def alive(self, client, message):
        if client.refreshed():
            self.sessions = message[3:]
            for obj in self.objects.keys():
                if obj not in self.sessions:
                    del self.objects[obj]

class Tuio2DobjProfile(TuioProfile):
    

    address = ""/tuio/2Dobj""
    list_label = ""objects""

    def __init__(self):
        super(Tuio2DobjProfile, self).__init__()

    def set(self, client, message):
        sessionid, objectid = message[3:5]
        if objectid not in self.objects:
            self.objects[objectid] = Tuio2DObject(objectid, sessionid)
        self.objects[objectid].update(sessionid, message[5:])

    def alive(self, client, message):
        if client.refreshed():
            self.sessions = message[3:]
","


from __future__ import division

import collections
import numpy as np
import ode


BodyState = collections.namedtuple(
    'BodyState', 'name position quaternion linear_velocity angular_velocity')


class Registrar(type):
    


    def __init__(cls, name, bases, dct):
        if not hasattr(cls, '_registry'):
            cls._registry = {}
        else:
            key = name.lower()
            for i in range(3, len(name) + 1):
                cls._registry[key[:i]] = cls
        super(Registrar, cls).__init__(name, bases, dct)

    def build(cls, key, *args, **kwargs):
        return cls._registry[key.lower()](*args, **kwargs)


class Body(Registrar(str('Base'), (), {})):
    


    def __init__(self, name, world, density=1000., mass=None, **shape):
        self.name = name
        self.world = world
        self.shape = shape

        m = ode.Mass()
        self.init_mass(m, density, mass)
        self.ode_body = ode.Body(world.ode_world)
        self.ode_body.setMass(m)
        self.ode_geom = getattr(ode, 'Geom%s' % self.__class__.__name__)(
            world.ode_space, **shape)
        self.ode_geom.setBody(self.ode_body)

    def __str__(self):
        return '{0.__class__.__name__} {0.name} at {1}'.format(
            self, self.position.round(3))

    @property
    def mass(self):
        

        return self.ode_body.getMass()

    @property
    def state(self):
        

        return BodyState(self.name,
                         tuple(self.position),
                         tuple(self.quaternion),
                         tuple(self.linear_velocity),
                         tuple(self.angular_velocity))

    @state.setter
    def state(self, state):
        

        assert self.name == state.name, \
            'state name ""{}"" != body name ""{}""'.format(state.name, self.name)
        self.position = state.position
        self.quaternion = state.quaternion
        self.linear_velocity = state.linear_velocity
        self.angular_velocity = state.angular_velocity

    @property
    def position(self):
        

        return np.array(self.ode_body.getPosition())

    @position.setter
    def position(self, position):
        

        self.ode_body.setPosition(tuple(position))

    @property
    def rotation(self):
        

        return np.array(self.ode_body.getRotation()).reshape((3, 3))

    @rotation.setter
    def rotation(self, rotation):
        

        if isinstance(rotation, np.ndarray):
            rotation = rotation.ravel()
        self.ode_body.setRotation(tuple(rotation))

    @property
    def quaternion(self):
        

        return np.array(self.ode_body.getQuaternion())

    @quaternion.setter
    def quaternion(self, quaternion):
        self.ode_body.setQuaternion(tuple(quaternion))

    @property
    def linear_velocity(self):
        

        return np.array(self.ode_body.getLinearVel())

    @linear_velocity.setter
    def linear_velocity(self, velocity):
        

        self.ode_body.setLinearVel(tuple(velocity))

    @property
    def angular_velocity(self):
        

        return np.array(self.ode_body.getAngularVel())

    @angular_velocity.setter
    def angular_velocity(self, velocity):
        

        self.ode_body.setAngularVel(tuple(velocity))

    @property
    def force(self):
        

        return np.array(self.ode_body.getForce())

    @force.setter
    def force(self, force):
        

        self.ode_body.setForce(tuple(force))

    @property
    def torque(self):
        

        return np.array(self.ode_body.getTorque())

    @torque.setter
    def torque(self, torque):
        

        self.ode_body.setTorque(tuple(torque))

    @property
    def is_kinematic(self):
        

        return self.ode_body.isKinematic()

    @is_kinematic.setter
    def is_kinematic(self, is_kinematic):
        

        if is_kinematic:
            self.ode_body.setKinematic()
        else:
            self.ode_body.setDynamic()

    @property
    def follows_gravity(self):
        

        return self.ode_body.getGravityMode()

    @follows_gravity.setter
    def follows_gravity(self, follows_gravity):
        

        self.ode_body.setGravityMode(follows_gravity)

    def rotate_to_body(self, x):
        

        return np.dot(x, self.rotation)

    def body_to_world(self, position):
        

        return np.array(self.ode_body.getRelPointPos(tuple(position)))

    def world_to_body(self, position):
        

        return np.array(self.ode_body.getPosRelPoint(tuple(position)))

    def relative_offset_to_world(self, offset):
        

        return np.array(self.body_to_world(offset * self.dimensions / 2))

    def add_force(self, force, relative=False, position=None, relative_position=None):
        

        b = self.ode_body
        if relative_position is not None:
            op = b.addRelForceAtRelPos if relative else b.addForceAtRelPos
            op(force, relative_position)
        elif position is not None:
            op = b.addRelForceAtPos if relative else b.addForceAtPos
            op(force, position)
        else:
            op = b.addRelForce if relative else b.addForce
            op(force)

    def add_torque(self, torque, relative=False):
        

        op = self.ode_body.addRelTorque if relative else self.ode_body.addTorque
        op(torque)

    def join_to(self, joint, other_body=None, **kwargs):
        

        self.world.join(joint, self, other_body, **kwargs)

    def connect_to(self, joint, other_body, offset=(0, 0, 0), other_offset=(0, 0, 0),
                   **kwargs):
        

        anchor = self.world.move_next_to(self, other_body, offset, other_offset)
        self.world.join(joint, self, other_body, anchor=anchor, **kwargs)


class Box(Body):
    @property
    def lengths(self):
        return self.shape['lengths']

    @property
    def dimensions(self):
        return np.array(self.lengths).squeeze()

    @property
    def volume(self):
        return np.prod(self.lengths)

    def init_mass(self, m, density, mass):
        if mass:
            density = mass / self.volume
        m.setBox(density, *self.lengths)


class Sphere(Body):
    @property
    def radius(self):
        return self.shape['radius']

    @property
    def dimensions(self):
        d = 2 * self.radius
        return np.array([d, d, d]).squeeze()

    @property
    def volume(self):
        return 4 / 3 * np.pi * self.radius ** 3

    def init_mass(self, m, density, mass):
        if mass:
            density = mass / self.volume
        m.setSphere(density, self.radius)


class Cylinder(Body):
    @property
    def radius(self):
        return self.shape['radius']

    @property
    def length(self):
        return self.shape['length']

    @property
    def dimensions(self):
        d = 2 * self.radius
        return np.array([d, d, self.length]).squeeze()

    @property
    def volume(self):
        return self.length * np.pi * self.radius ** 2

    def init_mass(self, m, density, mass):
        if mass:
            density = mass / self.volume
        m.setCylinder(density, 3, self.radius, self.length)


class Capsule(Body):
    @property
    def radius(self):
        return self.shape['radius']

    @property
    def length(self):
        return self.shape['length']

    @property
    def dimensions(self):
        d = 2 * self.radius
        return np.array([d, d, d + self.length]).squeeze()

    @property
    def volume(self):
        return 4 / 3 * np.pi * self.radius ** 3 + \
            self.length * np.pi * self.radius ** 2

    def init_mass(self, m, density, mass):
        if mass:
            density = mass / self.volume
        m.setCapsule(density, 3, self.radius, self.length)


def _get_params(target, param, dof):
    

    return [target.getParam(getattr(ode, 'Param{}{}'.format(param, s)))
            for s in ['', '2', '3'][:dof]]


def _set_params(target, param, values, dof):
    

    if not isinstance(values, (list, tuple, np.ndarray)):
        values = [values] * dof
    assert dof == len(values)
    for s, value in zip(['', '2', '3'][:dof], values):
        target.setParam(getattr(ode, 'Param{}{}'.format(param, s)), value)


class Joint(Registrar(str('Base'), (), {})):
    


    ADOF = 0
    LDOF = 0

    @property
    def feedback(self):
        

        return self.ode_obj.getFeedback()

    @property
    def positions(self):
        

        return [self.ode_obj.getPosition(i) for i in range(self.LDOF)]

    @property
    def position_rates(self):
        

        return [self.ode_obj.getPositionRate(i) for i in range(self.LDOF)]

    @property
    def angles(self):
        

        return [self.ode_obj.getAngle(i) for i in range(self.ADOF)]

    @property
    def angle_rates(self):
        

        return [self.ode_obj.getAngleRate(i) for i in range(self.ADOF)]

    @property
    def axes(self):
        

        return [np.array(self.ode_obj.getAxis(i))
                for i in range(self.ADOF or self.LDOF)]

    @axes.setter
    def axes(self, axes):
        

        assert self.ADOF == len(axes) or self.LDOF == len(axes)
        for i, axis in enumerate(axes):
            if axis is not None:
                self.ode_obj.setAxis(i, 0, axis)

    @property
    def lo_stops(self):
        

        return _get_params(self.ode_obj, 'LoStop', self.ADOF + self.LDOF)

    @lo_stops.setter
    def lo_stops(self, lo_stops):
        

        _set_params(self.ode_obj, 'LoStop', lo_stops, self.ADOF + self.LDOF)

    @property
    def hi_stops(self):
        

        return _get_params(self.ode_obj, 'HiStop', self.ADOF + self.LDOF)

    @hi_stops.setter
    def hi_stops(self, hi_stops):
        

        _set_params(self.ode_obj, 'HiStop', hi_stops, self.ADOF + self.LDOF)

    @property
    def velocities(self):
        

        return _get_params(self.ode_obj, 'Vel', self.ADOF + self.LDOF)

    @velocities.setter
    def velocities(self, velocities):
        

        _set_params(self.ode_obj, 'Vel', velocities, self.ADOF + self.LDOF)

    @property
    def max_forces(self):
        

        return _get_params(self.ode_obj, 'FMax', self.ADOF + self.LDOF)

    @max_forces.setter
    def max_forces(self, max_forces):
        

        _set_params(self.ode_obj, 'FMax', max_forces, self.ADOF + self.LDOF)

    @property
    def erps(self):
        

        return _get_params(self.ode_obj, 'ERP', self.ADOF + self.LDOF)

    @erps.setter
    def erps(self, erps):
        

        _set_params(self.ode_obj, 'ERP', erps, self.ADOF + self.LDOF)

    @property
    def cfms(self):
        

        return _get_params(self.ode_obj, 'CFM', self.ADOF + self.LDOF)

    @cfms.setter
    def cfms(self, cfms):
        

        _set_params(self.ode_obj, 'CFM', cfms, self.ADOF + self.LDOF)

    @property
    def stop_cfms(self):
        

        return _get_params(self.ode_obj, 'StopCFM', self.ADOF + self.LDOF)

    @stop_cfms.setter
    def stop_cfms(self, stop_cfms):
        

        _set_params(self.ode_obj, 'StopCFM', stop_cfms, self.ADOF + self.LDOF)

    @property
    def stop_erps(self):
        

        return _get_params(self.ode_obj, 'StopERP', self.ADOF + self.LDOF)

    @stop_erps.setter
    def stop_erps(self, stop_erps):
        

        _set_params(self.ode_obj, 'StopERP', stop_erps, self.ADOF + self.LDOF)

    def enable_feedback(self):
        

        self.ode_obj.setFeedback(True)

    def disable_feedback(self):
        

        self.ode_obj.setFeedback(False)


class Dynamic(Joint):
    


    def __init__(self, name, world, body_a, body_b=None, feedback=False, dof=3,
                 jointgroup=None):
        self.name = name
        self.ode_obj = self.MOTOR_FACTORY(world.ode_world, jointgroup=jointgroup)
        self.ode_obj.attach(body_a.ode_body, body_b.ode_body if body_b else None)
        self.ode_obj.setNumAxes(dof)
        self.cfms = 1e-8
        if feedback:
            self.enable_feedback()
        else:
            self.disable_feedback()


class AMotor(Dynamic):
    


    MOTOR_FACTORY = ode.AMotor

    def __init__(self, *args, **kwargs):
        mode = kwargs.pop('mode', 'user')
        if isinstance(mode, str):
            mode = ode.AMotorEuler if mode.lower() == 'euler' else ode.AMotorUser
        super(AMotor, self).__init__(*args, **kwargs)
        self.ode_obj.setMode(mode)

    @property
    def ADOF(self):
        

        return self.ode_obj.getNumAxes()

    @property
    def axes(self):
        

        return [np.array(self.ode_obj.getAxis(i)) for i in range(self.ADOF)]

    @axes.setter
    def axes(self, axes):
        

        assert len(axes) == self.ADOF
        for i, ax in enumerate(axes):
            if ax is None:
                continue
            if not isinstance(ax, dict):
                ax = dict(axis=ax)
            self.ode_obj.setAxis(i, ax.get('rel', 0), ax['axis'])

    def add_torques(self, torques):
        

        self.ode_obj.addTorques(*torques)


class LMotor(Dynamic):
    


    MOTOR_FACTORY = ode.LMotor

    @property
    def LDOF(self):
        

        return self.ode_obj.getNumAxes()


class Kinematic(Joint):
    


    def __init__(self, name, world, body_a, body_b=None, anchor=None,
                 feedback=False, jointgroup=None, amotor=True, lmotor=True):
        self.name = name

        build = getattr(ode, '{}Joint'.format(self.__class__.__name__))
        self.ode_obj = build(world.ode_world, jointgroup=jointgroup)
        self.ode_obj.attach(body_a.ode_body, body_b.ode_body if body_b else None)
        if anchor is not None:
            self.ode_obj.setAnchor(tuple(anchor))
            self.ode_obj.setParam(ode.ParamCFM, 0)

        self.amotor = None
        if self.ADOF > 0 and amotor:
            self.amotor = AMotor(name=name + ':amotor',
                                 world=world,
                                 body_a=body_a,
                                 body_b=body_b,
                                 feedback=feedback,
                                 jointgroup=jointgroup,
                                 dof=self.ADOF,
                                 mode='euler' if self.ADOF == 3 else 'user')

        self.lmotor = None
        if self.LDOF > 0 and lmotor:
            self.lmotor = LMotor(name=name + ':lmotor',
                                 world=world,
                                 body_a=body_a,
                                 body_b=body_b,
                                 feedback=feedback,
                                 jointgroup=jointgroup,
                                 dof=self.LDOF)

        if feedback:
            self.enable_feedback()
        else:
            self.disable_feedback()

    def __str__(self):
        return self.name

    @property
    def anchor(self):
        

        return np.array(self.ode_obj.getAnchor())

    @property
    def anchor2(self):
        

        return np.array(self.ode_obj.getAnchor2())

    def add_torques(self, *torques):
        

        self.amotor.add_torques(*torques)


class Fixed(Kinematic):
    ADOF = 0
    LDOF = 0


class Slider(Kinematic):
    ADOF = 0
    LDOF = 1

    @property
    def positions(self):
        

        return [self.ode_obj.getPosition()]

    @property
    def position_rates(self):
        

        return [self.ode_obj.getPositionRate()]

    @property
    def axes(self):
        

        return [np.array(self.ode_obj.getAxis())]

    @axes.setter
    def axes(self, axes):
        

        self.lmotor.axes = [axes[0]]
        self.ode_obj.setAxis(tuple(axes[0]))


class Hinge(Kinematic):
    ADOF = 1
    LDOF = 0

    @property
    def angles(self):
        

        return [self.ode_obj.getAngle()]

    @property
    def angle_rates(self):
        

        return [self.ode_obj.getAngleRate()]

    @property
    def axes(self):
        

        return [np.array(self.ode_obj.getAxis())]

    @axes.setter
    def axes(self, axes):
        

        self.amotor.axes = [axes[0]]
        self.ode_obj.setAxis(tuple(axes[0]))


class Piston(Kinematic):
    ADOF = 1
    LDOF = 1

    @property
    def axes(self):
        

        return [np.array(self.ode_obj.getAxis())]

    @axes.setter
    def axes(self, axes):
        self.amotor.axes = [axes[0]]
        self.lmotor.axes = [axes[0]]
        self.ode_obj.setAxis(axes[0])


class Universal(Kinematic):
    ADOF = 2
    LDOF = 0

    @property
    def axes(self):
        

        return [np.array(self.ode_obj.getAxis1()),
                np.array(self.ode_obj.getAxis2())]

    @axes.setter
    def axes(self, axes):
        self.amotor.axes = [axes[0], axes[1]]
        setters = [self.ode_obj.setAxis1, self.ode_obj.setAxis2]
        for axis, setter in zip(axes, setters):
            if axis is not None:
                setter(tuple(axis))

    @property
    def angles(self):
        

        return [self.ode_obj.getAngle1(), self.ode_obj.getAngle2()]

    @property
    def angle_rates(self):
        

        return [self.ode_obj.getAngle1Rate(), self.ode_obj.getAngle2Rate()]


class Ball(Kinematic):
    ADOF = 3
    LDOF = 0

    def __init__(self, name, *args, **kwargs):
        super(Ball, self).__init__(name, *args, **kwargs)

        
        
        keys = 'name world body_a body_b feedback dof jointgroup'.split()
        self.alimit = AMotor(name + ':alimit', *args, dof=self.ADOF, mode='euler',
                             **{k: v for k, v in kwargs.items() if k in keys})

    @property
    def angles(self):
        return self.alimit.angles

    @property
    def angle_rates(self):
        return self.alimit.angle_rates

    @property
    def axes(self):
        return self.alimit.axes

    @axes.setter
    def axes(self, axes):
        if len(axes) == 2:
            axes = dict(rel=1, axis=axes[0]), None, dict(rel=2, axis=axes[1])
        self.amotor.axes = axes
        self.alimit.axes = axes

    @property
    def lo_stops(self):
        return self.alimit.lo_stops

    @lo_stops.setter
    def lo_stops(self, lo_stops):
        self.alimit.lo_stops = lo_stops

    @property
    def hi_stops(self):
        return self.alimit.hi_stops

    @hi_stops.setter
    def hi_stops(self, hi_stops):
        self.alimit.hi_stops = hi_stops


def make_quaternion(theta, *axis):
    

    x, y, z = axis
    r = np.sqrt(x * x + y * y + z * z)
    st = np.sin(theta / 2.)
    ct = np.cos(theta / 2.)
    return [x * st / r, y * st / r, z * st / r, ct]


def center_of_mass(bodies):
    

    x = np.zeros(3.)
    t = 0.
    for b in bodies:
        m = b.mass
        x += b.body_to_world(m.c) * m.mass
        t += m.mass
    return x / t


class World(object):
    


    def __init__(self, dt=1. / 60, max_angular_speed=20):
        self.ode_world = ode.World()
        self.ode_world.setMaxAngularSpeed(max_angular_speed)
        self.ode_space = ode.QuadTreeSpace((0, 0, 0), (100, 100, 20), 10)
        self.ode_floor = ode.GeomPlane(self.ode_space, (0, 0, 1), 0)
        self.ode_contactgroup = ode.JointGroup()

        self.frame_no = 0
        self.dt = dt
        self.elasticity = 0.1
        self.friction = 2000
        self.gravity = 0, 0, -9.81
        self.cfm = 1e-6
        self.erp = 0.7

        self._bodies = {}
        self._joints = {}

    @property
    def gravity(self):
        

        return self.ode_world.getGravity()

    @gravity.setter
    def gravity(self, gravity):
        

        return self.ode_world.setGravity(gravity)

    @property
    def cfm(self):
        

        return self.ode_world.getCFM()

    @cfm.setter
    def cfm(self, cfm):
        

        return self.ode_world.setCFM(cfm)

    @property
    def erp(self):
        

        return self.ode_world.getERP()

    @erp.setter
    def erp(self, erp):
        

        return self.ode_world.setERP(erp)

    @property
    def bodies(self):
        

        for k in sorted(self._bodies):
            yield self._bodies[k]

    @property
    def joints(self):
        

        for k in sorted(self._joints):
            yield self._joints[k]

    def get_body(self, key):
        

        return self._bodies.get(key, key)

    def get_joint(self, key):
        

        return self._joints.get(key, None)

    def create_body(self, shape, name=None, **kwargs):
        

        shape = shape.lower()
        if name is None:
            for i in range(1 + len(self._bodies)):
                name = '{}{}'.format(shape, i)
                if name not in self._bodies:
                    break
        self._bodies[name] = Body.build(shape, name, self, **kwargs)
        return self._bodies[name]

    def join(self, shape, body_a, body_b=None, name=None, **kwargs):
        

        ba = self.get_body(body_a)
        bb = self.get_body(body_b)
        shape = shape.lower()
        if name is None:
            name = '{}^{}^{}'.format(ba.name, shape, bb.name if bb else '')
        self._joints[name] = Joint.build(
            shape, name, self, body_a=ba, body_b=bb, **kwargs)
        return self._joints[name]

    def move_next_to(self, body_a, body_b, offset_a, offset_b):
        

        ba = self.get_body(body_a)
        bb = self.get_body(body_b)
        if ba is None:
            return bb.relative_offset_to_world(offset_b)
        if bb is None:
            return ba.relative_offset_to_world(offset_a)
        anchor = ba.relative_offset_to_world(offset_a)
        offset = bb.relative_offset_to_world(offset_b)
        bb.position = bb.position + anchor - offset
        return anchor

    def get_body_states(self):
        

        return [b.state for b in self.bodies]

    def set_body_states(self, states):
        

        for state in states:
            self.get_body(state.name).state = state

    def step(self, substeps=2):
        

        self.frame_no += 1
        dt = self.dt / substeps
        for _ in range(substeps):
            self.ode_contactgroup.empty()
            self.ode_space.collide(None, self.on_collision)
            self.ode_world.step(dt)

    def needs_reset(self):
        

        return False

    def reset(self):
        

        pass

    def on_key_press(self, key, modifiers, keymap):
        

        if key == keymap.ENTER:
            self.reset()
            return True

    def are_connected(self, body_a, body_b):
        

        return bool(ode.areConnected(
            self.get_body(body_a).ode_body,
            self.get_body(body_b).ode_body))

    def on_collision(self, args, geom_a, geom_b):
        

        body_a = geom_a.getBody()
        body_b = geom_b.getBody()
        if ode.areConnected(body_a, body_b) or \
           (body_a and body_a.isKinematic()) or \
           (body_b and body_b.isKinematic()):
            return
        for c in ode.collide(geom_a, geom_b):
            c.setBounce(self.elasticity)
            c.setMu(self.friction)
            ode.ContactJoint(self.ode_world, self.ode_contactgroup, c).attach(
                geom_a.getBody(), geom_b.getBody())
","import urllib
import xml.etree.ElementTree as ET

import pandas as pd

from serenata_toolbox import log
from serenata_toolbox.datasets.helpers import (
    save_to_csv,
    translate_column,
    xml_extract_text,
)


class DeputiesDataset:

    URL = 'http://www.camara.leg.br/SitCamaraWS/deputados.asmx/ObterDeputados'

    def fetch(self):
        

        xml = urllib.request.urlopen(self.URL)

        tree = ET.ElementTree(file=xml)
        records = self._parse_deputies(tree.getroot())

        df = pd.DataFrame(records, columns=(
            'congressperson_id',
            'budget_id',
            'condition',
            'congressperson_document',
            'civil_name',
            'congressperson_name',
            'picture_url',
            'gender',
            'state',
            'party',
            'phone_number',
            'email'
        ))
        return self._translate(df)

    @staticmethod
    def _parse_deputies(root):
        for deputy in root:
            yield (
                xml_extract_text(deputy, 'ideCadastro'),
                xml_extract_text(deputy, 'codOrcamento'),
                xml_extract_text(deputy, 'condicao'),
                xml_extract_text(deputy, 'matricula'),
                xml_extract_text(deputy, 'nome'),
                xml_extract_text(deputy, 'nomeParlamentar'),
                xml_extract_text(deputy, 'urlFoto'),
                xml_extract_text(deputy, 'sexo'),
                xml_extract_text(deputy, 'uf'),
                xml_extract_text(deputy, 'partido'),
                xml_extract_text(deputy, 'fone'),
                xml_extract_text(deputy, 'email'),
            )

    @staticmethod
    def _translate(df):
        translate_column(df, 'gender', {
            'masculino': 'male',
            'feminino': 'female',
        })

        translate_column(df, 'condition', {
            'Titular': 'Holder',
            'Suplente': 'Substitute',
        })

        return df


def fetch_deputies(data_dir):
    

    deputies = DeputiesDataset()
    df = deputies.fetch()
    save_to_csv(df, data_dir, ""deputies"")

    holders = df.condition == 'Holder'
    substitutes = df.condition == 'Substitute'
    log.info(""Total deputies:"", len(df))
    log.info(""Holder deputies:"", len(df[holders]))
    log.info(""Substitute deputies:"", len(df[substitutes]))
    return df
","from __future__ import print_function

from collections import deque
from threading import Lock, Thread

import logging
import sys
import time
import traceback

from util import get_ip, get_ip_packet

from scapy.sendrecv import sniff
from scapy.config import conf as scapy_conf


scapy_conf.logLevel = logging.ERROR  


class Stream(object):
    

    def __init__(self, src, dst):
        self._packets = []
        self._src = src
        self._dst = dst
        self._length = 0
        self._remaining = 0
        self._next_seq_id = -1
        self._lock_packets = Lock()

    def __str__(self):
        return '%s<->%s (length: %d, remaining: %d, seq_id: %d)' % (
            self.src, self.dst, self.length, self.remaining, self._next_seq_id)

    @property
    def length(self):
        

        return self._length

    @property
    def remaining(self):
        return self._remaining

    @property
    def src(self):
        return self._src

    @property
    def dst(self):
        return self._dst

    def pop(self, nbytes):
        

        size = 0
        popped = []
        with self._lock_packets:
            while size < nbytes:
                try:
                    packet = self._packets.pop(0)
                    size += len(packet.data.data)
                    self._remaining -= len(packet.data.data)
                    popped.append(packet)
                except IndexError:
                    break
        return popped

    def pop_data(self, nbytes):
        

        last_timestamp = 0
        data = []
        for packet in self.pop(nbytes):
            last_timestamp = packet.timestamp
            data.append(packet.data.data)

        return ''.join(data), last_timestamp

    def push(self, ip_packet):
        


        data_len = len(ip_packet.data.data)
        seq_id = ip_packet.data.seq

        if data_len == 0:
            self._next_seq_id = seq_id
            return False

        
        if self._next_seq_id != -1 and seq_id != self._next_seq_id:
            return False

        self._next_seq_id = seq_id + data_len

        with self._lock_packets:
            
            self._length += len(ip_packet.data.data)
            self._remaining += len(ip_packet.data.data)

            self._packets.append(ip_packet)

        return True


class Dispatcher(Thread):
    

    def __init__(self, packet_queue):
        super(Dispatcher, self).__init__()
        self.setDaemon(True)
        self._queue = packet_queue
        self._streams = {}
        self._handlers = []
        self.start()

    @property
    def empty(self):
        return len(self._queue) == 0

    def add_handler(self, stream_handler):
        if stream_handler is None:
            return

        if stream_handler in self._handlers:
            raise ValueError('handler already registered')

        self._handlers.append(stream_handler)

    def run(self, *args, **kwargs):
        

        while True:
            try:
                timestamp, ip_p = self._queue.popleft()

                src_ip = get_ip(ip_p, ip_p.src)
                dst_ip = get_ip(ip_p, ip_p.dst)

                src = intern('%s:%s' % (src_ip, ip_p.data.sport))
                dst = intern('%s:%s' % (dst_ip, ip_p.data.dport))
                key = intern('%s<->%s' % (src, dst))

                stream = self._streams.get(key)
                if stream is None:
                    stream = Stream(src, dst)
                    self._streams[key] = stream

                
                setattr(ip_p, 'timestamp', timestamp)
                pushed = stream.push(ip_p)

                if not pushed:
                    continue

                
                for handler in self._handlers:
                    try:
                        handler(stream)
                    except Exception as ex:
                        print('handler exception: %s' % ex)
            except Exception:
                time.sleep(0.00001)


class Sniffer(Thread):
    


    def __init__(self, iface, port, stream_handler=None, offline=None, ip=None):
        

        super(Sniffer, self).__init__()
        self.setDaemon(True)

        self._iface = iface
        self._port = port
        self._offline = offline
        self._ip = ip if ip else []
        self._queue = deque()  
        self._dispatcher = Dispatcher(self._queue)

        self._dispatcher.add_handler(stream_handler)

        self._wants_stop = False

        self.start()

    @property
    def dispatcher(self):
        return self._dispatcher

    @property
    def pending_ip_packets(self):
        return len(self._queue)

    def add_handler(self, stream_handler):
        self._dispatcher.add_handler(stream_handler)

    def run(self):
        pfilter = 'port %d' % self._port
        try:
            kwargs = {
                'filter': pfilter,
                'store': 0,
                'prn': self._handle_packet,
                'iface': self._iface,
                'stop_filter': lambda p: self._wants_stop,
                }

            if self._offline:
                kwargs['offline'] = self._offline

            sniff(**kwargs)
        except Exception as ex:
            if 'Not a pcap capture file' in str(ex):
                print('%s is not a valid pcap file' % self._offline)
                return
            print('Error: %s: %s (device: %s)' % (ex, traceback.format_exc(), self._iface))
        finally:
            if self._offline:
                
                while not self._dispatcher.empty:
                    time.sleep(0.1)

    def stop(self, wait_for_stopped=False):
        if not self.isAlive():
            return

        self._wants_stop = True

        if wait_for_stopped:
            while self.isAlive():
                time.sleep(0.01)

    def _handle_packet(self, packet):
        try:
            ip_p = get_ip_packet(packet.load, 0, self._port)
        except ValueError:
            return

        ip_data = getattr(ip_p, 'data', None)
        if ip_data is None:
            return

        if ip_data.sport != self._port and ip_data.dport != self._port:
            return

        if self._ip:
            src_ip = get_ip(ip_p, ip_p.src)
            dst_ip = get_ip(ip_p, ip_p.dst)

            if src_ip not in self._ip and dst_ip not in self._ip:
                return

        self._queue.append((packet.time, ip_p))
","

__all__ = ['savetxt2', 'loadtxt2']

import numpy
import re
import base64
import pickle
import shlex
def savetxt2(fname, X, delimiter=' ', newline='\n', comment_character='
        header='', save_dtype=False, fmt={}):

    

    prefixfmt = {}
    for key in fmt:
            prefixfmt[key] = fmt[key]

    olddtype = X.dtype
    newdtype = flatten_dtype(numpy.dtype([('', (X.dtype, X.shape[1:]))]))
    X = X.view(dtype=newdtype)
    dtype = X.dtype
    X = numpy.atleast_1d(X.squeeze())
    header2 = _mkheader(dtype)
    fmtstr = _mkfmtstr(dtype, prefixfmt, delimiter, _default_fmt)
    if hasattr(fname, 'write'):
        fh = fname
        cleanup = lambda : None
    else:
        fh = file(fname, 'w+')
        cleanup = lambda : fh.close()
    try:
        fh.write (header)
        if header[:-1] != newline:
            fh.write(newline)
        fh.write (comment_character)
        fh.write ('!')
        fh.write (header2)
        fh.write (delimiter)
        fh.write ('*%d' % len(X))
        fh.write(newline)
        if save_dtype:
            fh.write (comment_character)
            fh.write ('?')
            fh.write (base64.b64encode(pickle.dumps(olddtype)))
            fh.write (newline)
        for row in X:
            fh.write(fmtstr % tuple(row))
            fh.write(newline)

        if hasattr(fh, 'flush'):
            fh.flush()
    finally:
        cleanup()
    
def loadtxt2(fname, dtype=None, delimiter=' ', newline='\n', comment_character='
        skiplines=0):
    

    dtypert = [None, None, None]
    def preparedtype(dtype):
        dtypert[0] = dtype
        flatten = flatten_dtype(dtype)
        dtypert[1] = flatten
        dtypert[2] = numpy.dtype([('a', (numpy.int8,
            flatten.itemsize))])
        buf = numpy.empty((), dtype=dtypert[1])
        converters = [_default_conv[flatten[name].char] for name in flatten.names]
        return buf, converters, flatten.names

    def fileiter(fh):
        converters = []
        buf = None

        if dtype is not None:
            buf, converters, names = preparedtype(dtype)
            yield None

        for lineno, line in enumerate(fh):
            if lineno < skiplines: continue
            if line[0] in comment_character:
                if buf is None and line[1] == '?':
                    ddtype = pickle.loads(base64.b64decode(line[2:]))
                    buf, converters, names = preparedtype(ddtype)
                    yield None
                continue
            for word, c, name in zip(line.split(), converters, names):
                buf[name] = c(word)
            buf2 = buf.copy().view(dtype=dtypert[2])
            yield buf2

    if isinstance(fname, basestring):
        fh = file(fh, 'r')
        cleanup = lambda : fh.close()
    else:
        fh = iter(fname)
        cleanup = lambda : None
    try:
        i = fileiter(fh)
        i.next()
        return numpy.fromiter(i, dtype=dtypert[2]).view(dtype=dtypert[0]) 
    finally:
        cleanup()

def test():
    from StringIO import StringIO
    d = numpy.dtype(
        [
           ('a', 'i4'),
           ('b', ([('c', 'S10')], 2)),
           ('d', numpy.dtype([('e', ('i4', 5)), ('f', 'S2')]))
           ])
    a = numpy.zeros(2, d)
    a['d']['e'][0] = [1, 2, 3, 4, 5]
    a['d']['e'][1] = [1, 2, 3, 4, 5]
    s = StringIO()
    savetxt2(s, a, fmt=dict([('a', '0x%.8X'), ('d.e', '%.8d')]), save_dtype=True)
    print s.getvalue()
    print loadtxt2(StringIO(s.getvalue()))
    print loadtxt2(StringIO(s.getvalue()), dtype=d)
    s = StringIO()
    array = numpy.arange(10).reshape(2, 5)
    savetxt2(s, array)
    print s.getvalue()

def _mkheader(dtype):
    return ' '.join(
            ['%d[%s]:%s' % (i, dtype[name].str, name) for i, name in
            enumerate(dtype.names)])

def _mkfmtstr(dtype, prefixfmt, delimiter, defaultfmt):
    l = []
    for name in dtype.names:
        val = None
        for key in prefixfmt:
            if name.startswith(key):
                val = prefixfmt[key]
                break
        if val is None:
            val = defaultfmt[dtype[name].char]
        l.append(val)
    return delimiter.join(l)
    
def _mkvalrow(dtype, row):
    vallist = []
    if dtype.names is None and dtype.base == dtype:
        if len(dtype.shape) == 0:
            vallist.append(row)
        else:
            for i in numpy.ndindex(dtype.shape):
                vallist.append(row[i])
    elif dtype.names is None:
        for i in numpy.ndindex(dtype.shape):
            var = _mkvalrow(dtype.base, row[i])
            vallist += var
    else:
        for field in dtype.names:
            var = _mkvalrow(dtype[field], row[field])
            vallist += var

    return vallist

def _psvalrow(dtype, row, vallist):
    if dtype.names is None and dtype.base == dtype:
        if len(dtype.shape) == 0:
            row[...] = dtype.type(vallist[0])
            vallist = vallist[1:]
        else:
            for i in numpy.ndindex(dtype.shape):
                row[i][...] = dtype.type(vallist[0])
                vallist = vallist[1:]
    elif dtype.names is None:
        for i in numpy.ndindex(dtype.shape):
            vallist = _psvalrow(dtype.base, row[i], vallist)
    else:
        for field in dtype.names:
            vallist = _psvalrow(dtype[field], row[field][...], vallist)

    return vallist

def simplerepr(i):
    if len(i) == 0:
        return ''
    if len(i) == 1:
        return '(' + str(i[0]) + ')'
    return '(' + str(i) + ')'

def flatten_dtype(dtype, _next=None):
    

    types = []
    if _next is None: 
        _next = [0, '']
        primary = True
    else:
        primary = False

    prefix = _next[1]

    if dtype.names is None:
        for i in numpy.ndindex(dtype.shape):
            if dtype.base == dtype:
                types.append(('%s%s' % (prefix, simplerepr(i)), dtype))
                _next[0] += 1
            else:
                _next[1] = '%s%s' % (prefix, simplerepr(i))
                types.extend(flatten_dtype(dtype.base, _next))
    else:
        for field in dtype.names:
            typ_fields = dtype.fields[field]
            if len(prefix) > 0:
                _next[1] = prefix + '.' + field
            else:
                _next[1] = '' + field
            flat_dt = flatten_dtype(typ_fields[0], _next)
            types.extend(flat_dt)

    _next[1] = prefix
    if primary:
        return numpy.dtype(types)
    else:
        return types
    
_default_fmt = {
        'f': '%g'        ,
        'd': '%g'        ,
        'b': '%d'        ,
        'B': '%d'        ,
        'i': '%d'        ,
        'I': '%d'        ,
        'l': '%d'        ,
        'L': '%d'        ,
        'S': '""%s""'        ,
}
_default_conv = {
        'f': float        ,
        'd': float        ,
        'i': lambda x: long(x, base=0),
        'L': lambda x: long(x, base=0),
        'I': lambda x: long(x, base=0),
        'S': lambda x: str(x[1:-1]),
}

if __name__ == '__main__':
    test()

","
























import fnmatch
import locale
import logging
import multiprocessing
import os
import pickle
import random
import sys
import zipfile

from click import progressbar, get_terminal_size
from collections import defaultdict
from datetime import datetime
from itertools import cycle
from os.path import isfile, join, splitext
from urllib.parse import quote as url_quote

from . import image, video, signals
from .image import (process_image, get_exif_tags, get_exif_data, get_size,
                    get_iptc_data)
from .settings import get_thumb
from .utils import (Devnull, copy, check_or_create_dir, url_from_path,
                    read_markdown, cached_property, is_valid_html5_video,
                    get_mime)
from .video import process_video
from .writer import AlbumPageWriter, AlbumListPageWriter


class Media:
    


    type = ''

    def __init__(self, filename, path, settings):
        self.src_filename = self.filename = filename
        self.path = path
        self.settings = settings
        self.ext = os.path.splitext(filename)[1].lower()

        self.src_path = join(settings['source'], path, filename)
        self.dst_path = join(settings['destination'], path, filename)

        self.thumb_name = get_thumb(self.settings, self.filename)
        self.thumb_path = join(settings['destination'], path, self.thumb_name)

        self.logger = logging.getLogger(__name__)
        self._get_metadata()
        
        if not self.title:
            self.title = self.filename
        signals.media_initialized.send(self)

    def __repr__(self):
        return ""<%s>(%r)"" % (self.__class__.__name__, str(self))

    def __str__(self):
        return join(self.path, self.filename)

    @property
    def url(self):
        

        return url_from_path(self.filename)

    @property
    def big(self):
        

        if self.settings['keep_orig']:
            s = self.settings
            if s['use_orig']:
                
                return self.filename
            orig_path = join(s['destination'], self.path, s['orig_dir'])
            check_or_create_dir(orig_path)
            big_path = join(orig_path, self.src_filename)
            if not isfile(big_path):
                copy(self.src_path, big_path, symlink=s['orig_link'],
                     rellink=self.settings['rel_link'])
            return join(s['orig_dir'], self.src_filename)

    @property
    def big_url(self):
        

        if self.big is not None:
            return url_from_path(self.big)

    @property
    def thumbnail(self):
        


        if not isfile(self.thumb_path):
            self.logger.debug('Generating thumbnail for %r', self)
            path = (self.dst_path if os.path.exists(self.dst_path)
                    else self.src_path)
            try:
                
                s = self.settings
                if self.type == 'image':
                    image.generate_thumbnail(
                        path, self.thumb_path, s['thumb_size'],
                        fit=s['thumb_fit'])
                elif self.type == 'video':
                    video.generate_thumbnail(
                        path, self.thumb_path, s['thumb_size'],
                        s['thumb_video_delay'], fit=s['thumb_fit'],
                        converter=s['video_converter'])
            except Exception as e:
                self.logger.error('Failed to generate thumbnail: %s', e)
                return
        return url_from_path(self.thumb_name)

    def _get_metadata(self):
        

        self.description = ''
        self.meta = {}
        self.title = ''

        descfile = splitext(self.src_path)[0] + '.md'
        if isfile(descfile):
            meta = read_markdown(descfile)
            for key, val in meta.items():
                setattr(self, key, val)

    def _get_file_date(self):
        stat = os.stat(self.src_path)
        return datetime.fromtimestamp(stat.st_mtime)


class Image(Media):
    


    type = 'image'

    @cached_property
    def date(self):
        return (self.exif and self.exif.get('dateobj', None) or
                self._get_file_date())

    @cached_property
    def exif(self):
        datetime_format = self.settings['datetime_format']
        return (get_exif_tags(self.raw_exif, datetime_format=datetime_format)
                if self.raw_exif and self.ext in ('.jpg', '.jpeg') else None)

    def _get_metadata(self):
        super(Image, self)._get_metadata()
        
        
        if self.title and self.description:
            
            return

        try:
            iptc_data = get_iptc_data(self.src_path)
        except Exception as e:
            self.logger.warning('Could not read IPTC data from %s: %s',
                                self.src_path, e)
        else:
            if not self.title and iptc_data.get('title'):
                self.title = iptc_data['title']
            if not self.description and iptc_data.get('description'):
                self.description = iptc_data['description']

    @cached_property
    def raw_exif(self):
        try:
            return (get_exif_data(self.src_path)
                    if self.ext in ('.jpg', '.jpeg') else None)
        except Exception as e:
            self.logger.warning('Could not read EXIF data from %s: %s',
                                self.src_path, e)

    @cached_property
    def size(self):
        return get_size(self.dst_path)

    @cached_property
    def thumb_size(self):
        return get_size(self.thumb_path)

    def has_location(self):
        return self.exif is not None and 'gps' in self.exif


class Video(Media):
    


    type = 'video'

    def __init__(self, filename, path, settings):
        super(Video, self).__init__(filename, path, settings)
        base, ext = splitext(filename)
        self.src_filename = filename
        self.date = self._get_file_date()
        if not settings['use_orig'] or not is_valid_html5_video(ext):
            video_format = settings['video_format']
            ext = '.' + video_format
            self.filename = base + ext
            self.mime = get_mime(ext)
            self.dst_path = join(settings['destination'], path, base + ext)
        else:
            self.mime = get_mime(ext)


class Album:
    


    description_file = ""index.md""

    def __init__(self, path, settings, dirnames, filenames, gallery):
        self.path = path
        self.name = path.split(os.path.sep)[-1]
        self.gallery = gallery
        self.settings = settings
        self.subdirs = dirnames
        self.output_file = settings['output_filename']
        self._thumbnail = None

        if path == '.':
            self.src_path = settings['source']
            self.dst_path = settings['destination']
        else:
            self.src_path = join(settings['source'], path)
            self.dst_path = join(settings['destination'], path)

        self.logger = logging.getLogger(__name__)
        self._get_metadata()

        
        self.url_ext = self.output_file if settings['index_in_url'] else ''

        self.index_url = url_from_path(os.path.relpath(
            settings['destination'], self.dst_path)) + '/' + self.url_ext

        
        
        self.medias = medias = []
        self.medias_count = defaultdict(int)

        for f in filenames:
            ext = splitext(f)[1]
            if ext.lower() in settings['img_extensions']:
                media = Image(f, self.path, settings)
            elif ext.lower() in settings['video_extensions']:
                media = Video(f, self.path, settings)
            else:
                continue

            self.medias_count[media.type] += 1
            medias.append(media)

        signals.album_initialized.send(self)

    def __repr__(self):
        return ""<%s>(path=%r, title=%r)"" % (self.__class__.__name__, self.path,
                                            self.title)

    def __str__(self):
        return ('{} : '.format(self.path) +
                ', '.join('{} {}s'.format(count, _type)
                          for _type, count in self.medias_count.items()))

    def __len__(self):
        return len(self.medias)

    def __iter__(self):
        return iter(self.medias)

    def _get_metadata(self):
        

        descfile = join(self.src_path, self.description_file)
        self.description = ''
        self.meta = {}
        
        self.title = os.path.basename(self.path if self.path != '.'
                                      else self.src_path)

        if isfile(descfile):
            meta = read_markdown(descfile)
            for key, val in meta.items():
                setattr(self, key, val)

        try:
            self.author = self.meta['author'][0]
        except KeyError:
            self.author = self.settings.get('author')

    def create_output_directories(self):
        

        check_or_create_dir(self.dst_path)

        if self.medias:
            check_or_create_dir(join(self.dst_path,
                                     self.settings['thumb_dir']))

        if self.medias and self.settings['keep_orig']:
            self.orig_path = join(self.dst_path, self.settings['orig_dir'])
            check_or_create_dir(self.orig_path)

    def sort_subdirs(self, albums_sort_attr):
        if self.subdirs:
            if albums_sort_attr:
                root_path = self.path if self.path != '.' else ''
                if albums_sort_attr.startswith(""meta.""):
                    meta_key = albums_sort_attr.split(""."", 1)[1]
                    key = lambda s: locale.strxfrm(
                        self.gallery.albums[join(root_path, s)].meta.get(meta_key, [''])[0])
                else:
                    key = lambda s: locale.strxfrm(
                        getattr(self.gallery.albums[join(root_path, s)],
                                albums_sort_attr))
            else:
                key = locale.strxfrm

            self.subdirs.sort(key=key,
                              reverse=self.settings['albums_sort_reverse'])

        signals.albums_sorted.send(self)

    def sort_medias(self, medias_sort_attr):
        if self.medias:
            if medias_sort_attr == 'date':
                key = lambda s: s.date or datetime.now()
            elif medias_sort_attr.startswith('meta.'):
                meta_key = medias_sort_attr.split(""."", 1)[1]
                key = lambda s: locale.strxfrm(s.meta.get(meta_key, [''])[0])
            else:
                key = lambda s: locale.strxfrm(getattr(s, medias_sort_attr))

            self.medias.sort(key=key,
                             reverse=self.settings['medias_sort_reverse'])

        signals.medias_sorted.send(self)

    @property
    def images(self):
        

        for media in self.medias:
            if media.type == 'image':
                yield media

    @property
    def videos(self):
        

        for media in self.medias:
            if media.type == 'video':
                yield media

    @property
    def albums(self):
        

        root_path = self.path if self.path != '.' else ''
        return [self.gallery.albums[join(root_path, path)]
                for path in self.subdirs]

    @property
    def url(self):
        

        url = self.name.encode('utf-8')
        return url_quote(url) + '/' + self.url_ext

    @property
    def thumbnail(self):
        


        if self._thumbnail:
            
            return self._thumbnail

        
        thumbnail = self.meta.get('thumbnail', [''])[0]

        if thumbnail and isfile(join(self.src_path, thumbnail)):
            self._thumbnail = url_from_path(join(
                self.name, get_thumb(self.settings, thumbnail)))
            self.logger.debug(""Thumbnail for %r : %s"", self, self._thumbnail)
            return self._thumbnail
        else:
            
            for f in self.medias:
                ext = splitext(f.filename)[1]
                if ext.lower() in self.settings['img_extensions']:
                    
                    
                    size = f.size
                    if size is None:
                        size = get_size(f.src_path)

                    if size['width'] > size['height']:
                        self._thumbnail = (url_quote(self.name) + '/' +
                                           f.thumbnail)
                        self.logger.debug(
                            ""Use 1st landscape image as thumbnail for %r : %s"",
                            self, self._thumbnail)
                        return self._thumbnail

            
            if not self._thumbnail and self.medias:
                for media in self.medias:
                    if media.thumbnail is not None:
                        self._thumbnail = (url_quote(self.name) + '/' +
                                           media.thumbnail)
                        break
                else:
                    self.logger.warning(""No thumbnail found for %r"", self)
                    return None

                self.logger.debug(""Use the 1st image as thumbnail for %r : %s"",
                                  self, self._thumbnail)
                return self._thumbnail

            
            if not self._thumbnail:
                for path, album in self.gallery.get_albums(self.path):
                    if album.thumbnail:
                        self._thumbnail = (url_quote(self.name) + '/' +
                                           album.thumbnail)
                        self.logger.debug(
                            ""Using thumbnail from sub-directory for %r : %s"",
                            self, self._thumbnail)
                        return self._thumbnail

        self.logger.error('Thumbnail not found for %r', self)
        return None

    @property
    def random_thumbnail(self):
        try:
            return url_from_path(join(self.name,
                                      random.choice(self.medias).thumbnail))
        except IndexError:
            return self.thumbnail

    @property
    def breadcrumb(self):
        

        if self.path == '.':
            return []

        path = self.path
        breadcrumb = [((self.url_ext or '.'), self.title)]

        while True:
            path = os.path.normpath(os.path.join(path, '..'))
            if path == '.':
                break

            url = (url_from_path(os.path.relpath(path, self.path)) + '/' +
                   self.url_ext)
            breadcrumb.append((url, self.gallery.albums[path].title))

        breadcrumb.reverse()
        return breadcrumb

    @property
    def show_map(self):
        

        return any(image.has_location() for image in self.images)

    @cached_property
    def zip(self):
        

        zip_gallery = self.settings['zip_gallery']

        if zip_gallery and len(self) > 0:
            zip_gallery = zip_gallery.format(album=self)
            archive_path = join(self.dst_path, zip_gallery)
            if (self.settings.get('zip_skip_if_exists', False) and
                    isfile(archive_path)):
                self.logger.debug(""Archive %s already created, passing"",
                                  archive_path)
                return zip_gallery

            archive = zipfile.ZipFile(archive_path, 'w', allowZip64=True)
            attr = ('src_path' if self.settings['zip_media_format'] == 'orig'
                    else 'dst_path')

            for p in self:
                path = getattr(p, attr)
                try:
                    archive.write(path, os.path.split(path)[1])
                except OSError as e:
                    self.logger.warn('Failed to add %s to the ZIP: %s', p, e)

            archive.close()
            self.logger.debug('Created ZIP archive %s', archive_path)
            return zip_gallery


class Gallery(object):

    def __init__(self, settings, ncpu=None):
        self.settings = settings
        self.logger = logging.getLogger(__name__)
        self.stats = defaultdict(int)
        self.init_pool(ncpu)
        check_or_create_dir(settings['destination'])

        
        albums = self.albums = {}
        src_path = self.settings['source']

        ignore_dirs = settings['ignore_directories']
        ignore_files = settings['ignore_files']

        progressChars = cycle([""/"", ""-"", ""\\"", ""|""])
        show_progress = (self.logger.getEffectiveLevel() >= logging.WARNING and
                         os.isatty(sys.stdout.fileno()))
        self.progressbar_target = None if show_progress else Devnull()

        for path, dirs, files in os.walk(src_path, followlinks=True,
                                         topdown=False):
            if show_progress:
                print(""\rCollecting albums "" + next(progressChars), end="""")
            relpath = os.path.relpath(path, src_path)

            
            if ignore_dirs and any(fnmatch.fnmatch(relpath, ignore)
                                   for ignore in ignore_dirs):
                self.logger.info('Ignoring %s', relpath)
                continue

            
            if ignore_files:
                files_path = {join(relpath, f) for f in files}
                for ignore in ignore_files:
                    files_path -= set(fnmatch.filter(files_path, ignore))

                self.logger.debug('Files before filtering: %r', files)
                files = [os.path.split(f)[1] for f in files_path]
                self.logger.debug('Files after filtering: %r', files)

            
            
            
            for d in dirs[:]:
                path = join(relpath, d) if relpath != '.' else d
                if path not in albums.keys():
                    dirs.remove(d)

            album = Album(relpath, settings, dirs, files, self)

            if not album.medias and not album.albums:
                self.logger.info('Skip empty album: %r', album)
            else:
                album.create_output_directories()
                albums[relpath] = album

        print(""\rCollecting albums, done."")

        with progressbar(albums.values(), label=""%16s"" % ""Sorting albums"",
                         file=self.progressbar_target) as progress_albums:
            for album in progress_albums:
                album.sort_subdirs(settings['albums_sort_attr'])

        with progressbar(albums.values(), label=""%16s"" % ""Sorting media"",
                         file=self.progressbar_target) as progress_albums:
            for album in progress_albums:
                album.sort_medias(settings['medias_sort_attr'])

        self.logger.debug('Albums:\n%r', albums.values())
        signals.gallery_initialized.send(self)

    @property
    def title(self):
        

        return self.settings['title'] or self.albums['.'].title

    def init_pool(self, ncpu):
        try:
            cpu_count = multiprocessing.cpu_count()
        except NotImplementedError:
            cpu_count = 1

        if ncpu is None:
            ncpu = cpu_count
        else:
            try:
                ncpu = int(ncpu)
            except ValueError:
                self.logger.error('ncpu should be an integer value')
                ncpu = cpu_count

        self.logger.info(""Using %s cores"", ncpu)
        if ncpu > 1:
            self.pool = multiprocessing.Pool(processes=ncpu)
        else:
            self.pool = None

    def get_albums(self, path):
        


        for name in self.albums[path].subdirs:
            subdir = os.path.normpath(join(path, name))
            yield subdir, self.albums[subdir]
            for subname, album in self.get_albums(subdir):
                yield subname, self.albums[subdir]

    def build(self, force=False):
        ""Create the image gallery""

        if not self.albums:
            self.logger.warning(""No albums found."")
            return

        def log_func(x):
            
            available_length = get_terminal_size()[0] - 64
            if x and available_length > 10:
                return x.name[:available_length]
            else:
                return """"

        try:
            with progressbar(self.albums.values(), label=""Collecting files"",
                             item_show_func=log_func, show_eta=False,
                             file=self.progressbar_target) as albums:
                media_list = [f for album in albums
                              for f in self.process_dir(album, force=force)]
        except KeyboardInterrupt:
            sys.exit('Interrupted')

        bar_opt = {'label': ""Processing files"",
                   'show_pos': True,
                   'file': self.progressbar_target}
        failed_files = []

        if self.pool:
            try:
                with progressbar(length=len(media_list), **bar_opt) as bar:
                    for res in self.pool.imap_unordered(worker, media_list):
                        if res:
                            failed_files.append(res)
                        bar.update(1)
                self.pool.close()
                self.pool.join()
            except KeyboardInterrupt:
                self.pool.terminate()
                sys.exit('Interrupted')
            except pickle.PicklingError:
                self.logger.critical(
                    ""Failed to process files with the multiprocessing feature.""
                    "" This can be caused by some module import or object ""
                    ""defined in the settings file, which can't be serialized."",
                    exc_info=True)
                sys.exit('Abort')
        else:
            with progressbar(media_list, **bar_opt) as medias:
                for media_item in medias:
                    res = process_file(media_item)
                    if res:
                        failed_files.append(res)

        if failed_files:
            self.remove_files(failed_files)

        if self.settings['write_html']:
            album_writer = AlbumPageWriter(self.settings,
                                           index_title=self.title)
            album_list_writer = AlbumListPageWriter(self.settings,
                                                    index_title=self.title)
            with progressbar(self.albums.values(),
                             label=""%16s"" % ""Writing files"",
                             item_show_func=log_func, show_eta=False,
                             file=self.progressbar_target) as albums:
                for album in albums:
                    if album.albums:
                        if album.medias:
                            self.logger.warning(
                                ""Album %s contains sub-albums and images. ""
                                ""Please move images to their own sub-album. ""
                                ""Images in album %s will not be visible."",
                                album.title, album.title
                            )
                        album_list_writer.write(album)
                    else:
                        album_writer.write(album)
        print('')

        signals.gallery_build.send(self)

    def remove_files(self, files):
        self.logger.error('Some files have failed to be processed:')
        for path, filename in files:
            self.logger.error('  - %s/%s', path, filename)
            album = self.albums[path]
            for f in album.medias:
                if f.filename == filename:
                    self.stats[f.type + '_failed'] += 1
                    album.medias.remove(f)
                    break
        self.logger.error('You can run ""sigal build"" in verbose (--verbose) or'
                          ' debug (--debug) mode to get more details.')

    def process_dir(self, album, force=False):
        

        for f in album:
            if isfile(f.dst_path) and not force:
                self.logger.info(""%s exists - skipping"", f.filename)
                self.stats[f.type + '_skipped'] += 1
            else:
                self.stats[f.type] += 1
                yield (f.type, f.path, f.filename, f.src_path, album.dst_path,
                       self.settings)


def process_file(args):
    
    processor = process_image if args[0] == 'image' else process_video
    ret = processor(*args[3:])
    
    
    return args[1:3] if ret else None


def worker(args):
    try:
        return process_file(args)
    except KeyboardInterrupt:
        pass
","import sys

import netaddr
from neutron.common import config
from neutron import context as neutron_context
from oslo_config import cfg
from oslo_log import log as logging
import requests
from sqlalchemy import not_

from quark.db import api as db_api
from quark.db import ip_types
from quark.db import models


CONF = cfg.CONF
LOG = logging.getLogger(__name__)
LOCK_NAME = ""null-routed""

null_routes_opts = [
    cfg.StrOpt(""null_routes_url"",
               help=_(""URL to GET null routes data"")),
    cfg.StrOpt(""null_routes_region"",
               help=_(""region to filter null routes data"")),
    cfg.ListOpt(""null_routes_network_ids"",
                default=[""00000000-0000-0000-0000-000000000000""],
                help=_(""UUIDs of networks to query for null-routed IP ""
                       ""addresses""))
]

CONF.register_opts(null_routes_opts, ""QUARK"")


def main():
    config.init(sys.argv[1:])
    if not cfg.CONF.config_file:
        sys.exit(_(""ERROR: Unable to find configuration file via the default""
                   "" search paths (~/.neutron/, ~/, /etc/neutron/, /etc/) and""
                   "" the '--config-file' option!""))
    config.setup_logging()

    
    from quark import network_strategy
    network_strategy.STRATEGY.load()

    context = neutron_context.get_admin_context()
    network_ids = cfg.CONF.QUARK.null_routes_network_ids
    ipset = get_subnets_cidr_set(context, network_ids)

    url = cfg.CONF.QUARK.null_routes_url
    region = cfg.CONF.QUARK.null_routes_region
    addresses = get_null_routes_addresses(url, region, ipset)

    delete_locks(context, network_ids, addresses)
    create_locks(context, network_ids, addresses)


def get_subnets_cidr_set(context, network_ids):
    ipset = netaddr.IPSet()
    subnets = db_api.subnet_find(context, network_id=network_ids,
                                 shared=[False])
    for subnet in subnets:
        net = netaddr.IPNetwork(subnet[""cidr""])
        ipset.add(net)
    return ipset


def _make_request(url, region):
    response = requests.get(url, verify=False)
    data = response.json()

    
    assert len(data) == 1
    assert sorted(data[0].keys()) == sorted([
        ""paginate"", ""request"", ""payload"", ""response""])
    assert sorted(data[0][""paginate""].keys()) == sorted([
        ""total_count"", ""total_count_display"", ""total_pages"",
        ""author_comment"", ""per_page"", ""page""])
    
    
    assert len(data[0][""payload""]) == data[0][""paginate""][""total_count""]

    return data


def get_null_routes_addresses(url, region, ipset):
    data = _make_request(url, region)
    addresses = netaddr.IPSet()
    for datum in data[0][""payload""]:
        assert sorted(datum.keys()) == sorted([
            ""status"", ""note"", ""updated"", ""name"", ""status_name"",
            ""region.id"", ""ip"", ""idql"", ""discovered"", ""netmask"", ""tag"",
            ""conf"", ""cidr"", ""id"", ""switch.hostname""])
        net = netaddr.IPNetwork(datum[""cidr""])
        if datum[""region.id""] != region or datum[""status""] != ""1"":
            continue
        for addr in net:
            if addr in ipset:
                addresses.add(addr)
    return addresses


def _to_int(addr):
    return int(addr.ipv6())


def _find_addresses_to_be_unlocked(context, network_ids, addresses):
    addresses = [_to_int(address) for address in addresses]
    query = context.session.query(models.IPAddress)
    query = query.filter(models.IPAddress.network_id.in_(network_ids))
    if addresses:
        query = query.filter(not_(models.IPAddress.address.in_(addresses)))
    query = query.filter(not_(models.IPAddress.lock_id.is_(None)))
    return query.all()


def delete_locks(context, network_ids, addresses):
    

    addresses_no_longer_null_routed = _find_addresses_to_be_unlocked(
        context, network_ids, addresses)
    LOG.info(""Deleting %s lock holders on IPAddress with ids: %s"",
             len(addresses_no_longer_null_routed),
             [addr.id for addr in addresses_no_longer_null_routed])

    for address in addresses_no_longer_null_routed:
        lock_holder = None
        try:
            lock_holder = db_api.lock_holder_find(
                context, lock_id=address.lock_id, name=LOCK_NAME,
                scope=db_api.ONE)
            if lock_holder:
                db_api.lock_holder_delete(context, address, lock_holder)
        except Exception:
            LOG.exception(""Failed to delete lock holder %s"", lock_holder)
            continue
    context.session.flush()


def _find_or_create_address(context, network_ids, address):
    address_model = db_api.ip_address_find(
        context,
        network_id=network_ids, address=_to_int(address), scope=db_api.ONE)
    if not address_model:
        query = context.session.query(models.Subnet)
        query = query.filter(models.Subnet.network_id.in_(network_ids))
        query = query.filter(models.Subnet.ip_version == address.version)
        query = query.filter(_to_int(address) >= models.Subnet.first_ip)
        query = query.filter(_to_int(address) <= models.Subnet.last_ip)
        subnet = query.one()
        address_model = db_api.ip_address_create(
            context,
            address=address,
            subnet_id=subnet[""id""],
            version=subnet[""ip_version""],
            network_id=subnet[""network_id""],
            address_type=ip_types.FIXED)
        address_model[""deallocated""] = 1
        context.session.add(address_model)
    return address_model


def create_locks(context, network_ids, addresses):
    


    for address in addresses:
        address_model = None
        try:
            address_model = _find_or_create_address(
                context, network_ids, address)
            lock_holder = None
            if address_model.lock_id:
                lock_holder = db_api.lock_holder_find(
                    context,
                    lock_id=address_model.lock_id, name=LOCK_NAME,
                    scope=db_api.ONE)

            if not lock_holder:
                LOG.info(""Creating lock holder on IPAddress %s with id %s"",
                         address_model.address_readable,
                         address_model.id)
                db_api.lock_holder_create(
                    context, address_model, name=LOCK_NAME, type=""ip_address"")
        except Exception:
            LOG.exception(""Failed to create lock holder on IPAddress %s"",
                          address_model)
            continue
    context.session.flush()
","


















from __future__ import print_function

import os
import sys
import datetime

from xml.dom.minidom import parse

from harvestingkit.minidom_utils import (get_value_in_tag,
                                         xml_to_text)
from harvestingkit.utils import (
    collapse_initials,
    fix_title_capitalization,
    convert_html_subscripts_to_latex,
    safe_title
)
from harvestingkit.bibrecord import (
    record_add_field,
    create_record,
    record_xml_output,
)
from harvestingkit.jats_package import JatsPackage


class DateNotFoundException(Exception):

    



class WorldScientific(JatsPackage):

    


    def __init__(self, journal_mappings={}):
        

        self.url_prefix = ""http://www.worldscientific.com/doi/pdf""
        super(WorldScientific, self).__init__(journal_mappings)

    def _get_date(self):
        def _extract_date(date):
            year = get_value_in_tag(date, 'year')
            month = get_value_in_tag(date, 'month').zfill(2)
            month = month if month != '00' else '01'
            day = get_value_in_tag(date, 'day').zfill(2)
            day = day if day != '00' else '01'
            return '%s-%s-%s' % (year, month, day)

        for date in self.document.getElementsByTagName('date'):
            if date.getAttribute('date-type') == 'published':
                return _extract_date(date)
        for date in self.document.getElementsByTagName('pub-date'):
            if date.getAttribute('pub-type') == 'ppub':
                return _extract_date(date)
        for date in self.document.getElementsByTagName('pub-date'):
            if date.getAttribute('pub-type') == 'epub':
                return _extract_date(date)
        for date in self.document.getElementsByTagName('pub-date'):
            if not date.getAttribute('pub-type'):
                return _extract_date(date)

        
        raise DateNotFoundException

    def get_date(self, filename):
        

        try:
            self.document = parse(filename)
            return self._get_date()
        except DateNotFoundException:
            print(""Date problem found in {0}"".format(filename))
            return datetime.datetime.strftime(datetime.datetime.now(),
                                              ""%Y-%m-%d"")

    def _get_authors(self):
        authors = []
        for contrib in self.document.getElementsByTagName('contrib'):
            if contrib.getAttribute('contrib-type') == 'author':
                surname = get_value_in_tag(contrib, 'surname')
                given_names = get_value_in_tag(contrib, 'given-names')
                given_names = collapse_initials(given_names)
                name = '%s, %s' % (surname, given_names)
                name = safe_title(name)
                affiliations = []
                for aff in contrib.getElementsByTagName('aff'):
                    affiliations.append(xml_to_text(aff))
                emails = []
                for email in contrib.getElementsByTagName('email'):
                    emails.append(xml_to_text(email))
                collaborations = []
                for collaboration in contrib.getElementsByTagName(""collab""):
                    collaborations.append(xml_to_text(collaboration))
                authors.append((name, affiliations, emails, collaborations))
        return authors

    def _add_authors(self, rec):
        authors = self._get_authors()
        first_author = True
        collaboration_added = False
        for author in authors:
            subfields = [('a', author[0])]
            if author[1]:
                for aff in author[1]:
                    subfields.append(('v', aff))
            if author[2]:
                for email in author[2]:
                    subfields.append(('m', email))
            if first_author:
                record_add_field(rec, '100', subfields=subfields)
                first_author = False
            else:
                record_add_field(rec, '700', subfields=subfields)
            if author[3] and not collaboration_added:
                collaborations = []
                for collab in author[3]:
                    collab_stripped = collab.replace(""for the"", """").strip()
                    if collab_stripped not in collaborations:
                        collaborations.append(collab_stripped)
                        record_add_field(rec, '710', subfields=[(""g"", collab_stripped)])
                collaboration_added = True

    def _get_related_article(self):
        for tag in self.document.getElementsByTagName('related-article'):
            if tag.getAttribute('ext-link-type') == 'doi':
                attributes = tag.attributes.keysNS()
                for attribute in attributes:
                    if attribute[1] == 'href':
                        return tag.getAttributeNS(*attribute)
        return ''

    def get_collection(self, journal):
        

        conference = ''
        for tag in self.document.getElementsByTagName('conference'):
            conference = xml_to_text(tag)
        if conference or journal == ""International Journal of Modern Physics: Conference Series"":
            return [('a', 'HEP'), ('a', 'ConferencePaper')]
        elif self._get_article_type() == ""review-article"":
            return [('a', 'HEP'), ('a', 'Review')]
        else:
            return [('a', 'HEP'), ('a', 'Published')]

    def get_record(self, filename, ref_extract_callback=None):
        

        self.document = parse(filename)

        article_type = self._get_article_type()
        if article_type not in ['research-article',
                                'corrected-article',
                                'original-article',
                                'introduction',
                                'letter',
                                'correction',
                                'addendum',
                                'review-article',
                                'rapid-communications']:
            return """"

        rec = create_record()
        title, subtitle, notes = self._get_title()
        subfields = []
        if subtitle:
            subfields.append(('b', subtitle))
        if title:
            title = fix_title_capitalization(title)
            subfields.append(('a', title))
            record_add_field(rec, '245', subfields=subfields)
        for note_id in notes:
            note = self._get_note(note_id)
            if note:
                record_add_field(rec, '500', subfields=[('a', note)])
        keywords = self._get_keywords()
        for keyword in keywords:
            record_add_field(rec, '653', ind1='1', subfields=[('a', keyword),
                                                              ('9', 'author')])
        journal, volume, issue, year, date, doi, page,\
            fpage, lpage = self._get_publication_information()
        if date:
            record_add_field(rec, '260', subfields=[('c', date),
                                                    ('t', 'published')])
        if doi:
            record_add_field(rec, '024', ind1='7', subfields=[('a', doi),
                                                              ('2', 'DOI')])
        abstract = self._get_abstract()
        if abstract:
            abstract = convert_html_subscripts_to_latex(abstract)
            record_add_field(rec, '520', subfields=[('a', abstract),
                                                    ('9', 'World Scientific')])
        license, license_type, license_url = self._get_license()
        subfields = []
        if license:
            subfields.append(('a', license))
        if license_url:
            subfields.append(('u', license_url))
        if subfields:
            record_add_field(rec, '540', subfields=subfields)
        if license_type == 'open-access':
            self._attach_fulltext(rec, doi)
        number_of_pages = self._get_page_count()
        if number_of_pages:
            record_add_field(rec, '300', subfields=[('a', number_of_pages)])
        c_holder, c_year, c_statement = self._get_copyright()
        if c_holder and c_year:
            record_add_field(rec, '542', subfields=[('d', c_holder),
                                                    ('g', c_year),
                                                    ('e', 'Article')])
        elif c_statement:
            record_add_field(rec, '542', subfields=[('f', c_statement),
                                                    ('e', 'Article')])
        subfields = []
        if journal:
            subfields.append(('p', journal))
        if issue:
            subfields.append(('n', issue))
        if volume:
            subfields.append(('v', volume))
        if fpage and lpage:
            subfields.append(('c', '%s-%s' % (fpage,
                                              lpage)))
        elif page:
            subfields.append(('c', page))
        if year:
            subfields.append(('y', year))
        if article_type == 'correction':
            subfields.append(('m', 'Erratum'))
        elif article_type == 'addendum':
            subfields.append(('m', 'Addendum'))
        record_add_field(rec, '773', subfields=subfields)

        collections = self.get_collection(journal)
        for collection in collections:
            record_add_field(rec, '980', subfields=[collection])

        self._add_authors(rec)
        if article_type in ['correction',
                            'addendum']:
            related_article = self._get_related_article()
            if related_article:
                record_add_field(rec, '024', ind1='7', subfields=[('a', related_article),
                                                                  ('2', 'DOI')])
        try:
            return record_xml_output(rec)
        except UnicodeDecodeError:
            message = ""Found a bad char in the file for the article "" + doi
            sys.stderr.write(message)
            return """"

    def _attach_fulltext(self, rec, doi):
        

        url = os.path.join(self.url_prefix, doi)
        record_add_field(rec, 'FFT',
                         subfields=[('a', url),
                                    ('t', 'INSPIRE-PUBLIC'),
                                    ('d', 'Fulltext')])
if __name__ == '__main__':
    filename = sys.argv[1]
    ws = WorldScientific()
    print(ws.get_record(filename))
","




from __future__ import absolute_import, division

from optlang.symbolics import Zero

from cobra.flux_analysis.parsimonious import pfba


def room(model, solution=None, linear=False, delta=0.03, epsilon=1E-03):
    

    with model:
        add_room(model=model, solution=solution, linear=linear, delta=delta,
                 epsilon=epsilon)
        solution = model.optimize()
    return solution


def add_room(model, solution=None, linear=False, delta=0.03, epsilon=1E-03):
    r


    if 'room_old_objective' in model.solver.variables:
        raise ValueError('model is already adjusted for ROOM')

    
    if solution is None:
        solution = pfba(model)

    prob = model.problem
    variable = prob.Variable(""room_old_objective"", ub=solution.objective_value)
    constraint = prob.Constraint(
        model.solver.objective.expression - variable,
        ub=0.0,
        lb=0.0,
        name=""room_old_objective_constraint""
    )
    model.objective = prob.Objective(Zero, direction=""min"", sloppy=True)
    vars_and_cons = [variable, constraint]
    obj_vars = []

    for rxn in model.reactions:
        flux = solution.fluxes[rxn.id]

        if linear:
            y = prob.Variable(""y_"" + rxn.id, lb=0, ub=1)
            delta = epsilon = 0.0
        else:
            y = prob.Variable(""y_"" + rxn.id, type=""binary"")

        
        w_u = flux + (delta * abs(flux)) + epsilon
        upper_const = prob.Constraint(
            rxn.flux_expression - y * (rxn.upper_bound - w_u),
            ub=w_u, name=""room_constraint_upper_"" + rxn.id)
        
        w_l = flux - (delta * abs(flux)) - epsilon
        lower_const = prob.Constraint(
            rxn.flux_expression - y * (rxn.lower_bound - w_l),
            lb=w_l, name=""room_constraint_lower_"" + rxn.id)
        vars_and_cons.extend([y, upper_const, lower_const])
        obj_vars.append(y)

    model.add_cons_vars(vars_and_cons)
    model.objective.set_linear_coefficients({v: 1.0 for v in obj_vars})
","





import librosa
import pyrubberband as pyrb
import re
import numpy as np
import six
from copy import deepcopy

from ..base import BaseTransformer

__all__ = ['PitchShift', 'RandomPitchShift', 'LinearPitchShift']


def transpose(label, n_semitones):
    


    
    match = re.match(six.text_type('(?P<note>[A-G][b
                     six.text_type(label))

    if not match:
        return label

    note = match.group('note')

    new_note = librosa.midi_to_note(librosa.note_to_midi(note) + n_semitones,
                                    octave=False)

    return new_note + match.group('mod')


class AbstractPitchShift(BaseTransformer):
    


    def __init__(self):
        


        BaseTransformer.__init__(self)

        
        self._register('key_mode|chord|chord_harte', self.deform_note)
        self._register('pitch_contour', self.deform_contour)
        self._register('pitch_hz', self.deform_hz)
        self._register('pitch_midi', self.deform_midi)
        self._register('chord_roman|pitch_class', self.deform_tonic)

    def states(self, jam):
        mudabox = jam.sandbox.muda
        state = dict(tuning=librosa.estimate_tuning(y=mudabox._audio['y'],
                                                    sr=mudabox._audio['sr']))
        yield state

    @staticmethod
    def audio(mudabox, state):
        mudabox._audio['y'] = pyrb.pitch_shift(mudabox._audio['y'],
                                               mudabox._audio['sr'],
                                               state['n_semitones'])

    @staticmethod
    def deform_contour(annotation, state):
        scale = 2.0**(state['n_semitones']/12.0)
        for obs in annotation.pop_data():
            annotation.append(time=obs.time, duration=obs.duration,
                              confidence=obs.confidence,
                              value={'index':obs.value['index'],
                                     'frequency':scale*obs.value['frequency'],
                                     'voiced':obs.value['voiced']})

    @staticmethod
    def deform_hz(annotation, state):
        scale = 2.0**(state['n_semitones']/12.0)
        for obs in annotation.pop_data():
                    annotation.append(time=obs.time, duration=obs.duration,
                                      confidence=obs.confidence,
                                      value=scale * obs.value)

    @staticmethod
    def deform_midi(annotation, state):
        for obs in annotation.pop_data():
            annotation.append(time=obs.time, duration=obs.duration,
                              confidence=obs.confidence,
                              value=obs.value + state['n_semitones'])

    @staticmethod
    def deform_tonic(annotation, state):
        
        if -0.5 < (state['tuning'] + state['n_semitones']) <= 0.5:
            
            
            return

        for obs in annotation.pop_data():
            value = deepcopy(obs.value)
            value['tonic'] = transpose(value['tonic'], state['n_semitones'])
            annotation.append(time=obs.time, duration=obs.duration,
                              confidence=obs.confidence,
                              value=value)

    @staticmethod
    def deform_note(annotation, state):
        
        if -0.5 < (state['tuning'] + state['n_semitones']) <= 0.5:
            
            
            return

        for obs in annotation.pop_data():
            annotation.append(time=obs.time, duration=obs.duration,
                              confidence=obs.confidence,
                              value=transpose(obs.value, state['n_semitones']))


class PitchShift(AbstractPitchShift):
    


    def __init__(self, n_semitones=1):
        AbstractPitchShift.__init__(self)
        self.n_semitones = np.atleast_1d(n_semitones).flatten().tolist()

    def states(self, jam):
        for state in AbstractPitchShift.states(self, jam):
            for semitones in self.n_semitones:
                state['n_semitones'] = semitones
                yield state


class RandomPitchShift(AbstractPitchShift):
    

    def __init__(self, n_samples=3, mean=0.0, sigma=1.0):
        AbstractPitchShift.__init__(self)

        if sigma <= 0:
            raise ValueError('sigma must be strictly positive')

        if n_samples <= 0:
            raise ValueError('n_samples must be None or positive')

        self.n_samples = n_samples
        self.mean = float(mean)
        self.sigma = float(sigma)

    def states(self, jam):
        
        for state in AbstractPitchShift.states(self, jam):
            for _ in range(self.n_samples):
                state['n_semitones'] = np.random.normal(loc=self.mean,
                                                        scale=self.sigma,
                                                        size=None)
                yield state


class LinearPitchShift(AbstractPitchShift):
    

    def __init__(self, n_samples=3, lower=-1, upper=1):
        AbstractPitchShift.__init__(self)

        if upper <= lower:
            raise ValueError('upper must be strictly larger than lower')

        if n_samples <= 0:
            raise ValueError('n_samples must be strictly positive')

        self.n_samples = n_samples
        self.lower = float(lower)
        self.upper = float(upper)

    def states(self, jam):
        shifts = np.linspace(self.lower,
                             self.upper,
                             num=self.n_samples,
                             endpoint=True)

        for state in AbstractPitchShift.states(self, jam):
            for n_semitones in shifts:
                state['n_semitones'] = n_semitones
                yield state
","

from __future__ import absolute_import

from collections import namedtuple

import numpy as np
import pandas as pd
from six import iteritems


try:
    from scipy.sparse import dok_matrix, lil_matrix
except ImportError:
    dok_matrix, lil_matrix = None, None


def create_stoichiometric_matrix(model, array_type='dense', dtype=None):
    

    if array_type not in ('DataFrame', 'dense') and not dok_matrix:
        raise ValueError('Sparse matrices require scipy')

    if dtype is None:
        dtype = np.float64

    array_constructor = {
        'dense': np.zeros, 'dok': dok_matrix, 'lil': lil_matrix,
        'DataFrame': np.zeros,
    }

    n_metabolites = len(model.metabolites)
    n_reactions = len(model.reactions)
    array = array_constructor[array_type]((n_metabolites, n_reactions),
                                          dtype=dtype)

    m_ind = model.metabolites.index
    r_ind = model.reactions.index

    for reaction in model.reactions:
        for metabolite, stoich in iteritems(reaction.metabolites):
            array[m_ind(metabolite), r_ind(reaction)] = stoich

    if array_type == 'DataFrame':
        metabolite_ids = [met.id for met in model.metabolites]
        reaction_ids = [rxn.id for rxn in model.reactions]
        return pd.DataFrame(array, index=metabolite_ids, columns=reaction_ids)

    else:
        return array


def nullspace(A, atol=1e-13, rtol=0):
    

    A = np.atleast_2d(A)
    u, s, vh = np.linalg.svd(A)
    tol = max(atol, rtol * s[0])
    nnz = (s >= tol).sum()
    ns = vh[nnz:].conj().T
    return ns


def constraint_matrices(model, array_type='dense', include_vars=False,
                        zero_tol=1e-6):
    

    if array_type not in ('DataFrame', 'dense') and not dok_matrix:
        raise ValueError('Sparse matrices require scipy')

    array_builder = {
        'dense': np.array, 'dok': dok_matrix, 'lil': lil_matrix,
        'DataFrame': pd.DataFrame,
    }[array_type]

    Problem = namedtuple(""Problem"",
                         [""equalities"", ""b"", ""inequalities"", ""bounds"",
                          ""variable_fixed"", ""variable_bounds""])
    equality_rows = []
    inequality_rows = []
    inequality_bounds = []
    b = []

    for const in model.constraints:
        lb = -np.inf if const.lb is None else const.lb
        ub = np.inf if const.ub is None else const.ub
        equality = (ub - lb) < zero_tol
        coefs = const.get_linear_coefficients(model.variables)
        coefs = [coefs[v] for v in model.variables]
        if equality:
            b.append(lb if abs(lb) > zero_tol else 0.0)
            equality_rows.append(coefs)
        else:
            inequality_rows.append(coefs)
            inequality_bounds.append([lb, ub])

    var_bounds = np.array([[v.lb, v.ub] for v in model.variables])
    fixed = var_bounds[:, 1] - var_bounds[:, 0] < zero_tol

    results = Problem(
        equalities=array_builder(equality_rows),
        b=np.array(b),
        inequalities=array_builder(inequality_rows),
        bounds=array_builder(inequality_bounds),
        variable_fixed=np.array(fixed),
        variable_bounds=array_builder(var_bounds))

    return results
","
































__author__ = 'kenton@google.com (Kenton Varda)'

import io
import re

import six

if six.PY3:
  long = int

from typy.google.protobuf.internal import type_checkers
from typy.google.protobuf import descriptor
from typy.google.protobuf import text_encoding

__all__ = ['MessageToString', 'PrintMessage', 'PrintField',
           'PrintFieldValue', 'Merge']


_INTEGER_CHECKERS = (type_checkers.Uint32ValueChecker(),
                     type_checkers.Int32ValueChecker(),
                     type_checkers.Uint64ValueChecker(),
                     type_checkers.Int64ValueChecker())
_FLOAT_INFINITY = re.compile('-?inf(?:inity)?f?', re.IGNORECASE)
_FLOAT_NAN = re.compile('nanf?', re.IGNORECASE)
_FLOAT_TYPES = frozenset([descriptor.FieldDescriptor.CPPTYPE_FLOAT,
                          descriptor.FieldDescriptor.CPPTYPE_DOUBLE])
_QUOTES = frozenset((
))


class Error(Exception):
  



class ParseError(Error):
  



class TextWriter(object):
  def __init__(self, as_utf8):
    if six.PY2:
      self._writer = io.BytesIO()
    else:
      self._writer = io.StringIO()

  def write(self, val):
    if six.PY2:
      if isinstance(val, six.text_type):
        val = val.encode('utf-8')
    return self._writer.write(val)

  def close(self):
    return self._writer.close()

  def getvalue(self):
    return self._writer.getvalue()


def MessageToString(message, as_utf8=False, as_one_line=False,
                    pointy_brackets=False, use_index_order=False,
                    float_format=None, use_field_number=False):
  

  out = TextWriter(as_utf8)
  printer = _Printer(out, 0, as_utf8, as_one_line,
                     pointy_brackets, use_index_order, float_format,
                     use_field_number)
  printer.PrintMessage(message)
  result = out.getvalue()
  out.close()
  if as_one_line:
    return result.rstrip()
  return result


def _IsMapEntry(field):
  return (field.type == descriptor.FieldDescriptor.TYPE_MESSAGE and
          field.message_type.has_options and
          field.message_type.GetOptions().map_entry)


def PrintMessage(message, out, indent=0, as_utf8=False, as_one_line=False,
                 pointy_brackets=False, use_index_order=False,
                 float_format=None, use_field_number=False):
  printer = _Printer(out, indent, as_utf8, as_one_line,
                     pointy_brackets, use_index_order, float_format,
                     use_field_number)
  printer.PrintMessage(message)


def PrintField(field, value, out, indent=0, as_utf8=False, as_one_line=False,
               pointy_brackets=False, use_index_order=False, float_format=None):
  

  printer = _Printer(out, indent, as_utf8, as_one_line,
                     pointy_brackets, use_index_order, float_format)
  printer.PrintField(field, value)


def PrintFieldValue(field, value, out, indent=0, as_utf8=False,
                    as_one_line=False, pointy_brackets=False,
                    use_index_order=False,
                    float_format=None):
  

  printer = _Printer(out, indent, as_utf8, as_one_line,
                     pointy_brackets, use_index_order, float_format)
  printer.PrintFieldValue(field, value)


class _Printer(object):
  


  def __init__(self, out, indent=0, as_utf8=False, as_one_line=False,
               pointy_brackets=False, use_index_order=False, float_format=None,
               use_field_number=False):
    

    self.out = out
    self.indent = indent
    self.as_utf8 = as_utf8
    self.as_one_line = as_one_line
    self.pointy_brackets = pointy_brackets
    self.use_index_order = use_index_order
    self.float_format = float_format
    self.use_field_number = use_field_number

  def PrintMessage(self, message):
    

    fields = message.ListFields()
    if self.use_index_order:
      fields.sort(key=lambda x: x[0].index)
    for field, value in fields:
      if _IsMapEntry(field):
        for key in sorted(value):
          
          
          
          
          
          entry_submsg = field.message_type._concrete_class(
              key=key, value=value[key])
          self.PrintField(field, entry_submsg)
      elif field.label == descriptor.FieldDescriptor.LABEL_REPEATED:
        for element in value:
          self.PrintField(field, element)
      else:
        self.PrintField(field, value)

  def PrintField(self, field, value):
    

    out = self.out
    out.write(' ' * self.indent)
    if self.use_field_number:
      out.write(str(field.number))
    else:
      if field.is_extension:
        out.write('[')
        if (field.containing_type.GetOptions().message_set_wire_format and
            field.type == descriptor.FieldDescriptor.TYPE_MESSAGE and
            field.label == descriptor.FieldDescriptor.LABEL_OPTIONAL):
          out.write(field.message_type.full_name)
        else:
          out.write(field.full_name)
        out.write(']')
      elif field.type == descriptor.FieldDescriptor.TYPE_GROUP:
        
        out.write(field.message_type.name)
      else:
        out.write(field.name)

    if field.cpp_type != descriptor.FieldDescriptor.CPPTYPE_MESSAGE:
      
      
      out.write(': ')

    self.PrintFieldValue(field, value)
    if self.as_one_line:
      out.write(' ')
    else:
      out.write('\n')

  def PrintFieldValue(self, field, value):
    

    out = self.out
    if self.pointy_brackets:
      openb = '<'
      closeb = '>'
    else:
      openb = '{'
      closeb = '}'

    if field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_MESSAGE:
      if self.as_one_line:
        out.write(' %s ' % openb)
        self.PrintMessage(value)
        out.write(closeb)
      else:
        out.write(' %s\n' % openb)
        self.indent += 2
        self.PrintMessage(value)
        self.indent -= 2
        out.write(' ' * self.indent + closeb)
    elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_ENUM:
      enum_value = field.enum_type.values_by_number.get(value, None)
      if enum_value is not None:
        out.write(enum_value.name)
      else:
        out.write(str(value))
    elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_STRING:
      out.write('\""')
      if isinstance(value, six.text_type):
        out_value = value.encode('utf-8')
      else:
        out_value = value
      if field.type == descriptor.FieldDescriptor.TYPE_BYTES:
        
        out_as_utf8 = False
      else:
        out_as_utf8 = self.as_utf8
      out.write(text_encoding.CEscape(out_value, out_as_utf8))
      out.write('\""')
    elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_BOOL:
      if value:
        out.write('true')
      else:
        out.write('false')
    elif field.cpp_type in _FLOAT_TYPES and self.float_format is not None:
      out.write('{1:{0}}'.format(self.float_format, value))
    else:
      out.write(str(value))


def Parse(text, message,
          allow_unknown_extension=False, allow_field_number=False):
  

  if not isinstance(text, str):
    text = text.decode('utf-8')
  return ParseLines(text.split('\n'), message, allow_unknown_extension,
                    allow_field_number)


def Merge(text, message, allow_unknown_extension=False,
          allow_field_number=False):
  

  return MergeLines(text.split('\n'), message, allow_unknown_extension,
                    allow_field_number)


def ParseLines(lines, message, allow_unknown_extension=False,
               allow_field_number=False):
  

  parser = _Parser(allow_unknown_extension, allow_field_number)
  return parser.ParseLines(lines, message)


def MergeLines(lines, message, allow_unknown_extension=False,
               allow_field_number=False):
  

  parser = _Parser(allow_unknown_extension, allow_field_number)
  return parser.MergeLines(lines, message)


class _Parser(object):
  


  def __init__(self, allow_unknown_extension=False, allow_field_number=False):
    self.allow_unknown_extension = allow_unknown_extension
    self.allow_field_number = allow_field_number

  def ParseFromString(self, text, message):
    

    if not isinstance(text, str):
      text = text.decode('utf-8')
    return self.ParseLines(text.split('\n'), message)

  def ParseLines(self, lines, message):
    

    self._allow_multiple_scalars = False
    self._ParseOrMerge(lines, message)
    return message

  def MergeFromString(self, text, message):
    

    return self._MergeLines(text.split('\n'), message)

  def MergeLines(self, lines, message):
    

    self._allow_multiple_scalars = True
    self._ParseOrMerge(lines, message)
    return message

  def _ParseOrMerge(self, lines, message):
    

    tokenizer = _Tokenizer(lines)
    while not tokenizer.AtEnd():
      self._MergeField(tokenizer, message)

  def _MergeField(self, tokenizer, message):
    

    message_descriptor = message.DESCRIPTOR
    if (hasattr(message_descriptor, 'syntax') and
        message_descriptor.syntax == 'proto3'):
      
      
      self._allow_multiple_scalars = True
    if tokenizer.TryConsume('['):
      name = [tokenizer.ConsumeIdentifier()]
      while tokenizer.TryConsume('.'):
        name.append(tokenizer.ConsumeIdentifier())
      name = '.'.join(name)

      if not message_descriptor.is_extendable:
        raise tokenizer.ParseErrorPreviousToken(
            'Message type ""%s"" does not have extensions.' %
            message_descriptor.full_name)
      
      field = message.Extensions._FindExtensionByName(name)
      
      if not field:
        if self.allow_unknown_extension:
          field = None
        else:
          raise tokenizer.ParseErrorPreviousToken(
              'Extension ""%s"" not registered.' % name)
      elif message_descriptor != field.containing_type:
        raise tokenizer.ParseErrorPreviousToken(
            'Extension ""%s"" does not extend message type ""%s"".' % (
                name, message_descriptor.full_name))

      tokenizer.Consume(']')

    else:
      name = tokenizer.ConsumeIdentifier()
      if self.allow_field_number and name.isdigit():
        number = ParseInteger(name, True, True)
        field = message_descriptor.fields_by_number.get(number, None)
        if not field and message_descriptor.is_extendable:
          field = message.Extensions._FindExtensionByNumber(number)
      else:
        field = message_descriptor.fields_by_name.get(name, None)

        
        
        
        if not field:
          field = message_descriptor.fields_by_name.get(name.lower(), None)
          if field and field.type != descriptor.FieldDescriptor.TYPE_GROUP:
            field = None

        if (field and field.type == descriptor.FieldDescriptor.TYPE_GROUP and
            field.message_type.name != name):
          field = None

      if not field:
        raise tokenizer.ParseErrorPreviousToken(
            'Message type ""%s"" has no field named ""%s"".' % (
                message_descriptor.full_name, name))

    if field:
      if not self._allow_multiple_scalars and field.containing_oneof:
        
        
        
        which_oneof = message.WhichOneof(field.containing_oneof.name)
        if which_oneof is not None and which_oneof != field.name:
          raise tokenizer.ParseErrorPreviousToken(
              'Field ""%s"" is specified along with field ""%s"", another member '
              'of oneof ""%s"" for message type ""%s"".' % (
                  field.name, which_oneof, field.containing_oneof.name,
                  message_descriptor.full_name))

      if field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_MESSAGE:
        tokenizer.TryConsume(':')
        merger = self._MergeMessageField
      else:
        tokenizer.Consume(':')
        merger = self._MergeScalarField

      if (field.label == descriptor.FieldDescriptor.LABEL_REPEATED
          and tokenizer.TryConsume('[')):
        
        while True:
          merger(tokenizer, message, field)
          if tokenizer.TryConsume(']'): break
          tokenizer.Consume(',')

      else:
        merger(tokenizer, message, field)

    else:  
      assert self.allow_unknown_extension
      _SkipFieldContents(tokenizer)

    
    
    if not tokenizer.TryConsume(','):
      tokenizer.TryConsume(';')

  def _MergeMessageField(self, tokenizer, message, field):
    

    is_map_entry = _IsMapEntry(field)

    if tokenizer.TryConsume('<'):
      end_token = '>'
    else:
      tokenizer.Consume('{')
      end_token = '}'

    if field.label == descriptor.FieldDescriptor.LABEL_REPEATED:
      if field.is_extension:
        sub_message = message.Extensions[field].add()
      elif is_map_entry:
        
        sub_message = field.message_type._concrete_class()
      else:
        sub_message = getattr(message, field.name).add()
    else:
      if field.is_extension:
        sub_message = message.Extensions[field]
      else:
        sub_message = getattr(message, field.name)
      sub_message.SetInParent()

    while not tokenizer.TryConsume(end_token):
      if tokenizer.AtEnd():
        raise tokenizer.ParseErrorPreviousToken('Expected ""%s"".' % (end_token,))
      self._MergeField(tokenizer, sub_message)

    if is_map_entry:
      value_cpptype = field.message_type.fields_by_name['value'].cpp_type
      if value_cpptype == descriptor.FieldDescriptor.CPPTYPE_MESSAGE:
        value = getattr(message, field.name)[sub_message.key]
        value.MergeFrom(sub_message.value)
      else:
        getattr(message, field.name)[sub_message.key] = sub_message.value

  def _MergeScalarField(self, tokenizer, message, field):
    

    _ = self.allow_unknown_extension
    value = None

    if field.type in (descriptor.FieldDescriptor.TYPE_INT32,
                      descriptor.FieldDescriptor.TYPE_SINT32,
                      descriptor.FieldDescriptor.TYPE_SFIXED32):
      value = tokenizer.ConsumeInt32()
    elif field.type in (descriptor.FieldDescriptor.TYPE_INT64,
                        descriptor.FieldDescriptor.TYPE_SINT64,
                        descriptor.FieldDescriptor.TYPE_SFIXED64):
      value = tokenizer.ConsumeInt64()
    elif field.type in (descriptor.FieldDescriptor.TYPE_UINT32,
                        descriptor.FieldDescriptor.TYPE_FIXED32):
      value = tokenizer.ConsumeUint32()
    elif field.type in (descriptor.FieldDescriptor.TYPE_UINT64,
                        descriptor.FieldDescriptor.TYPE_FIXED64):
      value = tokenizer.ConsumeUint64()
    elif field.type in (descriptor.FieldDescriptor.TYPE_FLOAT,
                        descriptor.FieldDescriptor.TYPE_DOUBLE):
      value = tokenizer.ConsumeFloat()
    elif field.type == descriptor.FieldDescriptor.TYPE_BOOL:
      value = tokenizer.ConsumeBool()
    elif field.type == descriptor.FieldDescriptor.TYPE_STRING:
      value = tokenizer.ConsumeString()
    elif field.type == descriptor.FieldDescriptor.TYPE_BYTES:
      value = tokenizer.ConsumeByteString()
    elif field.type == descriptor.FieldDescriptor.TYPE_ENUM:
      value = tokenizer.ConsumeEnum(field)
    else:
      raise RuntimeError('Unknown field type %d' % field.type)

    if field.label == descriptor.FieldDescriptor.LABEL_REPEATED:
      if field.is_extension:
        message.Extensions[field].append(value)
      else:
        getattr(message, field.name).append(value)
    else:
      if field.is_extension:
        if not self._allow_multiple_scalars and message.HasExtension(field):
          raise tokenizer.ParseErrorPreviousToken(
              'Message type ""%s"" should not have multiple ""%s"" extensions.' %
              (message.DESCRIPTOR.full_name, field.full_name))
        else:
          message.Extensions[field] = value
      else:
        if not self._allow_multiple_scalars and message.HasField(field.name):
          raise tokenizer.ParseErrorPreviousToken(
              'Message type ""%s"" should not have multiple ""%s"" fields.' %
              (message.DESCRIPTOR.full_name, field.name))
        else:
          setattr(message, field.name, value)


def _SkipFieldContents(tokenizer):
  

  
  
  
  
  
  
  if tokenizer.TryConsume(':') and not tokenizer.LookingAt(
      '{') and not tokenizer.LookingAt('<'):
    _SkipFieldValue(tokenizer)
  else:
    _SkipFieldMessage(tokenizer)


def _SkipField(tokenizer):
  

  if tokenizer.TryConsume('['):
    
    tokenizer.ConsumeIdentifier()
    while tokenizer.TryConsume('.'):
      tokenizer.ConsumeIdentifier()
    tokenizer.Consume(']')
  else:
    tokenizer.ConsumeIdentifier()

  _SkipFieldContents(tokenizer)

  
  
  if not tokenizer.TryConsume(','):
    tokenizer.TryConsume(';')


def _SkipFieldMessage(tokenizer):
  


  if tokenizer.TryConsume('<'):
    delimiter = '>'
  else:
    tokenizer.Consume('{')
    delimiter = '}'

  while not tokenizer.LookingAt('>') and not tokenizer.LookingAt('}'):
    _SkipField(tokenizer)

  tokenizer.Consume(delimiter)


def _SkipFieldValue(tokenizer):
  

  
  
  if tokenizer.TryConsumeByteString():
    while tokenizer.TryConsumeByteString():
      pass
    return

  if (not tokenizer.TryConsumeIdentifier() and
      not tokenizer.TryConsumeInt64() and
      not tokenizer.TryConsumeUint64() and
      not tokenizer.TryConsumeFloat()):
    raise ParseError('Invalid field value: ' + tokenizer.token)


class _Tokenizer(object):
  


  _WHITESPACE = re.compile('(\\s|(
  _TOKEN = re.compile('|'.join([
      r'[a-zA-Z_][0-9a-zA-Z_+-]*',             
      r'([0-9+-]|(\.[0-9]))[0-9a-zA-Z_.+-]*',  
  ] + [                                        
      r'{qt}([^{qt}\n\\]|\\.)*({qt}|\\?$)'.format(qt=mark) for mark in _QUOTES
  ]))

  _IDENTIFIER = re.compile(r'\w+')

  def __init__(self, lines):
    self._position = 0
    self._line = -1
    self._column = 0
    self._token_start = None
    self.token = ''
    self._lines = iter(lines)
    self._current_line = ''
    self._previous_line = 0
    self._previous_column = 0
    self._more_lines = True
    self._SkipWhitespace()
    self.NextToken()

  def LookingAt(self, token):
    return self.token == token

  def AtEnd(self):
    

    return not self.token

  def _PopLine(self):
    while len(self._current_line) <= self._column:
      try:
        self._current_line = next(self._lines)
      except StopIteration:
        self._current_line = ''
        self._more_lines = False
        return
      else:
        self._line += 1
        self._column = 0

  def _SkipWhitespace(self):
    while True:
      self._PopLine()
      match = self._WHITESPACE.match(self._current_line, self._column)
      if not match:
        break
      length = len(match.group(0))
      self._column += length

  def TryConsume(self, token):
    

    if self.token == token:
      self.NextToken()
      return True
    return False

  def Consume(self, token):
    

    if not self.TryConsume(token):
      raise self._ParseError('Expected ""%s"".' % token)

  def TryConsumeIdentifier(self):
    try:
      self.ConsumeIdentifier()
      return True
    except ParseError:
      return False

  def ConsumeIdentifier(self):
    

    result = self.token
    if not self._IDENTIFIER.match(result):
      raise self._ParseError('Expected identifier.')
    self.NextToken()
    return result

  def ConsumeInt32(self):
    

    try:
      result = ParseInteger(self.token, is_signed=True, is_long=False)
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def ConsumeUint32(self):
    

    try:
      result = ParseInteger(self.token, is_signed=False, is_long=False)
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def TryConsumeInt64(self):
    try:
      self.ConsumeInt64()
      return True
    except ParseError:
      return False

  def ConsumeInt64(self):
    

    try:
      result = ParseInteger(self.token, is_signed=True, is_long=True)
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def TryConsumeUint64(self):
    try:
      self.ConsumeUint64()
      return True
    except ParseError:
      return False

  def ConsumeUint64(self):
    

    try:
      result = ParseInteger(self.token, is_signed=False, is_long=True)
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def TryConsumeFloat(self):
    try:
      self.ConsumeFloat()
      return True
    except ParseError:
      return False

  def ConsumeFloat(self):
    

    try:
      result = ParseFloat(self.token)
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def ConsumeBool(self):
    

    try:
      result = ParseBool(self.token)
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def TryConsumeByteString(self):
    try:
      self.ConsumeByteString()
      return True
    except ParseError:
      return False

  def ConsumeString(self):
    

    the_bytes = self.ConsumeByteString()
    try:
      return six.text_type(the_bytes, 'utf-8')
    except UnicodeDecodeError as e:
      raise self._StringParseError(e)

  def ConsumeByteString(self):
    

    the_list = [self._ConsumeSingleByteString()]
    while self.token and self.token[0] in _QUOTES:
      the_list.append(self._ConsumeSingleByteString())
    return b''.join(the_list)

  def _ConsumeSingleByteString(self):
    

    text = self.token
    if len(text) < 1 or text[0] not in _QUOTES:
      raise self._ParseError('Expected string but found: %r' % (text,))

    if len(text) < 2 or text[-1] != text[0]:
      raise self._ParseError('String missing ending quote: %r' % (text,))

    try:
      result = text_encoding.CUnescape(text[1:-1])
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def ConsumeEnum(self, field):
    try:
      result = ParseEnum(field, self.token)
    except ValueError as e:
      raise self._ParseError(str(e))
    self.NextToken()
    return result

  def ParseErrorPreviousToken(self, message):
    

    return ParseError('%d:%d : %s' % (
        self._previous_line + 1, self._previous_column + 1, message))

  def _ParseError(self, message):
    

    return ParseError('%d:%d : %s' % (
        self._line + 1, self._column + 1, message))

  def _StringParseError(self, e):
    return self._ParseError('Couldn\'t parse string: ' + str(e))

  def NextToken(self):
    

    self._previous_line = self._line
    self._previous_column = self._column

    self._column += len(self.token)
    self._SkipWhitespace()

    if not self._more_lines:
      self.token = ''
      return

    match = self._TOKEN.match(self._current_line, self._column)
    if match:
      token = match.group(0)
      self.token = token
    else:
      self.token = self._current_line[self._column]


def ParseInteger(text, is_signed=False, is_long=False):
  

  
  try:
    
    
    
    if is_long:
      result = long(text, 0)
    else:
      result = int(text, 0)
  except ValueError:
    raise ValueError('Couldn\'t parse integer: %s' % text)

  
  checker = _INTEGER_CHECKERS[2 * int(is_long) + int(is_signed)]
  checker.CheckValue(result)
  return result


def ParseFloat(text):
  

  try:
    
    return float(text)
  except ValueError:
    
    if _FLOAT_INFINITY.match(text):
      if text[0] == '-':
        return float('-inf')
      else:
        return float('inf')
    elif _FLOAT_NAN.match(text):
      return float('nan')
    else:
      
      try:
        return float(text.rstrip('f'))
      except ValueError:
        raise ValueError('Couldn\'t parse float: %s' % text)


def ParseBool(text):
  

  if text in ('true', 't', '1'):
    return True
  elif text in ('false', 'f', '0'):
    return False
  else:
    raise ValueError('Expected ""true"" or ""false"".')


def ParseEnum(field, value):
  

  enum_descriptor = field.enum_type
  try:
    number = int(value, 0)
  except ValueError:
    
    enum_value = enum_descriptor.values_by_name.get(value, None)
    if enum_value is None:
      raise ValueError(
          'Enum type ""%s"" has no value named %s.' % (
              enum_descriptor.full_name, value))
  else:
    
    enum_value = enum_descriptor.values_by_number.get(number, None)
    if enum_value is None:
      raise ValueError(
          'Enum type ""%s"" has no value with number %d.' % (
              enum_descriptor.full_name, number))
  return enum_value.number
",,"













from collections import OrderedDict
from functools import partial

import matplotlib.pyplot as plt
import numpy as np


SECTORS = OrderedDict([
    (101, 'Basic Materials'),
    (102, 'Consumer Cyclical'),
    (103, 'Financial Services'),
    (104, 'Real Estate'),
    (205, 'Consumer Defensive'),
    (206, 'Healthcare'),
    (207, 'Utilities'),
    (308, 'Communication Services'),
    (309, 'Energy'),
    (310, 'Industrials'),
    (311, 'Technology')
])

CAP_BUCKETS = OrderedDict([
    ('Micro', (50000000, 300000000)),
    ('Small', (300000000, 2000000000)),
    ('Mid', (2000000000, 10000000000)),
    ('Large', (10000000000, 200000000000)),
    ('Mega', (200000000000, np.inf))
])


def compute_style_factor_exposures(positions, risk_factor):
    


    positions_wo_cash = positions.drop('cash', axis='columns')
    gross_exposure = positions_wo_cash.abs().sum(axis='columns')

    style_factor_exposure = positions_wo_cash.multiply(risk_factor) \
        .divide(gross_exposure, axis='index')
    tot_style_factor_exposure = style_factor_exposure.sum(axis='columns',
                                                          skipna=True)

    return tot_style_factor_exposure


def plot_style_factor_exposures(tot_style_factor_exposure, factor_name=None,
                                ax=None):
    


    if ax is None:
        ax = plt.gca()

    if factor_name is None:
        factor_name = tot_style_factor_exposure.name

    ax.plot(tot_style_factor_exposure.index, tot_style_factor_exposure,
            label=factor_name)
    avg = tot_style_factor_exposure.mean()
    ax.axhline(avg, linestyle='-.', label='Mean = {:.3}'.format(avg))
    ax.axhline(0, color='k', linestyle='-')
    _, _, y1, y2 = plt.axis()
    lim = max(abs(y1), abs(y2))
    ax.set(title='Exposure to {}'.format(factor_name),
           ylabel='{} \n weighted exposure'.format(factor_name),
           ylim=(-lim, lim))
    ax.legend(frameon=True, framealpha=0.5)

    return ax


def compute_sector_exposures(positions, sectors, sector_dict=SECTORS):
    


    sector_ids = sector_dict.keys()

    long_exposures = []
    short_exposures = []
    gross_exposures = []
    net_exposures = []

    positions_wo_cash = positions.drop('cash', axis='columns')
    long_exposure = positions_wo_cash[positions_wo_cash > 0] \
        .sum(axis='columns')
    short_exposure = positions_wo_cash[positions_wo_cash < 0] \
        .abs().sum(axis='columns')
    gross_exposure = positions_wo_cash.abs().sum(axis='columns')

    for sector_id in sector_ids:
        in_sector = positions_wo_cash[sectors == sector_id]

        long_sector = in_sector[in_sector > 0] \
            .sum(axis='columns').divide(long_exposure)
        short_sector = in_sector[in_sector < 0] \
            .sum(axis='columns').divide(short_exposure)
        gross_sector = in_sector.abs().sum(axis='columns') \
            .divide(gross_exposure)
        net_sector = long_sector.subtract(short_sector)

        long_exposures.append(long_sector)
        short_exposures.append(short_sector)
        gross_exposures.append(gross_sector)
        net_exposures.append(net_sector)

    return long_exposures, short_exposures, gross_exposures, net_exposures


def plot_sector_exposures_longshort(long_exposures, short_exposures,
                                    sector_dict=SECTORS, ax=None):
    


    if ax is None:
        ax = plt.gca()

    if sector_dict is None:
        sector_names = SECTORS.values()
    else:
        sector_names = sector_dict.values()

    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))

    ax.stackplot(long_exposures[0].index, long_exposures,
                 labels=sector_names, colors=color_list, alpha=0.8,
                 baseline='zero')
    ax.stackplot(long_exposures[0].index, short_exposures,
                 colors=color_list, alpha=0.8, baseline='zero')
    ax.axhline(0, color='k', linestyle='-')
    ax.set(title='Long and short exposures to sectors',
           ylabel='Proportion of long/short exposure in sectors')
    ax.legend(loc='upper left', frameon=True, framealpha=0.5)

    return ax


def plot_sector_exposures_gross(gross_exposures, sector_dict=None, ax=None):
    


    if ax is None:
        ax = plt.gca()

    if sector_dict is None:
        sector_names = SECTORS.values()
    else:
        sector_names = sector_dict.values()

    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))

    ax.stackplot(gross_exposures[0].index, gross_exposures,
                 labels=sector_names, colors=color_list, alpha=0.8,
                 baseline='zero')
    ax.axhline(0, color='k', linestyle='-')
    ax.set(title='Gross exposure to sectors',
           ylabel='Proportion of gross exposure \n in sectors')

    return ax


def plot_sector_exposures_net(net_exposures, sector_dict=None, ax=None):
    


    if ax is None:
        ax = plt.gca()

    if sector_dict is None:
        sector_names = SECTORS.values()
    else:
        sector_names = sector_dict.values()

    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 11))

    for i in range(len(net_exposures)):
        ax.plot(net_exposures[i], color=color_list[i], alpha=0.8,
                label=sector_names[i])
    ax.set(title='Net exposures to sectors',
           ylabel='Proportion of net exposure \n in sectors')

    return ax


def compute_cap_exposures(positions, caps):
    


    long_exposures = []
    short_exposures = []
    gross_exposures = []
    net_exposures = []

    positions_wo_cash = positions.drop('cash', axis='columns')
    tot_gross_exposure = positions_wo_cash.abs().sum(axis='columns')
    tot_long_exposure = positions_wo_cash[positions_wo_cash > 0] \
        .sum(axis='columns')
    tot_short_exposure = positions_wo_cash[positions_wo_cash < 0] \
        .abs().sum(axis='columns')

    for bucket_name, boundaries in CAP_BUCKETS.items():
        in_bucket = positions_wo_cash[(caps >= boundaries[0]) &
                                      (caps <= boundaries[1])]

        gross_bucket = in_bucket.abs().sum(axis='columns') \
            .divide(tot_gross_exposure)
        long_bucket = in_bucket[in_bucket > 0] \
            .sum(axis='columns').divide(tot_long_exposure)
        short_bucket = in_bucket[in_bucket < 0] \
            .sum(axis='columns').divide(tot_short_exposure)
        net_bucket = long_bucket.subtract(short_bucket)

        gross_exposures.append(gross_bucket)
        long_exposures.append(long_bucket)
        short_exposures.append(short_bucket)
        net_exposures.append(net_bucket)

    return long_exposures, short_exposures, gross_exposures, net_exposures


def plot_cap_exposures_longshort(long_exposures, short_exposures, ax=None):
    


    if ax is None:
        ax = plt.gca()

    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 5))

    ax.stackplot(long_exposures[0].index, long_exposures,
                 labels=CAP_BUCKETS.keys(), colors=color_list, alpha=0.8,
                 baseline='zero')
    ax.stackplot(long_exposures[0].index, short_exposures, colors=color_list,
                 alpha=0.8, baseline='zero')
    ax.axhline(0, color='k', linestyle='-')
    ax.set(title='Long and short exposures to market caps',
           ylabel='Proportion of long/short exposure in market cap buckets')
    ax.legend(loc='upper left', frameon=True, framealpha=0.5)

    return ax


def plot_cap_exposures_gross(gross_exposures, ax=None):
    


    if ax is None:
        ax = plt.gca()

    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 5))

    ax.stackplot(gross_exposures[0].index, gross_exposures,
                 labels=CAP_BUCKETS.keys(), colors=color_list, alpha=0.8,
                 baseline='zero')
    ax.axhline(0, color='k', linestyle='-')
    ax.set(title='Gross exposure to market caps',
           ylabel='Proportion of gross exposure \n in market cap buckets')

    return ax


def plot_cap_exposures_net(net_exposures, ax=None):
    


    if ax is None:
        ax = plt.gca()

    color_list = plt.cm.gist_rainbow(np.linspace(0, 1, 5))

    cap_names = CAP_BUCKETS.keys()
    for i in range(len(net_exposures)):
        ax.plot(net_exposures[i], color=color_list[i], alpha=0.8,
                label=cap_names[i])
    ax.axhline(0, color='k', linestyle='-')
    ax.set(title='Net exposure to market caps',
           ylabel='Proportion of net exposure \n in market cap buckets')

    return ax


def compute_volume_exposures(shares_held, volumes, percentile):
    


    shares_held = shares_held.replace(0, np.nan)

    shares_longed = shares_held[shares_held > 0]
    shares_shorted = -1 * shares_held[shares_held < 0]
    shares_grossed = shares_held.abs()

    longed_frac = shares_longed.divide(volumes)
    shorted_frac = shares_shorted.divide(volumes)
    grossed_frac = shares_grossed.divide(volumes)

    
    
    
    
    
    
    

    longed_threshold = 100 * longed_frac.apply(
        partial(np.nanpercentile, q=100 * percentile),
        axis='columns',
    )
    shorted_threshold = 100 * shorted_frac.apply(
        partial(np.nanpercentile, q=100 * percentile),
        axis='columns',
    )
    grossed_threshold = 100 * grossed_frac.apply(
        partial(np.nanpercentile, q=100 * percentile),
        axis='columns',
    )

    return longed_threshold, shorted_threshold, grossed_threshold


def plot_volume_exposures_longshort(longed_threshold, shorted_threshold,
                                    percentile, ax=None):
    


    if ax is None:
        ax = plt.gca()

    ax.plot(longed_threshold.index, longed_threshold,
            color='b', label='long')
    ax.plot(shorted_threshold.index, shorted_threshold,
            color='r', label='short')
    ax.axhline(0, color='k')
    ax.set(title='Long and short exposures to illiquidity',
           ylabel='{}th percentile of proportion of volume (%)'
           .format(100 * percentile))
    ax.legend(frameon=True, framealpha=0.5)

    return ax


def plot_volume_exposures_gross(grossed_threshold, percentile, ax=None):
    


    if ax is None:
        ax = plt.gca()

    ax.plot(grossed_threshold.index, grossed_threshold,
            color='b', label='gross')
    ax.axhline(0, color='k')
    ax.set(title='Gross exposure to illiquidity',
           ylabel='{}th percentile of \n proportion of volume (%)'
           .format(100 * percentile))
    ax.legend(frameon=True, framealpha=0.5)

    return ax
","import numpy as np
from astropy import constants
from astropy import cosmology as cosmo
from functools import wraps

from autolens import exc
from autolens.data.array import grids
from autolens.data.array import scaled_array
from autolens.lens.util import lens_util
from autolens.model import dimensions as dim
from autolens.model import cosmology_util
from autolens.model.galaxy.util import galaxy_util


class AbstractPlane(object):

    def __init__(self, redshift, galaxies, cosmology=cosmo.Planck15):
        


        self.redshift = redshift
        self.galaxies = galaxies
        self.cosmology = cosmology

    @property
    def galaxy_redshifts(self):
        return [galaxy.redshift for galaxy in self.galaxies]

    @property
    def arcsec_per_kpc(self):
        return cosmology_util.arcsec_per_kpc_from_redshift_and_cosmology(redshift=self.redshift,
                                                                         cosmology=self.cosmology)

    @property
    def kpc_per_arcsec(self):
        return 1.0 / self.arcsec_per_kpc

    def angular_diameter_distance_to_earth_in_units(self, unit_length='arcsec'):
        return cosmology_util.angular_diameter_distance_to_earth_from_redshift_and_cosmology(
            redshift=self.redshift, cosmology=self.cosmology, unit_length=unit_length)

    def cosmic_average_density_in_units(self, unit_length='arcsec', unit_mass='solMass'):
        return cosmology_util.cosmic_average_density_from_redshift_and_cosmology(
            redshift=self.redshift, cosmology=self.cosmology, unit_length=unit_length, unit_mass=unit_mass)

    @property
    def has_light_profile(self):
        return any(list(map(lambda galaxy: galaxy.has_light_profile, self.galaxies)))

    @property
    def has_mass_profile(self):
        return any(list(map(lambda galaxy: galaxy.has_mass_profile, self.galaxies)))

    @property
    def has_pixelization(self):
        return any(list(map(lambda galaxy: galaxy.has_pixelization, self.galaxies)))

    @property
    def has_regularization(self):
        return any(list(map(lambda galaxy: galaxy.has_regularization, self.galaxies)))

    @property
    def regularization(self):

        galaxies_with_regularization = list(filter(lambda galaxy: galaxy.has_regularization, self.galaxies))

        if len(galaxies_with_regularization) == 0:
            return None
        if len(galaxies_with_regularization) == 1:
            return galaxies_with_regularization[0].regularization
        elif len(galaxies_with_regularization) > 1:
            raise exc.PixelizationException('The number of galaxies with regularizations in one plane is above 1')

    @property
    def centres_of_galaxy_mass_profiles(self):

        if self.has_mass_profile:

            galaxies_with_mass_profiles = [galaxy for galaxy in self.galaxies if galaxy.has_mass_profile]

            mass_profile_centres = [[] for i in range(len(galaxies_with_mass_profiles))]

            for galaxy_index, galaxy in enumerate(galaxies_with_mass_profiles):
                mass_profile_centres[galaxy_index] = [profile.centre for profile in galaxy.mass_profiles]
            return mass_profile_centres

        else:

            return None

    @property
    def axis_ratios_of_galaxy_mass_profiles(self):

        if self.has_mass_profile:

            galaxies_with_mass_profiles = [galaxy for galaxy in self.galaxies if galaxy.has_mass_profile]

            mass_profile_axis_ratios = [[] for i in range(len(galaxies_with_mass_profiles))]

            for galaxy_index, galaxy in enumerate(galaxies_with_mass_profiles):
                mass_profile_axis_ratios[galaxy_index] = [profile.axis_ratio for profile in galaxy.mass_profiles]
            return mass_profile_axis_ratios

        else:

            return None
        
    @property
    def phis_of_galaxy_mass_profiles(self):

        if self.has_mass_profile:

            galaxies_with_mass_profiles = [galaxy for galaxy in self.galaxies if galaxy.has_mass_profile]

            mass_profile_phis = [[] for i in range(len(galaxies_with_mass_profiles))]

            for galaxy_index, galaxy in enumerate(galaxies_with_mass_profiles):
                mass_profile_phis[galaxy_index] = [profile.phi for profile in galaxy.mass_profiles]
            return mass_profile_phis

        else:

            return None

    def luminosities_of_galaxies_within_circles_in_units(self, radius : dim.Length, unit_luminosity='eps', exposure_time=None):
        

        return list(map(lambda galaxy: galaxy.luminosity_within_circle_in_units(
            radius=radius, unit_luminosity=unit_luminosity, kpc_per_arcsec=self.kpc_per_arcsec,
            exposure_time=exposure_time),
                        self.galaxies))

    def luminosities_of_galaxies_within_ellipses_in_units(self, major_axis : dim.Length, unit_luminosity='eps',
                                                          exposure_time=None):
        

        return list(map(lambda galaxy: galaxy.luminosity_within_ellipse_in_units(
            major_axis=major_axis, unit_luminosity=unit_luminosity, kpc_per_arcsec=self.kpc_per_arcsec,
            exposure_time=exposure_time),
                        self.galaxies))

    def masses_of_galaxies_within_circles_in_units(self, radius : dim.Length, unit_mass='angular',
                                                   critical_surface_density=None):
        

        return list(map(lambda galaxy: galaxy.mass_within_circle_in_units(
                        radius=radius, unit_mass=unit_mass, kpc_per_arcsec=self.kpc_per_arcsec,
                        critical_surface_density=critical_surface_density),
                        self.galaxies))

    def masses_of_galaxies_within_ellipses_in_units(self, major_axis : dim.Length, unit_mass='angular',
                                                    critical_surface_density=None):
        

        return list(map(lambda galaxy: galaxy.mass_within_ellipse_in_units(
                        major_axis=major_axis, unit_mass=unit_mass, kpc_per_arcsec=self.kpc_per_arcsec,
                        critical_surface_density=critical_surface_density),
                        self.galaxies))

    def einstein_radius_in_units(self, unit_length='arcsec', kpc_per_arcsec=None):

        if self.has_mass_profile:
            return sum(filter(None,
                   list(map(lambda galaxy: galaxy.einstein_radius_in_units(
                       unit_length=unit_length, kpc_per_arcsec=kpc_per_arcsec),
                            self.galaxies))))

    def einstein_mass_in_units(self, unit_mass='angular', critical_surface_density=None):

        if self.has_mass_profile:
            return sum(filter(None,
                   list(map(lambda galaxy: galaxy.einstein_mass_in_units(
                       unit_mass=unit_mass, critical_surface_density=critical_surface_density),
                            self.galaxies))))

class AbstractGriddedPlane(AbstractPlane):

    def __init__(self, redshift, galaxies, grid_stack, border, compute_deflections, cosmology=cosmo.Planck15):
        


        super(AbstractGriddedPlane, self).__init__(redshift=redshift, galaxies=galaxies, cosmology=cosmology)

        self.grid_stack = grid_stack
        self.border = border

        if compute_deflections:

            def calculate_deflections(grid):

                if galaxies:
                    return sum(map(lambda galaxy: galaxy.deflections_from_grid(grid), galaxies))
                else:
                    return np.full((grid.shape[0], 2), 0.0)

            self.deflection_stack = self.grid_stack.apply_function(calculate_deflections)

        else:

            self.deflection_stack = None

    def trace_grid_stack_to_next_plane(self):
        


        def minus(grid, deflections):
            return grid - deflections

        return self.grid_stack.map_function(minus, self.deflection_stack)

    @property
    def image_plane_image(self):
        return self.grid_stack.scaled_array_2d_from_array_1d(self.image_plane_image_1d)

    @property
    def image_plane_image_for_simulation(self):
        if not self.has_padded_grid_stack:
            raise exc.RayTracingException(
                'To retrieve an image plane image for the simulation, the grid_stacks in the tracer_normal'
                'must be padded grid_stacks')
        return self.grid_stack.regular.map_to_2d_keep_padded(padded_array_1d=self.image_plane_image_1d)

    @property
    def image_plane_image_1d(self):
        return galaxy_util.intensities_of_galaxies_from_grid(grid=self.grid_stack.sub, galaxies=self.galaxies)

    @property
    def image_plane_image_1d_of_galaxies(self):
        return list(map(self.image_plane_image_1d_of_galaxy, self.galaxies))

    def image_plane_image_1d_of_galaxy(self, galaxy):
        return galaxy_util.intensities_of_galaxies_from_grid(grid=self.grid_stack.sub, galaxies=[galaxy])

    @property
    def image_plane_blurring_image_1d(self):
        return galaxy_util.intensities_of_galaxies_from_grid(grid=self.grid_stack.blurring, galaxies=self.galaxies)

    @property
    def convergence(self):
        convergence_1d = galaxy_util.convergence_of_galaxies_from_grid(
            grid=self.grid_stack.sub.unlensed_grid, galaxies=self.galaxies)
        return self.grid_stack.scaled_array_2d_from_array_1d(array_1d=convergence_1d)

    @property
    def potential(self):
        potential_1d = galaxy_util.potential_of_galaxies_from_grid(grid=self.grid_stack.sub.unlensed_grid,
                                                                   galaxies=self.galaxies)
        return self.grid_stack.scaled_array_2d_from_array_1d(array_1d=potential_1d)

    @property
    def deflections_y(self):
        return self.grid_stack.scaled_array_2d_from_array_1d(self.deflections_1d[:, 0])

    @property
    def deflections_x(self):
        return self.grid_stack.scaled_array_2d_from_array_1d(self.deflections_1d[:, 1])

    @property
    def deflections_1d(self):
        return galaxy_util.deflections_of_galaxies_from_grid(grid=self.grid_stack.sub.unlensed_grid,
                                                             galaxies=self.galaxies)

    @property
    def has_padded_grid_stack(self):
        return isinstance(self.grid_stack.regular, grids.PaddedRegularGrid)

    @property
    def plane_image(self):
        return lens_util.plane_image_of_galaxies_from_grid(shape=self.grid_stack.regular.mask.shape,
                                                           grid=self.grid_stack.regular,
                                                           galaxies=self.galaxies)

    @property
    def mapper(self):

        galaxies_with_pixelization = list(filter(lambda galaxy: galaxy.has_pixelization, self.galaxies))

        if len(galaxies_with_pixelization) == 0:
            return None
        if len(galaxies_with_pixelization) == 1:
            pixelization = galaxies_with_pixelization[0].pixelization
            return pixelization.mapper_from_grid_stack_and_border(grid_stack=self.grid_stack, border=self.border)
        elif len(galaxies_with_pixelization) > 1:
            raise exc.PixelizationException('The number of galaxies with pixelizations in one plane is above 1')

    @property
    def yticks(self):
        

        return np.linspace(np.amin(self.grid_stack.regular[:, 0]), np.amax(self.grid_stack.regular[:, 0]), 4)

    @property
    def xticks(self):
        

        return np.linspace(np.amin(self.grid_stack.regular[:, 1]), np.amax(self.grid_stack.regular[:, 1]), 4)


class Plane(AbstractGriddedPlane):

    def __init__(self, galaxies, grid_stack, border=None, compute_deflections=True, cosmology=cosmo.Planck15):
        


        if not galaxies:
            raise exc.RayTracingException('An empty list of galaxies was supplied to Plane')

        galaxy_redshifts = [galaxy.redshift for galaxy in galaxies]

        if any([redshift is not None for redshift in galaxy_redshifts]):
            if not all([galaxies[0].redshift == galaxy.redshift for galaxy in galaxies]):
                raise exc.RayTracingException('The galaxies supplied to A Plane have different redshifts or one galaxy '
                                              'does not have a redshift.')

        super(Plane, self).__init__(redshift=galaxies[0].redshift, galaxies=galaxies, grid_stack=grid_stack,
                                    border=border, compute_deflections=compute_deflections, cosmology=cosmology)

    def unmasked_blurred_image_of_galaxies_from_psf(self, padded_grid_stack, psf):
        

        return [padded_grid_stack.unmasked_blurred_image_from_psf_and_unmasked_image(
            psf, image) if not galaxy.has_pixelization else None for galaxy, image in
                zip(self.galaxies, self.image_plane_image_1d_of_galaxies)]

    def unmasked_blurred_image_of_galaxy_with_grid_stack_psf(self, galaxy, padded_grid_stack, psf):
        return padded_grid_stack.unmasked_blurred_image_from_psf_and_unmasked_image(
            psf,
            self.image_plane_image_1d_of_galaxy(
                galaxy))


class PlaneSlice(AbstractGriddedPlane):

    def __init__(self, galaxies, grid_stack, redshift, border=None, compute_deflections=True, cosmology=cosmo.Planck15):
        


        super(PlaneSlice, self).__init__(redshift=redshift, galaxies=galaxies, grid_stack=grid_stack, border=border,
                                         compute_deflections=compute_deflections, cosmology=cosmology)


class PlanePositions(object):

    def __init__(self, redshift, galaxies, positions, compute_deflections=True, cosmology=None):
        


        self.redshift = redshift
        self.galaxies = galaxies
        self.positions = positions

        if compute_deflections:
            def calculate_deflections(pos):
                return sum(map(lambda galaxy: galaxy.deflections_from_grid(pos), galaxies))

            self.deflections = list(map(lambda pos: calculate_deflections(pos), self.positions))

        self.cosmology = cosmology

    def trace_to_next_plane(self):
        

        return list(map(lambda positions, deflections: np.subtract(positions, deflections),
                        self.positions, self.deflections))


class PlaneImage(scaled_array.ScaledRectangularPixelArray):

    def __init__(self, array, pixel_scales, grid, origin=(0.0, 0.0)):
        self.grid = grid
        super(PlaneImage, self).__init__(array=array, pixel_scales=pixel_scales, origin=origin)
","from redis import Redis, ConnectionPool
from six.moves import xrange

from ._util import to_string

class Suggestion(object):
    

    def __init__(self, string, score=1.0, payload=None):
        self.string = to_string(string)
        self.payload = to_string(payload)
        self.score = score

    def __repr__(self):
        return self.string


class SuggestionParser(object):
    

    def __init__(self, with_scores, with_payloads, ret):
        self.with_scores = with_scores
        self.with_payloads = with_payloads

        if with_scores and with_payloads:
            self.sugsize = 3
            self._scoreidx = 1
            self._payloadidx = 2
        elif with_scores:
            self.sugsize = 2
            self._scoreidx = 1
        elif with_payloads:
            self.sugsize = 2
            self._payloadidx = 1
        else:
            self.sugsize = 1
            self._scoreidx = -1

        self._sugs = ret

    def __iter__(self):
        for i in xrange(0, len(self._sugs), self.sugsize):
            ss = self._sugs[i]
            score = float(self._sugs[i + self._scoreidx]) if self.with_scores else 1.0
            payload = self._sugs[i + self._payloadidx] if self.with_payloads else None
            yield Suggestion(ss, score, payload)


class AutoCompleter(object):
    


    SUGADD_COMMAND = ""FT.SUGADD""
    SUGDEL_COMMAND = ""FT.SUGDEL""
    SUGLEN_COMMAND = ""FT.SUGLEN""
    SUGGET_COMMAND = ""FT.SUGGET""

    INCR = 'INCR'
    WITHSCORES = 'WITHSCORES'
    FUZZY = 'FUZZY'
    WITHPAYLOADS = 'WITHPAYLOADS'

    def __init__(self, key, host='localhost', port=6379, conn = None):
        


        self.key = key
        self.redis = conn if conn is not None else Redis(
            connection_pool = ConnectionPool(host=host, port=port))

    def add_suggestions(self,  *suggestions, **kwargs):
        

        pipe = self.redis.pipeline()
        for sug in suggestions:
            args = [AutoCompleter.SUGADD_COMMAND, self.key, sug.string, sug.score]
            if kwargs.get('increment'):
                args.append(AutoCompleter.INCR)
            if sug.payload:
                args.append('PAYLOAD')
                args.append(sug.payload)

            pipe.execute_command(*args)

        return pipe.execute()[-1]



    def len(self):
        

        return self.redis.execute_command(AutoCompleter.SUGLEN_COMMAND, self.key)

    def delete(self, string):
        

        return self.redis.execute_command(AutoCompleter.SUGDEL_COMMAND, self.key, string)

    def get_suggestions(self, prefix, fuzzy = False, num = 10, with_scores = False, with_payloads=False):
        


        args = [AutoCompleter.SUGGET_COMMAND, self.key, prefix, 'MAX', num]
        if fuzzy:
            args.append(AutoCompleter.FUZZY)
        if with_scores:
            args.append(AutoCompleter.WITHSCORES)
        if with_payloads:
            args.append(AutoCompleter.WITHPAYLOADS)

        ret = self.redis.execute_command(*args)
        results = []
        if not ret:
            return results

        parser = SuggestionParser(with_scores, with_payloads, ret)
        return [s for s in parser]","




















from bisect import bisect_left
from collections import defaultdict

from nupic.serializable import Serializable
try:
  import capnp
except ImportError:
  capnp = None
if capnp:
  from nupic.proto.ConnectionsProto_capnp import ConnectionsProto

EPSILON = 0.00001 
                  



class Segment(object):
  


  __slots__ = [""cell"", ""flatIdx"", ""_synapses"", ""_ordinal""]

  def __init__(self, cell, flatIdx, ordinal):
    self.cell = cell
    self.flatIdx = flatIdx
    self._synapses = set()
    self._ordinal = ordinal


  def __eq__(self, other):
    


    return (self.cell == other.cell and
            (sorted(self._synapses, key=lambda x: x._ordinal) ==
             sorted(other._synapses, key=lambda x: x._ordinal)))



class Synapse(object):
  


  __slots__ = [""segment"", ""presynapticCell"", ""permanence"", ""_ordinal""]

  def __init__(self, segment, presynapticCell, permanence, ordinal):
    self.segment = segment
    self.presynapticCell = presynapticCell
    self.permanence = permanence
    self._ordinal = ordinal


  def __eq__(self, other):
    

    return (self.segment.cell == other.segment.cell and
            self.presynapticCell == other.presynapticCell and
            abs(self.permanence - other.permanence) < EPSILON)



class CellData(object):
  

  __slots__ = [""_segments""]

  def __init__(self):
    self._segments = []

  def __eq__(self, other):
      return self._segments == other._segments

  def __ne__(self, other):
    return not self.__eq__(other)



def binSearch(arr, val):
  

  i = bisect_left(arr, val)
  if i != len(arr) and arr[i] == val:
    return i
  return -1



class Connections(Serializable):
  


  def __init__(self, numCells):

    
    self.numCells = numCells

    self._cells = [CellData() for _ in xrange(numCells)]
    self._synapsesForPresynapticCell = defaultdict(set)
    self._segmentForFlatIdx = []

    self._numSynapses = 0
    self._freeFlatIdxs = []
    self._nextFlatIdx = 0

    
    
    self._nextSynapseOrdinal = long(0)
    self._nextSegmentOrdinal = long(0)


  def segmentsForCell(self, cell):
    


    return self._cells[cell]._segments


  def synapsesForSegment(self, segment):
    


    return segment._synapses


  def dataForSynapse(self, synapse):
    

    return synapse


  def dataForSegment(self, segment):
    

    return segment


  def getSegment(self, cell, idx):
    


    return self._cells[cell]._segments[idx]


  def segmentForFlatIdx(self, flatIdx):
    

    return self._segmentForFlatIdx[flatIdx]


  def segmentFlatListLength(self):
    

    return self._nextFlatIdx


  def synapsesForPresynapticCell(self, presynapticCell):
    

    return self._synapsesForPresynapticCell[presynapticCell]


  def createSegment(self, cell):
    

    cellData = self._cells[cell]

    if len(self._freeFlatIdxs) > 0:
      flatIdx = self._freeFlatIdxs.pop()
    else:
      flatIdx = self._nextFlatIdx
      self._segmentForFlatIdx.append(None)
      self._nextFlatIdx += 1

    ordinal = self._nextSegmentOrdinal
    self._nextSegmentOrdinal += 1

    segment = Segment(cell, flatIdx, ordinal)
    cellData._segments.append(segment)
    self._segmentForFlatIdx[flatIdx] = segment

    return segment


  def destroySegment(self, segment):
    

    
    for synapse in segment._synapses:
      self._removeSynapseFromPresynapticMap(synapse)
    self._numSynapses -= len(segment._synapses)

    
    segments = self._cells[segment.cell]._segments
    i = segments.index(segment)
    del segments[i]

    
    
    self._freeFlatIdxs.append(segment.flatIdx)
    self._segmentForFlatIdx[segment.flatIdx] = None


  def createSynapse(self, segment, presynapticCell, permanence):
    

    idx = len(segment._synapses)
    synapse = Synapse(segment, presynapticCell, permanence,
                      self._nextSynapseOrdinal)
    self._nextSynapseOrdinal += 1
    segment._synapses.add(synapse)

    self._synapsesForPresynapticCell[presynapticCell].add(synapse)

    self._numSynapses += 1

    return synapse


  def _removeSynapseFromPresynapticMap(self, synapse):
    inputSynapses = self._synapsesForPresynapticCell[synapse.presynapticCell]

    inputSynapses.remove(synapse)

    if len(inputSynapses) == 0:
      del self._synapsesForPresynapticCell[synapse.presynapticCell]


  def destroySynapse(self, synapse):
    


    self._numSynapses -= 1

    self._removeSynapseFromPresynapticMap(synapse)

    synapse.segment._synapses.remove(synapse)


  def updateSynapsePermanence(self, synapse, permanence):
    


    synapse.permanence = permanence


  def computeActivity(self, activePresynapticCells, connectedPermanence):
    


    numActiveConnectedSynapsesForSegment = [0] * self._nextFlatIdx
    numActivePotentialSynapsesForSegment = [0] * self._nextFlatIdx

    threshold = connectedPermanence - EPSILON

    for cell in activePresynapticCells:
      for synapse in self._synapsesForPresynapticCell[cell]:
        flatIdx = synapse.segment.flatIdx
        numActivePotentialSynapsesForSegment[flatIdx] += 1
        if synapse.permanence > threshold:
          numActiveConnectedSynapsesForSegment[flatIdx] += 1

    return (numActiveConnectedSynapsesForSegment,
            numActivePotentialSynapsesForSegment)


  def numSegments(self, cell=None):
    

    if cell is not None:
      return len(self._cells[cell]._segments)

    return self._nextFlatIdx - len(self._freeFlatIdxs)


  def numSynapses(self, segment=None):
    

    if segment is not None:
      return len(segment._synapses)
    return self._numSynapses


  def segmentPositionSortKey(self, segment):
    

    return segment.cell + (segment._ordinal / float(self._nextSegmentOrdinal))


  def write(self, proto):
    

    protoCells = proto.init('cells', self.numCells)

    for i in xrange(self.numCells):
      segments = self._cells[i]._segments
      protoSegments = protoCells[i].init('segments', len(segments))

      for j, segment in enumerate(segments):
        synapses = segment._synapses
        protoSynapses = protoSegments[j].init('synapses', len(synapses))

        for k, synapse in enumerate(sorted(synapses, key=lambda s: s._ordinal)):
          protoSynapses[k].presynapticCell = synapse.presynapticCell
          protoSynapses[k].permanence = synapse.permanence


  @classmethod
  def getSchema(cls):
    return ConnectionsProto


  @classmethod
  def read(cls, proto):
    

    
    protoCells = proto.cells
    connections = cls(len(protoCells))

    for cellIdx, protoCell in enumerate(protoCells):
      protoCell = protoCells[cellIdx]
      protoSegments = protoCell.segments
      connections._cells[cellIdx] = CellData()
      segments = connections._cells[cellIdx]._segments

      for segmentIdx, protoSegment in enumerate(protoSegments):
        segment = Segment(cellIdx, connections._nextFlatIdx,
                          connections._nextSegmentOrdinal)

        segments.append(segment)
        connections._segmentForFlatIdx.append(segment)
        connections._nextFlatIdx += 1
        connections._nextSegmentOrdinal += 1

        synapses = segment._synapses
        protoSynapses = protoSegment.synapses

        for synapseIdx, protoSynapse in enumerate(protoSynapses):
          presynapticCell = protoSynapse.presynapticCell
          synapse = Synapse(segment, presynapticCell, protoSynapse.permanence,
                            ordinal=connections._nextSynapseOrdinal)
          connections._nextSynapseOrdinal += 1
          synapses.add(synapse)
          connections._synapsesForPresynapticCell[presynapticCell].add(synapse)

          connections._numSynapses += 1

    
    return connections


  def __eq__(self, other):
    

    
    for i in xrange(self.numCells):
      segments = self._cells[i]._segments
      otherSegments = other._cells[i]._segments

      if len(segments) != len(otherSegments):
        return False

      for j in xrange(len(segments)):
        segment = segments[j]
        otherSegment = otherSegments[j]
        synapses = segment._synapses
        otherSynapses = otherSegment._synapses

        if len(synapses) != len(otherSynapses):
          return False

        for synapse in synapses:
          found = False
          for candidate in otherSynapses:
            if synapse == candidate:
              found = True
              break

          if not found:
            return False

    if (len(self._synapsesForPresynapticCell) !=
        len(self._synapsesForPresynapticCell)):
      return False

    for i in self._synapsesForPresynapticCell.keys():
      synapses = self._synapsesForPresynapticCell[i]
      otherSynapses = other._synapsesForPresynapticCell[i]
      if len(synapses) != len(otherSynapses):
        return False

      for synapse in synapses:
        found = False
        for candidate in otherSynapses:
          if synapse == candidate:
            found = True
            break

        if not found:
          return False

    if self._numSynapses != other._numSynapses:
      return False

    
    return True


  def __ne__(self, other):
    

    return not self.__eq__(other)
","

from botocore.exceptions import ClientError

from cloudaux.aws.ec2 import describe_vpcs, describe_dhcp_options, describe_vpc_classic_link, \
    describe_vpc_classic_link_dns_support, describe_internet_gateways, describe_vpc_peering_connections, \
    describe_subnets, describe_route_tables, describe_network_acls, describe_vpc_attribute, describe_flow_logs
from cloudaux.decorators import modify_output
from flagpole import FlagRegistry, Flags

from cloudaux.exceptions import CloudAuxException

registry = FlagRegistry()
FLAGS = Flags('BASE', 'INTERNET_GATEWAY', 'CLASSIC_LINK', 'VPC_PEERING_CONNECTIONS', 'SUBNETS', 'ROUTE_TABLES',
              'NETWORK_ACLS', 'FLOW_LOGS')


@registry.register(flag=FLAGS.FLOW_LOGS, depends_on=FLAGS.BASE, key=""flow_logs"")
def get_vpc_flow_logs(vpc, **conn):
    

    fl_result = describe_flow_logs(Filters=[{""Name"": ""resource-id"", ""Values"": [vpc[""id""]]}], **conn)

    fl_ids = []
    for fl in fl_result:
        fl_ids.append(fl[""FlowLogId""])

    return fl_ids


@registry.register(flag=FLAGS.CLASSIC_LINK, depends_on=FLAGS.BASE, key=""classic_link"")
def get_classic_link(vpc, **conn):
    

    result = {}

    try:
        cl_result = describe_vpc_classic_link(VpcIds=[vpc[""id""]], **conn)[0]
        result[""Enabled""] = cl_result[""ClassicLinkEnabled""]

        
        dns_result = describe_vpc_classic_link_dns_support(VpcIds=[vpc[""id""]], **conn)[0]
        result[""DnsEnabled""] = dns_result[""ClassicLinkDnsSupported""]
    except ClientError as e:
        
        if 'UnsupportedOperation' not in str(e):
            raise e

    return result


@registry.register(flag=FLAGS.INTERNET_GATEWAY, depends_on=FLAGS.BASE, key=""internet_gateway"")
def get_internet_gateway(vpc, **conn):
    

    result = {}
    ig_result = describe_internet_gateways(Filters=[{""Name"": ""attachment.vpc-id"", ""Values"": [vpc[""id""]]}], **conn)

    if ig_result:
        
        result.update({
            ""State"": ig_result[0][""Attachments""][0][""State""],
            ""Id"": ig_result[0][""InternetGatewayId""],
            ""Tags"": ig_result[0].get(""Tags"", [])
        })

    return result


@registry.register(flag=FLAGS.VPC_PEERING_CONNECTIONS, depends_on=FLAGS.BASE, key=""vpc_peering_connections"")
def get_vpc_peering_connections(vpc, **conn):
    

    accepter_result = describe_vpc_peering_connections(Filters=[{""Name"": ""accepter-vpc-info.vpc-id"",
                                                                 ""Values"": [vpc[""id""]]}], **conn)

    requester_result = describe_vpc_peering_connections(Filters=[{""Name"": ""requester-vpc-info.vpc-id"",
                                                                 ""Values"": [vpc[""id""]]}], **conn)

    
    peer_ids = []
    for peering in accepter_result + requester_result:
        peer_ids.append(peering[""VpcPeeringConnectionId""])

    return peer_ids


@registry.register(flag=FLAGS.SUBNETS, depends_on=FLAGS.BASE, key=""subnets"")
def get_subnets(vpc, **conn):
    

    subnets = describe_subnets(Filters=[{""Name"": ""vpc-id"", ""Values"": [vpc[""id""]]}], **conn)

    s_ids = []
    for s in subnets:
        s_ids.append(s[""SubnetId""])

    return s_ids


@registry.register(flag=FLAGS.ROUTE_TABLES, depends_on=FLAGS.BASE, key=""route_tables"")
def get_route_tables(vpc, **conn):
    

    route_tables = describe_route_tables(Filters=[{""Name"": ""vpc-id"", ""Values"": [vpc[""id""]]}], **conn)

    rt_ids = []
    for r in route_tables:
        rt_ids.append(r[""RouteTableId""])

    return rt_ids


@registry.register(flag=FLAGS.NETWORK_ACLS, depends_on=FLAGS.BASE, key=""network_acls"")
def get_network_acls(vpc, **conn):
    

    route_tables = describe_network_acls(Filters=[{""Name"": ""vpc-id"", ""Values"": [vpc[""id""]]}], **conn)

    nacl_ids = []
    for r in route_tables:
        nacl_ids.append(r[""NetworkAclId""])

    return nacl_ids


@registry.register(flag=FLAGS.BASE)
def get_base(vpc, **conn):
    

    
    base_result = describe_vpcs(VpcIds=[vpc[""id""]], **conn)[0]

    
    vpc_name = None
    for t in base_result.get(""Tags"", []):
        if t[""Key""] == ""Name"":
            vpc_name = t[""Value""]

    dhcp_opts = None
    
    if base_result.get(""DhcpOptionsId""):
        
        dhcp_opts = describe_dhcp_options(DhcpOptionsIds=[base_result[""DhcpOptionsId""]], **conn)[0][""DhcpOptionsId""]

    
    attributes = {}
    attr_vals = [
        (""EnableDnsHostnames"", ""enableDnsHostnames""),
        (""EnableDnsSupport"", ""enableDnsSupport"")
    ]
    for attr, query in attr_vals:
        attributes[attr] = describe_vpc_attribute(VpcId=vpc[""id""], Attribute=query, **conn)[attr]

    vpc.update({
        'name': vpc_name,
        'region': conn[""region""],
        'tags': base_result.get(""Tags"", []),
        'is_default': base_result[""IsDefault""],
        'instance_tenancy': base_result[""InstanceTenancy""],
        'dhcp_options_id': dhcp_opts,
        'cidr_block': base_result[""CidrBlock""],
        'cidr_block_association_set': base_result.get(""CidrBlockAssociationSet"", []),
        'ipv6_cidr_block_association_set': base_result.get(""Ipv6CidrBlockAssociationSet"", []),
        'attributes': attributes,
        '_version': 1
    })
    return vpc


@modify_output
def get_vpc(vpc_id, flags=FLAGS.ALL, **conn):
    

    
    if not conn.get(""account_number""):
        raise CloudAuxException({""message"": ""Must supply account number in the connection dict to construct ""
                                            ""the VPC ARN."",
                                 ""vpc_id"": vpc_id})

    if not conn.get(""region""):
        raise CloudAuxException({""message"": ""Must supply region in the connection dict to construct ""
                                            ""the VPC ARN."",
                                 ""vpc_id"": vpc_id})

    start = {
        'arn': ""arn:aws:ec2:{region}:{account}:vpc/{vpc_id}"".format(region=conn[""region""],
                                                                    account=conn[""account_number""],
                                                                    vpc_id=vpc_id),
        'id': vpc_id
    }

    return registry.build_out(flags, start_with=start, pass_datastructure=True, **conn)
","import os
import sys
import struct

from ._compat import raw_input, text_type, string_types, \
     isatty, strip_ansi, get_winterm_size, DEFAULT_COLUMNS, WIN
from .utils import echo
from .exceptions import Abort, UsageError
from .types import convert_type
from .globals import resolve_color_default




visible_prompt_func = raw_input

_ansi_colors = ('black', 'red', 'green', 'yellow', 'blue', 'magenta',
                'cyan', 'white', 'reset')
_ansi_reset_all = '\033[0m'


def hidden_prompt_func(prompt):
    import getpass
    return getpass.getpass(prompt)


def _build_prompt(text, suffix, show_default=False, default=None):
    prompt = text
    if default is not None and show_default:
        prompt = '%s [%s]' % (prompt, default)
    return prompt + suffix


def prompt(text, default=None, hide_input=False,
           confirmation_prompt=False, type=None,
           value_proc=None, prompt_suffix=': ',
           show_default=True, err=False):
    

    result = None

    def prompt_func(text):
        f = hide_input and hidden_prompt_func or visible_prompt_func
        try:
            
            
            echo(text, nl=False, err=err)
            return f('')
        except (KeyboardInterrupt, EOFError):
            
            
            
            if hide_input:
                echo(None, err=err)
            raise Abort()

    if value_proc is None:
        value_proc = convert_type(type, default)

    prompt = _build_prompt(text, prompt_suffix, show_default, default)

    while 1:
        while 1:
            value = prompt_func(prompt)
            if value:
                break
            
            
            
            elif default is not None:
                return default
        try:
            result = value_proc(value)
        except UsageError as e:
            echo('Error: %s' % e.message, err=err)
            continue
        if not confirmation_prompt:
            return result
        while 1:
            value2 = prompt_func('Repeat for confirmation: ')
            if value2:
                break
        if value == value2:
            return result
        echo('Error: the two entered values do not match', err=err)


def confirm(text, default=False, abort=False, prompt_suffix=': ',
            show_default=True, err=False):
    

    prompt = _build_prompt(text, prompt_suffix, show_default,
                           default and 'Y/n' or 'y/N')
    while 1:
        try:
            
            
            echo(prompt, nl=False, err=err)
            value = visible_prompt_func('').lower().strip()
        except (KeyboardInterrupt, EOFError):
            raise Abort()
        if value in ('y', 'yes'):
            rv = True
        elif value in ('n', 'no'):
            rv = False
        elif value == '':
            rv = default
        else:
            echo('Error: invalid input', err=err)
            continue
        break
    if abort and not rv:
        raise Abort()
    return rv


def get_terminal_size():
    

    
    if sys.version_info >= (3, 3):
        import shutil
        shutil_get_terminal_size = getattr(shutil, 'get_terminal_size', None)
        if shutil_get_terminal_size:
            sz = shutil_get_terminal_size()
            return sz.columns, sz.lines

    if get_winterm_size is not None:
        return get_winterm_size()

    def ioctl_gwinsz(fd):
        try:
            import fcntl
            import termios
            cr = struct.unpack(
                'hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))
        except Exception:
            return
        return cr

    cr = ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)
    if not cr:
        try:
            fd = os.open(os.ctermid(), os.O_RDONLY)
            try:
                cr = ioctl_gwinsz(fd)
            finally:
                os.close(fd)
        except Exception:
            pass
    if not cr or not cr[0] or not cr[1]:
        cr = (os.environ.get('LINES', 25),
              os.environ.get('COLUMNS', DEFAULT_COLUMNS))
    return int(cr[1]), int(cr[0])


def echo_via_pager(text, color=None):
    

    color = resolve_color_default(color)
    if not isinstance(text, string_types):
        text = text_type(text)
    from ._termui_impl import pager
    return pager(text + '\n', color)


def progressbar(iterable=None, length=None, label=None, show_eta=True,
                show_percent=None, show_pos=False,
                item_show_func=None, fill_char='
                bar_template='%(label)s  [%(bar)s]  %(info)s',
                info_sep='  ', width=36, file=None, color=None):
    

    from ._termui_impl import ProgressBar
    color = resolve_color_default(color)
    return ProgressBar(iterable=iterable, length=length, show_eta=show_eta,
                       show_percent=show_percent, show_pos=show_pos,
                       item_show_func=item_show_func, fill_char=fill_char,
                       empty_char=empty_char, bar_template=bar_template,
                       info_sep=info_sep, file=file, label=label,
                       width=width, color=color)


def clear():
    

    if not isatty(sys.stdout):
        return
    
    
    
    if WIN:
        os.system('cls')
    else:
        sys.stdout.write('\033[2J\033[1;1H')


def style(text, fg=None, bg=None, bold=None, dim=None, underline=None,
          blink=None, reverse=None, reset=True):
    

    bits = []
    if fg:
        try:
            bits.append('\033[%dm' % (_ansi_colors.index(fg) + 30))
        except ValueError:
            raise TypeError('Unknown color %r' % fg)
    if bg:
        try:
            bits.append('\033[%dm' % (_ansi_colors.index(bg) + 40))
        except ValueError:
            raise TypeError('Unknown color %r' % bg)
    if bold is not None:
        bits.append('\033[%dm' % (1 if bold else 22))
    if dim is not None:
        bits.append('\033[%dm' % (2 if dim else 22))
    if underline is not None:
        bits.append('\033[%dm' % (4 if underline else 24))
    if blink is not None:
        bits.append('\033[%dm' % (5 if blink else 25))
    if reverse is not None:
        bits.append('\033[%dm' % (7 if reverse else 27))
    bits.append(text)
    if reset:
        bits.append(_ansi_reset_all)
    return ''.join(bits)


def unstyle(text):
    

    return strip_ansi(text)


def secho(text, file=None, nl=True, err=False, color=None, **styles):
    

    return echo(style(text, **styles), file=file, nl=nl, err=err, color=color)


def edit(text=None, editor=None, env=None, require_save=True,
         extension='.txt', filename=None):
    r

    from ._termui_impl import Editor
    editor = Editor(editor=editor, env=env, require_save=require_save,
                    extension=extension)
    if filename is None:
        return editor.edit(text)
    editor.edit_file(filename)


def launch(url, wait=False, locate=False):
    

    from ._termui_impl import open_url
    return open_url(url, wait=wait, locate=locate)




_getchar = None


def getchar(echo=False):
    

    f = _getchar
    if f is None:
        from ._termui_impl import getchar as f
    return f(echo)


def pause(info='Press any key to continue ...', err=False):
    

    if not isatty(sys.stdin) or not isatty(sys.stdout):
        return
    try:
        if info:
            echo(info, nl=False, err=err)
        try:
            getchar()
        except (KeyboardInterrupt, EOFError):
            pass
    finally:
        if info:
            echo(err=err)
","

from __future__ import absolute_import
import inspect
from itertools import chain
import json
import logging
import re
import shlex
import textwrap

from django.conf import settings
from django.core.serializers.json import DjangoJSONEncoder

import six
from .utils import _load_class

DESIRED_EXCERPT_LENGTH = 100
ELLIPSIS = '<span class=""search-results-ellipsis""></span>'


log = logging.getLogger(__name__)  


class SearchResultProcessor(object):

    


    _results_fields = {}
    _match_phrase = None

    def __init__(self, dictionary, match_phrase):
        self._results_fields = dictionary
        self._match_phrase = match_phrase

    @staticmethod
    def strings_in_dictionary(dictionary):
        

        strings = [value for value in six.itervalues(dictionary) if not isinstance(value, dict)]
        for child_dict in [dv for dv in six.itervalues(dictionary) if isinstance(dv, dict)]:
            strings.extend(SearchResultProcessor.strings_in_dictionary(child_dict))
        return strings

    @staticmethod
    def find_matches(strings, words, length_hoped):
        

        lower_words = [w.lower() for w in words]

        def has_match(string):
            

            lower_string = string.lower()
            for test_word in lower_words:
                if test_word in lower_string:
                    return True
            return False

        shortened_strings = [textwrap.wrap(s) for s in strings]
        short_string_list = list(chain.from_iterable(shortened_strings))
        matches = [ms for ms in short_string_list if has_match(ms)]

        cumulative_len = 0
        break_at = None
        for idx, match in enumerate(matches):
            cumulative_len += len(match)
            if cumulative_len >= length_hoped:
                break_at = idx
                break

        return matches[0:break_at]

    @staticmethod
    def decorate_matches(match_in, match_word):
        

        matches = re.finditer(match_word, match_in, re.IGNORECASE)
        for matched_string in set([match.group() for match in matches]):
            match_in = match_in.replace(
                matched_string,
                getattr(settings, ""SEARCH_MATCH_DECORATION"", u""<b>{}</b>"").format(matched_string)
            )
        return match_in

    
    def should_remove(self, user):  
        

        return False

    def add_properties(self):
        

        for property_name in [p[0] for p in inspect.getmembers(self.__class__) if isinstance(p[1], property)]:
            self._results_fields[property_name] = getattr(self, property_name, None)

    @classmethod
    def process_result(cls, dictionary, match_phrase, user):
        

        result_processor = _load_class(getattr(settings, ""SEARCH_RESULT_PROCESSOR"", None), cls)
        srp = result_processor(dictionary, match_phrase)
        if srp.should_remove(user):
            return None
        try:
            srp.add_properties()
        
        except Exception as ex:  
            log.exception(""error processing properties for %s - %s: will remove from results"",
                          json.dumps(dictionary, cls=DjangoJSONEncoder), str(ex))
            return None
        return dictionary

    @property
    def excerpt(self):
        

        if ""content"" not in self._results_fields:
            return None

        match_phrases = [self._match_phrase]
        if six.PY2:
            separate_phrases = [
                phrase.decode('utf-8')
                for phrase in shlex.split(self._match_phrase.encode('utf-8'))
            ]
        else:
            separate_phrases = [
                phrase
                for phrase in shlex.split(self._match_phrase)
            ]
        if len(separate_phrases) > 1:
            match_phrases.extend(separate_phrases)
        else:
            match_phrases = separate_phrases

        matches = SearchResultProcessor.find_matches(
            SearchResultProcessor.strings_in_dictionary(self._results_fields[""content""]),
            match_phrases,
            DESIRED_EXCERPT_LENGTH
        )
        excerpt_text = ELLIPSIS.join(matches)

        for match_word in match_phrases:
            excerpt_text = SearchResultProcessor.decorate_matches(excerpt_text, match_word)

        return excerpt_text
","




















import networkx as nx

from nupic.frameworks.viz import DotRenderer as DEFAULT_RENDERER



class NetworkVisualizer(object):
  


  def __init__(self, network):
    self.network = network


  def export(self):
    

    graph = nx.MultiDiGraph()

    
    regions = self.network.getRegions()

    for idx in xrange(regions.getCount()):
      regionPair = regions.getByIndex(idx)
      regionName = regionPair[0]
      graph.add_node(regionName, label=regionName)

    
    
    for linkName, link in self.network.getLinks():
      graph.add_edge(link.getSrcRegionName(),
                     link.getDestRegionName(),
                     src=link.getSrcOutputName(),
                     dest=link.getDestInputName())

    return graph


  def render(self, renderer=DEFAULT_RENDERER):
    

    renderer().render(self.export())
","







import urllib
import xml.dom.minidom

from url import URLAccumulator, HTTP403Forbidden
from html import replace_entities
from cache import Cache

def clear_cache():
    Cache(""yahoo"").clear()



YAHOO_ID         = ""Bsx0rSzV34HQ9sXprWCaAWCHCINnLFtRF_4wahO1tiVEPpFSltMdqkM1z6Xubg""



YAHOO_SEARCH     = ""search""
YAHOO_IMAGES     = ""images""
YAHOO_NEWS       = ""news""
YAHOO_SPELLING   = ""spelling""



class YahooError(Exception):
    def __str__(self): return str(self.__class__) 
    
class YahooLimitError(YahooError):
    
    def __str__(self): return str(self.__class__) 



def license_key(id=None):
    
    global YAHOO_ID
    if id != None:
        YAHOO_ID = id
    return YAHOO_ID



def format_data(s):
    
    

    
    return s.encode(""utf-8"")


    
class YahooResult:

    

    
    def __init__(self):
        
        self.title       = None
        self.url         = None
        self.description = None
        self.type        = None
        self.date        = None
        self.width       = None 
        self.height      = None 
        self.source      = None 
        self.language    = None 

    def __repr__(self):
        
        s = format_data(self.url)
        return s




class YahooResults(list):
    
    

    
    def __init__(self, q, data, service=YAHOO_SEARCH):
        
        self.query = q
        self.total = 0
        if data == """": return
        dom = xml.dom.minidom.parseString(data)
        doc = dom.childNodes[0]
        self.total = int(doc.attributes[""totalResultsAvailable""].value)
        
        for r in doc.getElementsByTagName('Result'):
            
            item = YahooResult()
            item.title        = self._parse(r, 'Title')
            item.url          = self._parse(r, 'Url')
            item.description  = self._parse(r, 'Summary')
            
            if service == YAHOO_SEARCH:
                item.type     = self._parse(r, 'MimeType')
                item.date     = self._parse(r, 'ModificationDate')
            if service == YAHOO_IMAGES:
                item.type     = self._parse(r, 'FileFormat')
                item.width    = int(self._parse(r, 'Width'))
                item.height   = int(self._parse(r, 'Height'))
            if service == YAHOO_NEWS:
                item.date     = self._parse(r, 'ModificationDate')
                item.source   = self._parse(r, 'NewsSourceUrl')
                item.language = self._parse(r, 'Language')
            
            self.append(item)
            
    def _parse(self, e, tag):
        
        

        
        tags = e.getElementsByTagName(tag)
        children = tags[0].childNodes
        if len(children) != 1: return None
        assert children[0].nodeType == xml.dom.minidom.Element.TEXT_NODE
        
        s = children[0].nodeValue
        s = format_data(s)
        s = replace_entities(s)
        
        return s
        
    def __cmp__(self, other):
        
        

    
        if self.total > other.total:
            return 1
        elif self.total < other.total: 
            return -1
        else:
            return 0



class YahooSearch(YahooResults, URLAccumulator):
    
    def __init__(self, q, start=1, count=10, service=YAHOO_SEARCH, context=None, 
                 wait=10, asynchronous=False, cached=True):

        

        
        self.query = q
        self.service = service
        
        if cached:
            cache = ""yahoo""
        else:
            cache = None

        url = ""http://search.yahooapis.com/""
        if service == YAHOO_SEARCH and context == None : url += ""WebSearchService/V1/webSearch?""
        if service == YAHOO_SEARCH and context != None : url += ""WebSearchService/V1/contextSearch?""
        if service == YAHOO_IMAGES   :  url += ""ImageSearchService/V1/imageSearch?""
        if service == YAHOO_NEWS     :  url += ""NewsSearchService/V1/newsSearch?""
        if service == YAHOO_SPELLING :  url += ""WebSearchService/V1/spellingSuggestion?""
        arg = urllib.urlencode(((""appid"", YAHOO_ID), 
                                (""query"", q),
                                (""start"", start),
                                (""results"", count),
                                (""context"", unicode(context))))
        
        url += arg
        URLAccumulator.__init__(self, url, wait, asynchronous, cache, "".xml"")
        
    def load(self, data):
        
        if str(self.error.__class__) == str(HTTP403Forbidden().__class__):
            self.error = YahooLimitError()
            
        YahooResults.__init__(self, self.query, data, self.service)



def search(q, start=1, count=10, context=None, wait=10, asynchronous=False, cached=False):
    
    

    
    service = YAHOO_SEARCH
    return YahooSearch(q, start, count, service, context, wait, asynchronous, cached)  

def search_images(q, start=1, count=10, wait=10, asynchronous=False, cached=False):
    
    

    
    service = YAHOO_IMAGES
    return YahooSearch(q, start, count, service, None, wait, asynchronous, cached)   

def search_news(q, start=1, count=10, wait=10, asynchronous=False, cached=False):
    
    

    
    service = YAHOO_NEWS
    return YahooSearch(q, start, count, service, None, wait, asynchronous, cached)  



class YahooSpelling(YahooSearch):
    
    def __init__(self, q, wait, asynchronous, cached):
        
        service = YAHOO_SPELLING
        YahooSearch.__init__(self, q, 1, 1, service, None, wait, asynchronous, cached)
        
    def load(self, data):
        
        dom = xml.dom.minidom.parseString(data)
        doc = dom.childNodes[0]
        r = doc.getElementsByTagName('Result')
        if len(r) > 0:
            r = r[0].childNodes[0].nodeValue
            r = format_data(r)
        else:
            r = q
        
        self.append(r)

def suggest_spelling(q, wait=10, asynchronous=False, cached=False):
    
    

    
    return YahooSpelling(q, wait, asynchronous, cached)



def sort(words, context="""", strict=True, relative=True, service=YAHOO_SEARCH,
         wait=10, asynchronous=False, cached=False):
    
    

    
    results = []
    for word in words:
        q = word + "" "" + context
        q.strip()
        if strict: q = ""\""""+q+""\""""
        r = YahooSearch(q, 1, 1, service, context, wait, asynchronous, cached)
        results.append(r)
        
    results.sort(YahooResults.__cmp__)
    results.reverse()
    
    if relative and len(results) > 0:
        sum = 0.000000000000000001
        for r in results: sum += r.total
        for r in results: r.total /= float(sum)
    
    results = [(r.query, r.total) for r in results]
    return results






































#    print item.title","























import numpy as np
from nupic.bindings.math import Random



class SequenceMachine(object):
  


  def __init__(self,
               patternMachine,
               seed=42):
    

    
    self.patternMachine = patternMachine

    
    self._random = Random(seed)


  def generateFromNumbers(self, numbers):
    

    sequence = []

    for number in numbers:
      if number == None:
        sequence.append(number)
      else:
        pattern = self.patternMachine.get(number)
        sequence.append(pattern)

    return sequence


  def addSpatialNoise(self, sequence, amount):
    

    newSequence = []

    for pattern in sequence:
      if pattern is not None:
        pattern = self.patternMachine.addNoise(pattern, amount)
      newSequence.append(pattern)

    return newSequence


  def prettyPrintSequence(self, sequence, verbosity=1):
    

    text = """"

    for i in xrange(len(sequence)):
      pattern = sequence[i]

      if pattern == None:
        text += ""<reset>""
        if i < len(sequence) - 1:
          text += ""\n""
      else:
        text += self.patternMachine.prettyPrintPattern(pattern,
                                                       verbosity=verbosity)

    return text


  def generateNumbers(self, numSequences, sequenceLength, sharedRange=None):
    

    numbers = []

    if sharedRange:
      sharedStart, sharedEnd = sharedRange
      sharedLength = sharedEnd - sharedStart
      sharedNumbers = range(numSequences * sequenceLength,
                            numSequences * sequenceLength + sharedLength)

    for i in xrange(numSequences):
      start = i * sequenceLength
      newNumbers = np.array(range(start, start + sequenceLength), np.uint32)
      self._random.shuffle(newNumbers)
      newNumbers = list(newNumbers)

      if sharedRange is not None:
        newNumbers[sharedStart:sharedEnd] = sharedNumbers

      numbers += newNumbers
      numbers.append(None)

    return numbers
","






from __future__ import print_function





import logging
from astrobase import log_sub, log_fmt, log_date_fmt

DEBUG = False
if DEBUG:
    level = logging.DEBUG
else:
    level = logging.INFO
LOGGER = logging.getLogger(__name__)
logging.basicConfig(
    level=level,
    style=log_sub,
    format=log_fmt,
    datefmt=log_date_fmt,
)

LOGDEBUG = LOGGER.debug
LOGINFO = LOGGER.info
LOGWARNING = LOGGER.warning
LOGERROR = LOGGER.error
LOGEXCEPTION = LOGGER.exception






import os.path
import os

import numpy as np



import astropy.time as astime



from jplephem.spk import SPK







modpath = os.path.abspath(os.path.dirname(__file__))
planetdatafile = os.path.join(modpath,'data/de430.bsp')




try:

    
    jplkernel = SPK.open(planetdatafile)
    HAVEKERNEL = True

except Exception as e:

    
    def on_download_chunk(transferred,blocksize,totalsize):
        progress = transferred*blocksize/float(totalsize)*100.0
        print('{progress:.1f}%'.format(progress=progress),end='\r')

    
    spkurl = (
        'https://naif.jpl.nasa.gov/'
        'pub/naif/generic_kernels/spk/planets/de430.bsp'
    )

    LOGINFO('JPL kernel de430.bsp not found. Downloading from:\n\n%s\n' %
            spkurl)
    try:
        from urllib import urlretrieve
    except Exception as e:
        from urllib.request import urlretrieve

    localf, headerr = urlretrieve(
        spkurl,planetdatafile,reporthook=on_download_chunk
    )
    if os.path.exists(localf):
        print('\nDone.')
        jplkernel = SPK.open(planetdatafile)
        HAVEKERNEL = True
    else:
        print('failed to download the JPL kernel!')
        HAVEKERNEL = False








CLIGHT_KPS = 299792.458


JD1800 = 2378495.0
JD2000 = 2451545.0
JD2000INT = 2451545
JD2050 = 2469807.5


MAS_P_YR_TO_RAD_P_DAY = 1.3273475e-11
ARCSEC_TO_RADIANS = 4.84813681109536e-6
KM_P_AU = 1.49597870691e8
SEC_P_DAY = 86400.0





EMRAT = 81.30056941599857






def precess_coordinates(ra, dec,
                        epoch_one, epoch_two,
                        jd=None,
                        mu_ra=0.0,
                        mu_dec=0.0,
                        outscalar=False):
    


    raproc, decproc = np.radians(ra), np.radians(dec)

    if ((mu_ra != 0.0) and (mu_dec != 0.0) and jd):

        jd_epoch_one = JD2000 + (epoch_one - epoch_two)*365.25
        raproc = (
            raproc +
            (jd - jd_epoch_one)*mu_ra*MAS_P_YR_TO_RAD_P_DAY/np.cos(decproc)
        )
        decproc = decproc + (jd - jd_epoch_one)*mu_dec*MAS_P_YR_TO_RAD_P_DAY

    ca = np.cos(raproc)
    cd = np.cos(decproc)
    sa = np.sin(raproc)
    sd = np.sin(decproc)

    if epoch_one != epoch_two:

        t1 = 1.0e-3 * (epoch_two - epoch_one)
        t2 = 1.0e-3 * (epoch_one - 2000.0)

        a = ( t1*ARCSEC_TO_RADIANS * (23062.181 + t2*(139.656 + 0.0139*t2) +
                                      t1*(30.188 - 0.344*t2+17.998*t1)) )
        b = t1*t1*ARCSEC_TO_RADIANS*(79.280 + 0.410*t2 + 0.205*t1) + a
        c = (
            ARCSEC_TO_RADIANS*t1*(20043.109 - t2*(85.33 + 0.217*t2) +
                                  t1*(-42.665 - 0.217*t2 - 41.833*t2))
        )
        sina, sinb, sinc = np.sin(a), np.sin(b), np.sin(c)
        cosa, cosb, cosc = np.cos(a), np.cos(b), np.cos(c)

        precmatrix = np.matrix([[cosa*cosb*cosc - sina*sinb,
                                 sina*cosb + cosa*sinb*cosc,
                                 cosa*sinc],
                                [-cosa*sinb - sina*cosb*cosc,
                                 cosa*cosb - sina*sinb*cosc,
                                 -sina*sinc],
                                [-cosb*sinc,
                                 -sinb*sinc,
                                 cosc]])

        precmatrix = precmatrix.transpose()

        x = (np.matrix([cd*ca, cd*sa, sd])).transpose()

        x2 = precmatrix * x

        outra = np.arctan2(x2[1],x2[0])
        outdec = np.arcsin(x2[2])


        outradeg = np.rad2deg(outra)
        outdecdeg = np.rad2deg(outdec)

        if outradeg < 0.0:
            outradeg = outradeg + 360.0

        if outscalar:
            return float(outradeg), float(outdecdeg)
        else:
            return outradeg, outdecdeg

    else:

        
        
        
        
        return np.degrees(raproc), np.degrees(decproc)







def _single_true(iterable):
    


    
    iterator = iter(iterable)

    
    has_true = any(iterator)

    
    has_another_true = any(iterator)

    return has_true and not has_another_true



def get_epochs_given_midtimes_and_period(
        t_mid,
        period,
        err_t_mid=None,
        t0_fixed=None,
        t0_percentile=None,
        verbose=False
):
    


    kwargarr = np.array([isinstance(err_t_mid,np.ndarray),
                         t0_fixed,
                         t0_percentile])
    if not _single_true(kwargarr) and not np.all(~kwargarr.astype(bool)):
        raise AssertionError(
            'can have at most one of err_t_mid, t0_fixed, t0_percentile')

    t_mid = t_mid[np.isfinite(t_mid)]
    N_midtimes = len(t_mid)

    if t0_fixed:
        t0 = t0_fixed
    elif isinstance(err_t_mid,np.ndarray):
        
        t0_avg = np.average(t_mid, weights=1/err_t_mid**2)
        t0_options = np.arange(min(t_mid), max(t_mid)+period, period)
        t0 = t0_options[np.argmin(np.abs(t0_options - t0_avg))]
    else:
        if not t0_percentile:
            
            
            
            if N_midtimes % 2 == 1:
                t0 = np.median(t_mid)
            else:
                t0 = t_mid[int(N_midtimes/2)]
        else:
            t0 = np.sort(t_mid)[int(N_midtimes*t0_percentile/100)]

    epoch = (t_mid - t0)/period

    
    int_epoch = np.round(epoch, 0)

    if verbose:
        LOGINFO('epochs before rounding')
        LOGINFO('\n{:s}'.format(repr(epoch)))
        LOGINFO('epochs after rounding')
        LOGINFO('\n{:s}'.format(repr(int_epoch)))

    return int_epoch, t0







def unixtime_to_jd(unix_time):
    


    
    jdutc = astime.Time(unix_time, format='unix', scale='utc')
    return jdutc.jd



def datetime_to_jd(dt):
    


    jdutc = astime.Time(dt, format='datetime',scale='utc')
    return jdutc.jd



def jd_to_datetime(jd, returniso=False):
    


    tt = astime.Time(jd, format='jd', scale='utc')

    if returniso:
        return tt.iso
    else:
        return tt.datetime



def jd_now():
    

    return astime.Time.now().jd



def jd_to_mjd(jd):
    


    return jd - 2400000.5



def mjd_to_jd(mjd):
    


    return mjd + 2400000.5






def jd_corr(jd,
            ra, dec,
            obslon=None,
            obslat=None,
            obsalt=None,
            jd_type='bjd'):
    


    if not HAVEKERNEL:
        LOGERROR('no JPL kernel available, can\'t continue!')
        return

    
    
    

    
    rarad = np.radians(ra)
    decrad = np.radians(dec)
    cosra = np.cos(rarad)
    sinra = np.sin(rarad)
    cosdec = np.cos(decrad)
    sindec = np.sin(decrad)

    
    src_unitvector = np.array([cosdec*cosra,cosdec*sinra,sindec])

    
    
    if (obslon is None) or (obslat is None) or (obsalt is None):
        t = astime.Time(jd, scale='utc', format='jd')
    else:
        t = astime.Time(jd, scale='utc', format='jd',
                        location=('%.5fd' % obslon,
                                  '%.5fd' % obslat,
                                  obsalt))

    
    
    
    barycenter_earthmoon = jplkernel[0,3].compute(t.tdb.jd)

    
    
    
    
    
    
    moonvector = (jplkernel[3,301].compute(t.tdb.jd) -
                  jplkernel[3,399].compute(t.tdb.jd))

    
    
    
    pos_earth = (barycenter_earthmoon - moonvector * 1.0/(1.0+EMRAT))

    if jd_type == 'bjd':

        
        
        
        
        
        
        
        correction_seconds = np.dot(pos_earth.T, src_unitvector)/CLIGHT_KPS
        correction_days = correction_seconds/SEC_P_DAY

    elif jd_type == 'hjd':

        

        
        
        pos_sun = jplkernel[0,10].compute(t.tdb.jd)

        
        
        sun_earth_vec = pos_earth - pos_sun

        
        correction_seconds = np.dot(sun_earth_vec.T, src_unitvector)/CLIGHT_KPS
        correction_days = correction_seconds/SEC_P_DAY

    
    new_jd = t.tdb.jd + correction_days

    return new_jd
","




















from __future__ import (
    absolute_import,
    division,
    print_function,
    unicode_literals,
)

from unicodedata import normalize

from six import text_type
from six.moves import range

from ._stemmer import _Stemmer

__all__ = ['Schinke', 'schinke']


class Schinke(_Stemmer):
    


    _keep_que = {
        'at',
        'quo',
        'ne',
        'ita',
        'abs',
        'aps',
        'abus',
        'adae',
        'adus',
        'deni',
        'de',
        'sus',
        'obli',
        'perae',
        'plenis',
        'quando',
        'quis',
        'quae',
        'cuius',
        'cui',
        'quem',
        'quam',
        'qua',
        'qui',
        'quorum',
        'quarum',
        'quibus',
        'quos',
        'quas',
        'quotusquis',
        'quous',
        'ubi',
        'undi',
        'us',
        'uter',
        'uti',
        'utro',
        'utribi',
        'tor',
        'co',
        'conco',
        'contor',
        'detor',
        'deco',
        'exco',
        'extor',
        'obtor',
        'optor',
        'retor',
        'reco',
        'attor',
        'inco',
        'intor',
        'praetor',
    }

    _n_endings = {
        4: {'ibus'},
        3: {'ius'},
        2: {
            'is',
            'nt',
            'ae',
            'os',
            'am',
            'ud',
            'as',
            'um',
            'em',
            'us',
            'es',
            'ia',
        },
        1: {'a', 'e', 'i', 'o', 'u'},
    }

    _v_endings_strip = {
        6: {},
        5: {},
        4: {'mini', 'ntur', 'stis'},
        3: {'mur', 'mus', 'ris', 'sti', 'tis', 'tur'},
        2: {'ns', 'nt', 'ri'},
        1: {'m', 'r', 's', 't'},
    }
    _v_endings_alter = {
        6: {'iuntur'},
        5: {'beris', 'erunt', 'untur'},
        4: {'iunt'},
        3: {'bor', 'ero', 'unt'},
        2: {'bo'},
        1: {},
    }

    def stem(self, word):
        

        word = normalize('NFKD', text_type(word.lower()))
        word = ''.join(
            c
            for c in word
            if c
            in {
                'a',
                'b',
                'c',
                'd',
                'e',
                'f',
                'g',
                'h',
                'i',
                'j',
                'k',
                'l',
                'm',
                'n',
                'o',
                'p',
                'q',
                'r',
                's',
                't',
                'u',
                'v',
                'w',
                'x',
                'y',
                'z',
            }
        )

        
        word = word.replace('j', 'i').replace('v', 'u')

        
        if word[-3:] == 'que':
            
            
            if word[:-3] in self._keep_que or word == 'que':
                return {'n': word, 'v': word}
            else:
                word = word[:-3]

        
        noun = word
        verb = word

        
        for endlen in range(4, 0, -1):
            if word[-endlen:] in self._n_endings[endlen]:
                if len(word) - 2 >= endlen:
                    noun = word[:-endlen]
                else:
                    noun = word
                break

        for endlen in range(6, 0, -1):
            if word[-endlen:] in self._v_endings_strip[endlen]:
                if len(word) - 2 >= endlen:
                    verb = word[:-endlen]
                else:
                    verb = word
                break
            if word[-endlen:] in self._v_endings_alter[endlen]:
                if word[-endlen:] in {
                    'iuntur',
                    'erunt',
                    'untur',
                    'iunt',
                    'unt',
                }:
                    new_word = word[:-endlen] + 'i'
                    addlen = 1
                elif word[-endlen:] in {'beris', 'bor', 'bo'}:
                    new_word = word[:-endlen] + 'bi'
                    addlen = 2
                else:
                    new_word = word[:-endlen] + 'eri'
                    addlen = 3

                
                
                if len(new_word) >= 2 + addlen:
                    verb = new_word
                else:
                    verb = word
                break

        return {'n': noun, 'v': verb}


def schinke(word):
    

    return Schinke().stem(word)


if __name__ == '__main__':
    import doctest

    doctest.testmod()
","class SpreadingStop:

    def __init__(self, stop_I, min_transfer_time):
        self.stop_I = stop_I
        self.min_transfer_time = min_transfer_time
        self.visit_events = []

    def get_min_visit_time(self):
        

        if not self.visit_events:
            return float('inf')
        else:
            return min(self.visit_events, key=lambda event: event.arr_time_ut).arr_time_ut

    def get_min_event(self):
        if len(self.visit_events) == 0:
            return None
        else:
            return min(self.visit_events, key=lambda event: event.arr_time_ut)

    def visit(self, event):
        

        to_visit = False
        if event.arr_time_ut <= self.min_transfer_time+self.get_min_visit_time():
            to_visit = True
        else:
            for ve in self.visit_events:
                if (event.trip_I == ve.trip_I) and event.arr_time_ut < ve.arr_time_ut:
                    to_visit = True

        if to_visit:
            self.visit_events.append(event)
            min_time = self.get_min_visit_time()
            
            self.visit_events = [v for v in self.visit_events if v.arr_time_ut <= min_time+self.min_transfer_time]
        return to_visit

    def has_been_visited(self):
        return len(self.visit_events) > 0

    def can_infect(self, event):
        

        if event.from_stop_I != self.stop_I:
            return False

        if not self.has_been_visited():
            return False
        else:
            time_sep = event.dep_time_ut-self.get_min_visit_time()
            
            
            
            if (time_sep >= self.min_transfer_time) or (event.trip_I == -1 and time_sep >= 0):
                return True
            else:
                for visit in self.visit_events:
                    
                    if (event.trip_I == visit.trip_I) and (time_sep >= 0):
                        return True
            return False
","
from __future__ import absolute_import, unicode_literals, division
import re
import six
from importlib import import_module
from armet import utils, authentication, authorization
from armet.exceptions import ImproperlyConfigured
from armet import connectors as included_connectors


def _merge(options, name, bases, default=None):
    

    result = None
    for base in bases:
        if base is None:
            continue

        value = getattr(base, name, None)
        if value is None:
            continue

        result = utils.cons(result, value)

    value = options.get(name)
    if value is not None:
        result = utils.cons(result, value)

    return result or default


class ResourceOptions(object):

    def __init__(self, meta, name, data, bases):
        

        
        
        self.debug = meta.get('debug')
        if self.debug is None:
            self.debug = False

        
        
        
        
        
        self.abstract = data.get('abstract')

        
        
        
        
        
        self.name = meta.get('name')
        if self.name is None:
            
            
            dashed = utils.dasherize(name).strip()
            if dashed:
                
                self.name = re.sub(r'-resource$', '', dashed)

            else:
                
                self.name = name

        elif callable(self.name):
            
            
            self.name = self.name(name)

        
        
        
        
        
        
        self.asynchronous = meta.get('asynchronous', False)

        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        self.connectors = connectors = _merge(meta, 'connectors', bases, {})

        if not connectors.get('http') and not self.abstract:
            raise ImproperlyConfigured('No valid HTTP connector was detected.')

        
        for key in connectors:
            connector = connectors[key]
            if isinstance(connector, six.string_types):
                if connector in getattr(included_connectors, key):
                    
                    connectors[key] = 'armet.connectors.{}'.format(connector)

        
        
        
        
        
        
        
        
        
        
        
        
        self.options = options = _merge(meta, 'options', bases, {})
        for name in options:
            
            setattr(self, name, meta.get(name))

        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        self.patterns = meta.get('patterns', [])
        for index, pattern in enumerate(self.patterns):
            
            if isinstance(pattern, six.string_types):
                pattern = (None, pattern)

            
            self.patterns[index] = (pattern[0], re.compile(pattern[1]))

        
        
        
        
        self.trailing_slash = meta.get('trailing_slash', True)

        
        
        
        
        self.http_allowed_methods = meta.get('http_allowed_methods')
        if self.http_allowed_methods is None:
            self.http_allowed_methods = (
                'HEAD',
                'OPTIONS',
                'GET',
                'POST',
                'PUT',
                'PATCH',
                'DELETE',
                'LINK',
                'UNLINK'
            )

        
        
        self.http_allowed_headers = meta.get('http_allowed_headers')
        if self.http_allowed_headers is None:
            self.http_allowed_headers = (
                'Content-Type',
                'Authorization',
                'Accept',
                'Origin'
            )

        
        
        self.http_exposed_headers = meta.get('http_exposed_headers')
        if self.http_exposed_headers is None:
            self.http_exposed_headers = (
                'Content-Type',
                'Authorization',
                'Accept',
                'Origin'
            )

        
        
        
        
        
        self.http_allowed_origins = meta.get('http_allowed_origins')
        if self.http_allowed_origins is None:
            self.http_allowed_origins = ()

        
        
        
        
        
        
        
        
        
        
        self.legacy_redirect = meta.get('legacy_redirect', True)

        
        
        
        self.serializers = serializers = meta.get('serializers')
        if not serializers:
            self.serializers = {
                'json': 'armet.serializers.JSONSerializer',
                'url': 'armet.serializers.URLSerializer'
            }

        
        for name, serializer in six.iteritems(self.serializers):
            if isinstance(serializer, six.string_types):
                segments = serializer.split('.')
                module = '.'.join(segments[:-1])
                module = import_module(module)
                self.serializers[name] = getattr(module, segments[-1])

        
        self.allowed_serializers = meta.get('allowed_serializers')
        if not self.allowed_serializers:
            self.allowed_serializers = tuple(self.serializers.keys())

        
        
        for name in self.allowed_serializers:
            if name not in self.serializers:
                raise ImproperlyConfigured(
                    'The allowed serializer, {}, is not one of the '
                    'understood serializers'.format(name))

        
        
        self.default_serializer = meta.get('default_serializer')
        if not self.default_serializer:
            if 'json' in self.allowed_serializers:
                self.default_serializer = 'json'

            else:
                self.default_serializer = self.allowed_serializers[0]

        if self.default_serializer not in self.allowed_serializers:
            raise ImproperlyConfigured(
                'The chosen default serializer, {}, is not one of the '
                'allowed serializers'.format(self.default_serializer))

        
        
        
        self.deserializers = deserializers = meta.get('deserializers')
        if not deserializers:
            self.deserializers = {
                'json': 'armet.deserializers.JSONDeserializer',
                'url': 'armet.deserializers.URLDeserializer'
            }

        
        for name, deserializer in six.iteritems(self.deserializers):
            if isinstance(deserializer, six.string_types):
                segments = deserializer.split('.')
                module = '.'.join(segments[:-1])
                module = import_module(module)
                self.deserializers[name] = getattr(module, segments[-1])

        
        self.allowed_deserializers = meta.get('allowed_deserializers')
        if not self.allowed_deserializers:
            self.allowed_deserializers = tuple(self.deserializers.keys())

        
        
        for name in self.allowed_deserializers:
            if name not in self.deserializers:
                raise ImproperlyConfigured(
                    'The allowed deserializer, {}, is not one of the '
                    'understood deserializers'.format(name))

        
        
        self.authentication = meta.get('authentication')
        if self.authentication is None:
            
            self.authentication = (authentication.Authentication(),)

        
        
        
        self.authorization = meta.get('authorization')
        if self.authorization is None:
            
            self.authorization = authorization.Authorization()
","



















from __future__ import absolute_import, division

__docformat__ = ""restructuredtext en""

import re
import weakref
import warnings
import socket
import logging

from encodings import idna

from .xmppstringprep import NODEPREP, RESOURCEPREP
from .exceptions import JIDError, StringprepError

logger = logging.getLogger(""pyxmpp2.jid"")


GOOD_OUTER = u""[^\x00-\x2C\x2E-\x2F\x3A-\x40\x5B-\x60\x7B-\x7F-]""
GOOD_INNER = u""[^\x00-\x2C\x2E-\x2F\x3A-\x40\x5B-\x60\x7B-\x7F]""
STD3_LABEL_RE = re.compile(u""^{0}({1}*{0})?$"".format(GOOD_OUTER, GOOD_INNER))


UNICODE_DOT_RE = re.compile(u""[\u3002\uFF0E\uFF61]"")

def are_domains_equal(domain1, domain2):
    


    domain1 = domain1.encode(""idna"")
    domain2 = domain2.encode(""idna"")
    return domain1.lower() == domain2.lower()

def _validate_ip_address(family, address):
    

    try:
        info = socket.getaddrinfo(address, 0, family, socket.SOCK_STREAM, 0,
                                                        socket.AI_NUMERICHOST)
    except socket.gaierror, err:
        logger.debug(""gaierror: {0} for {1!r}"".format(err, address))
        raise ValueError(""Bad IP address"")

    if not info:
        logger.debug(""getaddrinfo result empty"")
        raise ValueError(""Bad IP address"")
    addr = info[0][4]
    logger.debug("" got address: {0!r}"".format(addr))

    try:
        return socket.getnameinfo(addr, socket.NI_NUMERICHOST)[0]
    except socket.gaierror, err:
        logger.debug(""gaierror: {0} for {1!r}"".format(err, addr))
        raise ValueError(""Bad IP address"")

class JID(object):
    

    cache = weakref.WeakValueDictionary()
    __slots__ = (""local"", ""domain"", ""resource"", ""__weakref__"",)
    def __new__(cls, local_or_jid = None, domain = None, resource = None,
                                                                check = True):
        


        if isinstance(local_or_jid, JID):
            return local_or_jid

        if domain is None and resource is None:
            obj = cls.cache.get(unicode(local_or_jid))
            if obj:
                return obj

        obj = object.__new__(cls)

        if local_or_jid:
            local_or_jid = unicode(local_or_jid)
        if (local_or_jid and not domain and not resource):
            local, domain, resource = cls.__from_unicode(local_or_jid)
            cls.cache[local_or_jid] = obj
        else:
            if domain is None and resource is None:
                raise JIDError(""At least domain must be given"")
            if check:
                local = cls.__prepare_local(local_or_jid)
                domain = cls.__prepare_domain(domain)
                resource = cls.__prepare_resource(resource)
            else:
                local = local_or_jid
        object.__setattr__(obj, ""local"", local)
        object.__setattr__(obj, ""domain"", domain)
        object.__setattr__(obj, ""resource"", resource)
        return obj

    def __setattr__(self, name, value):
        raise RuntimeError(""JID objects are immutable!"")

    def __attribute_declarations__(self):
        
        self.local = u""""
        self.domain = u""""
        self.resource = u""""

    @classmethod
    def __from_unicode(cls, data, check = True):
        

        parts1 = data.split(u""/"", 1)
        parts2 = parts1[0].split(u""@"", 1)
        if len(parts2) == 2:
            local = parts2[0]
            domain = parts2[1]
            if check:
                local = cls.__prepare_local(local)
                domain = cls.__prepare_domain(domain)
        else:
            local = None
            domain = parts2[0]
            if check:
                domain = cls.__prepare_domain(domain)
        if len(parts1) == 2:
            resource = parts1[1]
            if check:
                resource = cls.__prepare_resource(parts1[1])
        else:
            resource = None
        if not domain:
            raise JIDError(""Domain is required in JID."")
        return (local, domain, resource)

    @staticmethod
    def __prepare_local(data):
        

        if not data:
            return None
        data = unicode(data)
        try:
            local = NODEPREP.prepare(data)
        except StringprepError, err:
            raise JIDError(u""Local part invalid: {0}"".format(err))
        if len(local.encode(""utf-8"")) > 1023:
            raise JIDError(u""Local part too long"")
        return local

    @staticmethod
    def __prepare_domain(data):
        

        
        if not data:
            raise JIDError(""Domain must be given"")
        data = unicode(data)
        if not data:
            raise JIDError(""Domain must be given"")
        if u'[' in data:
            if data[0] == u'[' and data[-1] == u']':
                try:
                    addr = _validate_ip_address(socket.AF_INET6, data[1:-1])
                    return ""[{0}]"".format(addr)
                except ValueError, err:
                    logger.debug(""ValueError: {0}"".format(err))
                    raise JIDError(u""Invalid IPv6 literal in JID domainpart"")
            else:
                raise JIDError(u""Invalid use of '[' or ']' in JID domainpart"")
        elif data[0].isdigit() and data[-1].isdigit():
            try:
                addr = _validate_ip_address(socket.AF_INET, data)
            except ValueError, err:
                logger.debug(""ValueError: {0}"".format(err))
        data = UNICODE_DOT_RE.sub(u""."", data)
        data = data.rstrip(u""."")
        labels = data.split(u""."")
        try:
            labels = [idna.nameprep(label) for label in labels]
        except UnicodeError:
            raise JIDError(u""Domain name invalid"")
        for label in labels:
            if not STD3_LABEL_RE.match(label):
                raise JIDError(u""Domain name invalid"")
            try:
                idna.ToASCII(label)
            except UnicodeError:
                raise JIDError(u""Domain name invalid"")
        domain = u""."".join(labels)
        if len(domain.encode(""utf-8"")) > 1023:
            raise JIDError(u""Domain name too long"")
        return domain

    @staticmethod
    def __prepare_resource(data):
        

        if not data:
            return None
        data = unicode(data)
        try:
            resource = RESOURCEPREP.prepare(data)
        except StringprepError, err:
            raise JIDError(u""Local part invalid: {0}"".format(err))
        if len(resource.encode(""utf-8"")) > 1023:
            raise JIDError(""Resource name too long"")
        return resource

    def __unicode__(self):
        return self.as_unicode()

    def __repr__(self):
        return ""JID(%r)"" % (self.as_unicode())

    def as_utf8(self):
        

        return self.as_unicode().encode(""utf-8"")

    def as_string(self):
        

        warnings.warn(""JID.as_string() is deprecated. Use unicode()""
                "" or `as_utf8` instead."", DeprecationWarning, stacklevel=1)
        return self.as_utf8()

    def as_unicode(self):
        

        result = self.domain
        if self.local:
            result = self.local + u'@' + result
        if self.resource:
            result = result + u'/' + self.resource
        if not JID.cache.has_key(result):
            JID.cache[result] = self
        return result

    def bare(self):
        

        return JID(self.local, self.domain, check = False)

    def __eq__(self, other):
        if other is None:
            return False
        elif type(other) in (str, unicode):
            try:
                other = JID(other)
            except StandardError:
                return False
        elif not isinstance(other, JID):
            return False

        return (self.local == other.local
            and are_domains_equal(self.domain, other.domain)
            and self.resource == other.resource)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if other is None:
            return False
        return unicode(self) < unicode(other)

    def __gt__(self, other):
        if other is None:
            return True
        return unicode(self) > unicode(other)

    def __le__(self, other):
        if other is None:
            return False
        return unicode(self) <= unicode(other)

    def __ge__(self, other):
        if other is None:
            return True
        return unicode(self) >= unicode(other)

    def __hash__(self):
        return hash(self.local) ^ hash(self.domain) ^ hash(self.resource)


","






from .metadata import MetadataObject
from .sortabledict import SortableDict
from collections import MutableSequence
from .version import Version, VER_3_0, VER_2_0

class Grid(MutableSequence):
    


    def __init__(self, version=None, metadata=None, columns=None):
        

        
        version_given = version is not None
        if version_given:
            version = Version(version)
        else:
            version = VER_2_0
        self._version   = version
        self._version_given = version_given

        
        self.metadata   = MetadataObject(validate_fn=self._detect_or_validate)

        
        self.column     = SortableDict()

        
        self._row       = []

        if metadata is not None:
            self.metadata.update(metadata.items())

        if columns is not None:
            if isinstance(columns, dict) or isinstance(columns, SortableDict):
                columns = list(columns.items())

            for col_id, col_meta in columns:
                
                if isinstance(col_meta, dict) or \
                        isinstance(col_meta, SortableDict):
                    col_meta = list(col_meta.items())

                mo = MetadataObject(validate_fn=self._detect_or_validate)
                mo.extend(col_meta)
                self.column.add_item(col_id, mo)

    @property
    def version(self): 
        
        return self._version

    @property
    def nearest_version(self): 
        
        return Version.nearest(self._version)

    @property
    def ver_str(self): 
        
        return str(self.version)

    def __repr__(self): 
        
        

        parts = [u'\tVersion: %s' % self.ver_str]
        if bool(self.metadata):
            parts.append(u'\tMetadata: %s' % self.metadata)

        column_meta = []
        for col, col_meta in self.column.items():
            if bool(col_meta):
                column_meta.append(u'\t\t%s: %s' % (col, col_meta))
            else:
                column_meta.append(u'\t\t%s' % col)

        if bool(column_meta):
            parts.append(u'\tColumns:\n%s' % '\n'.join(column_meta))
        elif len(self.column):
            parts.append(u'\tColumns: %s' % ', '.join(self.column.keys()))
        else:
            parts.append(u'\tNo columns')

        if bool(self):
            parts.extend([
                u'\tRow %4d:\n\t%s' % (row, u'\n\t'.join([
                    ((u'%s=%r' % (col, data[col])) \
                            if col in data else \
                    (u'%s absent' % col)) for col \
                    in self.column.keys()]))
                for (row, data) in enumerate(self)
            ])
        else:
            parts.append(u'\tNo rows')
        
        class_name = self.__class__.__name__
        return u'<%s>\n%s\n</%s>' % (
                class_name, u'\n'.join(parts), class_name
        )

    def __getitem__(self, index):
        

        return self._row[index]

    def __len__(self):
        

        return len(self._row)

    def __setitem__(self, index, value):
        

        if not isinstance(value, dict):
            raise TypeError('value must be a dict')
        for val in value.values():
            self._detect_or_validate(val)
        self._row[index] = value

    def __delitem__(self, index):
        

        del self._row[index]

    def insert(self, index, value):
        

        if not isinstance(value, dict):
            raise TypeError('value must be a dict')
        for val in value.values():
            self._detect_or_validate(val)
        self._row.insert(index, value)

    def _detect_or_validate(self, val):
        

        if isinstance(val, list) \
                or isinstance(val, dict) \
                or isinstance(val, SortableDict) \
                or isinstance(val, Grid):
            
            self._assert_version(VER_3_0)

    def _assert_version(self, version):
        

        if self.nearest_version < version:
            if self._version_given:
                raise ValueError(
                        'Data type requires version %s' \
                        % version)
            else:
                self._version = version
","























import numpy as np
import random
from nupic.bindings.algorithms import SpatialPooler as SP



uintType = ""uint32""



class Example(object):
  



  def __init__(self, inputDimensions, columnDimensions):
    

    self.inputDimensions = inputDimensions
    self.columnDimensions = columnDimensions
    self.inputSize = np.array(inputDimensions).prod()
    self.columnNumber = np.array(columnDimensions).prod()
    self.inputArray = np.zeros(self.inputSize, dtype=uintType)
    self.activeArray = np.zeros(self.columnNumber, dtype=uintType)

    random.seed(1)

    self.sp = SP(self.inputDimensions,
                 self.columnDimensions,
                 potentialRadius = self.inputSize,
                 numActiveColumnsPerInhArea = int(0.02*self.columnNumber),
                 globalInhibition = True,
                 seed = 1,
                 synPermActiveInc = 0.01,
                 synPermInactiveDec = 0.008)


  def createInput(self):
    


    print ""-"" * 70 + ""Creating a random input vector"" + ""-"" * 70

    
    self.inputArray[0:] = 0

    for i in range(self.inputSize):
      
      self.inputArray[i] = random.randrange(2)


  def run(self):
    


    print ""-"" * 80 + ""Computing the SDR"" + ""-"" * 80

    
    self.sp.compute(self.inputArray, True, self.activeArray)

    print self.activeArray.nonzero()


  def addNoise(self, noiseLevel):
    


    for _ in range(int(noiseLevel * self.inputSize)):
      
      
      randomPosition = int(random.random() * self.inputSize)

      
      if self.inputArray[randomPosition] == 1:
        self.inputArray[randomPosition] = 0

      else:
        self.inputArray[randomPosition] = 1

      
      

example = Example((32, 32), (64, 64))


print ""\n \nFollowing columns represent the SDR""
print ""Different set of columns each time since we randomize the input""
print ""Lesson - different input vectors give different SDRs\n\n""


for i in range(3):
  example.createInput()
  example.run()


print ""\n\nIdentical SDRs because we give identical inputs""
print ""Lesson - identical inputs give identical SDRs\n\n""

print ""-"" * 75 + ""Using identical input vectors"" + ""-"" * 75


for i in range(2):
  example.run()


print ""\n\nNow we are changing the input vector slightly.""
print ""We change a small percentage of 1s to 0s and 0s to 1s.""
print ""The resulting SDRs are similar, but not identical to the original SDR""
print ""Lesson - Similar input vectors give similar SDRs\n\n""



print ""-"" * 75 + ""After adding 10% noise to the input vector"" + ""-"" * 75
example.addNoise(0.1)
example.run()



print ""-"" * 75 + ""After adding another 20% noise to the input vector"" + ""-"" * 75
example.addNoise(0.2)
example.run()
","
from peri.viz import base

from builtins import range

import matplotlib as mpl
import matplotlib.pylab as pl

from matplotlib import ticker
from matplotlib.gridspec import GridSpec
from matplotlib.offsetbox import AnchoredText
from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes
from mpl_toolkits.axes_grid1.inset_locator import mark_inset
from mpl_toolkits.axes_grid1 import ImageGrid
from matplotlib.patches import Circle, Rectangle

from peri.test import analyze
from peri import util
from peri.logger import log

import numpy as np
import time
import pickle
plt = mpl.pyplot





def lbl(axis, label, size=22):
    

    at = AnchoredText(label, loc=2, prop=dict(size=size), frameon=True)
    at.patch.set_boxstyle(""round,pad=0.,rounding_size=0.0"")
    
    
    
    
    
    
    axis.add_artist(at)

def summary_plot(state, samples, zlayer=None, xlayer=None, truestate=None):
    def MAD(d):
        return np.median(np.abs(d - np.median(d)))

    s = state
    t = s.get_model_image()

    if zlayer is None:
        zlayer = t.shape[0]//2

    if xlayer is None:
        xlayer = t.shape[2]//2

    mu = samples.mean(axis=0)
    std = samples.std(axis=0)

    fig, axs = pl.subplots(3,3, figsize=(20,12))
    axs[0][0].imshow(s.image[zlayer], vmin=0, vmax=1)
    axs[0][1].imshow(t[zlayer], vmin=0, vmax=1)
    axs[0][2].imshow((s.image-t)[zlayer], vmin=-1, vmax=1)
    axs[0][0].set_xticks([])
    axs[0][0].set_yticks([])
    axs[0][1].set_xticks([])
    axs[0][1].set_yticks([])
    axs[0][2].set_xticks([])
    axs[0][2].set_yticks([])

    axs[1][0].imshow(s.image[:,:,xlayer], vmin=0, vmax=1)
    axs[1][1].imshow(t[:,:,xlayer], vmin=0, vmax=1)
    axs[1][2].imshow((s.image-t)[:,:,xlayer], vmin=-1, vmax=1)
    axs[1][0].set_xticks([])
    axs[1][0].set_yticks([])
    axs[1][1].set_xticks([])
    axs[1][1].set_yticks([])
    axs[1][2].set_xticks([])
    axs[1][2].set_yticks([])

    try:
        alpha = 0.5 if truestate is not None else 0.8
        axs[2][0].hist(std[s.b_rad], bins=np.logspace(-3,0,50), label='Radii',
                histtype='stepfilled', alpha=alpha, color='red')
        if truestate is not None:
            d = np.abs(mu - truestate)
            axs[2][0].hist(d[s.b_pos], bins=np.logspace(-3,0,50), color='red',
                    histtype='step', alpha=1)
        axs[2][0].semilogx()

        axs[2][0].hist(std[s.b_pos], bins=np.logspace(-3,0,50), label='Positions',
                histtype='stepfilled', alpha=alpha, color='blue')
        if truestate is not None:
            d = np.abs(mu - truestate)
            axs[2][0].hist(d[s.b_rad], bins=np.logspace(-3,0,50), color='blue',
                    histtype='step', alpha=1)
        axs[2][0].semilogx()
        axs[2][0].legend(loc='upper right')
        axs[2][0].set_xlabel(""Estimated standard deviation"")
        axs[2][0].set_ylim(bottom=0)
    except Exception as e:
        pass

    d = s.state[s.b_rad]
    m = 2*1.4826 * MAD(d)
    mb = d.mean()

    d = d[(d > mb - m) & (d < mb +m)]
    d = s.state[s.b_rad]
    axs[2][1].hist(d, bins=50, histtype='stepfilled', alpha=0.8)
    axs[2][1].set_xlabel(""Radii"")
    axs[2][1].set_ylim(bottom=0)

    if truestate is not None:
        axs[2][1].hist(truestate[s.b_rad], bins=50, histtype='step', alpha=0.8)

    axs[2][2].hist((s.image-t)[s.image_mask==1].ravel(), bins=150,
            histtype='stepfilled', alpha=0.8)
    axs[2][2].set_xlim(-0.35, 0.35)
    axs[2][2].semilogy()
    axs[2][2].set_ylim(bottom=0)
    axs[2][2].set_xlabel(""Pixel value differences"")

    pl.subplots_adjust(left=0, right=1, bottom=0, top=1, wspace=0.05, hspace=0.05)
    pl.tight_layout()

def pretty_summary(state, samples, zlayer=None, xlayer=None, vertical=False):
    s = state
    h = np.array(samples)

    slicez = zlayer or s.image.shape[0]//2
    slicex = xlayer or s.image.shape[2]//2
    slicer1 = np.s_[slicez,s.pad:-s.pad,s.pad:-s.pad]
    slicer2 = np.s_[s.pad:-s.pad,s.pad:-s.pad,slicex]
    center = (slicez, s.image.shape[1]//2, slicex)

    if vertical:
        fig = pl.figure(figsize=(12,24))
    else:
        fig = pl.figure(figsize=(24,8))

    
    
    if vertical:
        gs1 = ImageGrid(fig, rect=[0.02, 0.55, 0.99, 0.40], nrows_ncols=(2,3), axes_pad=0.1)
    else:
        gs1 = ImageGrid(fig, rect=[0.02, 0.0, 0.4, 1.00], nrows_ncols=(2,3), axes_pad=0.1)

    for i,slicer in enumerate([slicer1, slicer2]):
        ax_real = gs1[3*i+0]
        ax_fake = gs1[3*i+1]
        ax_diff = gs1[3*i+2]

        diff = s.get_model_image() - s.image
        ax_real.imshow(s.image[slicer], cmap=pl.cm.bone_r)
        ax_real.set_xticks([])
        ax_real.set_yticks([])
        ax_fake.imshow(s.get_model_image()[slicer], cmap=pl.cm.bone_r)
        ax_fake.set_xticks([])
        ax_fake.set_yticks([])
        ax_diff.imshow(diff[slicer], cmap=pl.cm.RdBu, vmin=-1.0, vmax=1.0)
        ax_diff.set_xticks([])
        ax_diff.set_yticks([])

        if i == 0:
            ax_real.set_title(""Confocal image"", fontsize=24)
            ax_fake.set_title(""Model image"", fontsize=24)
            ax_diff.set_title(""Difference"", fontsize=24)
            ax_real.set_ylabel('x-y')
        else:
            ax_real.set_ylabel('x-z')

    
    
    mu = h.mean(axis=0)
    std = h.std(axis=0)

    if vertical:
        gs2 = GridSpec(2,2, left=0.10, bottom=0.10, right=0.99, top=0.52,
                wspace=0.45, hspace=0.45)
    else:
        gs2 = GridSpec(2,2, left=0.50, bottom=0.12, right=0.95, top=0.95,
                wspace=0.35, hspace=0.35)

    ax_hist = pl.subplot(gs2[0,0])
    ax_hist.hist(std[s.b_pos], bins=np.logspace(-2.5, 0, 50), alpha=0.7, label='POS', histtype='stepfilled')
    ax_hist.hist(std[s.b_rad], bins=np.logspace(-2.5, 0, 50), alpha=0.7, label='RAD', histtype='stepfilled')
    ax_hist.set_xlim((10**-2.4, 1))
    ax_hist.semilogx()
    ax_hist.set_xlabel(r""$\bar{\sigma}$"")
    ax_hist.set_ylabel(r""$P(\bar{\sigma})$"")
    ax_hist.legend(loc='upper right')

    ax_diff = pl.subplot(gs2[0,1])
    ax_diff.hist((s.get_model_image() - s.image)[s.image_mask==1.].ravel(), bins=1000, histtype='stepfilled', alpha=0.7)
    ax_diff.semilogy()
    ax_diff.set_ylabel(r""$P(\delta)$"")
    ax_diff.set_xlabel(r""$\delta = M_i - d_i$"")
    ax_diff.locator_params(axis='x', nbins=5)

    pos = mu[s.b_pos].reshape(-1,3)
    rad = mu[s.b_rad]
    mask = analyze.trim_box(s, pos)
    pos = pos[mask]
    rad = rad[mask]

    gx, gy = analyze.gofr(pos, rad, mu[s.b_zscale][0], resolution=5e-2,mask_start=0.5)
    mask = gx < 5
    gx = gx[mask]
    gy = gy[mask]
    ax_gofr = pl.subplot(gs2[1,0])
    ax_gofr.plot(gx, gy, '-', lw=1)
    ax_gofr.set_xlabel(r""$r/d$"")
    ax_gofr.set_ylabel(r""$g(r/d)$"")
    ax_gofr.locator_params(axis='both', nbins=5)

    gx, gy = analyze.gofr(pos, rad, mu[s.b_zscale][0], method='surface')
    mask = gx < 5
    gx = gx[mask]
    gy = gy[mask]
    gy[gy <= 0.] = gy[gy>0].min()
    ax_gofrs = pl.subplot(gs2[1,1])
    ax_gofrs.plot(gx, gy, '-', lw=1)
    ax_gofrs.set_xlabel(r""$r/d$"")
    ax_gofrs.set_ylabel(r""$g_{\rm{surface}}(r/d)$"")
    ax_gofrs.locator_params(axis='both', nbins=5)
    ax_gofrs.grid(b=False, which='minor', axis='y')
    

    ylim = ax_gofrs.get_ylim()
    ax_gofrs.set_ylim(gy.min(), ylim[1])

    
    

def scan(im, cycles=1, sleep=0.3, vmin=0, vmax=1, cmap='bone'):
    pl.figure(1)
    pl.show()
    time.sleep(3)
    for c in range(cycles):
        for i, sl in enumerate(im):
            log.info('{}'.format(i))
            pl.clf()
            pl.imshow(sl, cmap=cmap, interpolation='nearest',
                    origin='lower', vmin=vmin, vmax=vmax)
            pl.draw()
            time.sleep(sleep)

def scan_together(im, p, delay=2, vmin=0, vmax=1, cmap='bone'):
    pl.figure(1)
    pl.show()
    time.sleep(3)
    z,y,x = p.T
    for i in range(len(im)):
        log.info('{}'.format(i))
        sl = im[i]
        pl.clf()
        pl.imshow(sl, cmap=cmap, interpolation='nearest', origin='lower',
                vmin=vmin, vmax=vmax)
        m = z.astype('int') == i
        pl.plot(x[m], y[m], 'o')
        pl.xlim(0, sl.shape[0])
        pl.ylim(0, sl.shape[1])
        pl.draw()
        time.sleep(delay)

def sample_compare(N, samples, truestate, burn=0):
    h = samples[burn:]
    strue = truestate

    mu = h.mean(axis=0)
    std = h.std(axis=0)
    pl.figure(figsize=(20,4))
    pl.errorbar(range(len(mu)), (mu-strue), yerr=5*std/np.sqrt(h.shape[0]),
            fmt='.', lw=0.15, alpha=0.5)
    pl.vlines([0,3*N-0.5, 4*N-0.5], -1, 1, linestyle='dashed', lw=4, alpha=0.5)
    pl.hlines(0, 0, len(mu), linestyle='dashed', lw=5, alpha=0.5)
    pl.xlim(0, len(mu))
    pl.ylim(-0.02, 0.02)
    pl.show()

def generative_model(s,x,y,z,r, factor=1.1):
    

    pl.close('all')

    slicez = int(round(z.mean()))
    slicex = s.image.shape[2]//2
    slicer1 = np.s_[slicez,s.pad:-s.pad,s.pad:-s.pad]
    slicer2 = np.s_[s.pad:-s.pad,s.pad:-s.pad,slicex]
    center = (slicez, s.image.shape[1]//2, slicex)

    fig = pl.figure(figsize=(factor*13,factor*10))

    
    
    gs1 = ImageGrid(fig, rect=[0.0, 0.6, 1.0, 0.35], nrows_ncols=(1,3),
            axes_pad=0.1)
    ax_real = gs1[0]
    ax_fake = gs1[1]
    ax_diff = gs1[2]

    diff = s.get_model_image() - s.image
    ax_real.imshow(s.image[slicer1], cmap=pl.cm.bone_r)
    ax_real.set_xticks([])
    ax_real.set_yticks([])
    ax_real.set_title(""Confocal image"", fontsize=24)
    ax_fake.imshow(s.get_model_image()[slicer1], cmap=pl.cm.bone_r)
    ax_fake.set_xticks([])
    ax_fake.set_yticks([])
    ax_fake.set_title(""Model image"", fontsize=24)
    ax_diff.imshow(diff[slicer1], cmap=pl.cm.RdBu, vmin=-0.1, vmax=0.1)
    ax_diff.set_xticks([])
    ax_diff.set_yticks([])
    ax_diff.set_title(""Difference"", fontsize=24)

    
    
    gs2 = ImageGrid(fig, rect=[0.1, 0.0, 0.4, 0.55], nrows_ncols=(3,2),
            axes_pad=0.1)
    ax_plt1 = fig.add_subplot(gs2[0])
    ax_plt2 = fig.add_subplot(gs2[1])
    ax_ilm1 = fig.add_subplot(gs2[2])
    ax_ilm2 = fig.add_subplot(gs2[3])
    ax_psf1 = fig.add_subplot(gs2[4])
    ax_psf2 = fig.add_subplot(gs2[5])

    c = int(z.mean()), int(y.mean())+s.pad, int(x.mean())+s.pad
    if s.image.shape[0] > 2*s.image.shape[1]//3:
        w = s.image.shape[2] - 2*s.pad
        h = 2*w//3
    else:
        h = s.image.shape[0] - 2*s.pad
        w = 3*h//2

    w,h = w//2, h//2
    xyslice = np.s_[slicez, c[1]-h:c[1]+h, c[2]-w:c[2]+w]
    yzslice = np.s_[c[0]-h:c[0]+h, c[1]-w:c[1]+w, slicex]

    
    
    

    ax_plt1.imshow(1-s.obj.get_field()[xyslice], cmap=pl.cm.bone_r, vmin=0, vmax=1)
    ax_plt1.set_xticks([])
    ax_plt1.set_yticks([])
    ax_plt1.set_ylabel(""Platonic"", fontsize=22)
    ax_plt1.set_title(""x-y"", fontsize=24)
    ax_plt2.imshow(1-s._platonic_image()[yzslice], cmap=pl.cm.bone_r, vmin=0, vmax=1)
    ax_plt2.set_xticks([])
    ax_plt2.set_yticks([])
    ax_plt2.set_title(""y-z"", fontsize=24)

    ax_ilm1.imshow(s.ilm.get_field()[xyslice], cmap=pl.cm.bone_r)
    ax_ilm1.set_xticks([])
    ax_ilm1.set_yticks([])
    ax_ilm1.set_ylabel(""ILM"", fontsize=22)
    ax_ilm2.imshow(s.ilm.get_field()[yzslice], cmap=pl.cm.bone_r)
    ax_ilm2.set_xticks([])
    ax_ilm2.set_yticks([])

    t = s.ilm.get_field().copy()
    t *= 0
    t[c] = 1
    s.psf.set_tile(util.Tile(t.shape))
    psf = (s.psf.execute(t)+5e-5)**0.1

    ax_psf1.imshow(psf[xyslice], cmap=pl.cm.bone)
    ax_psf1.set_xticks([])
    ax_psf1.set_yticks([])
    ax_psf1.set_ylabel(""PSF"", fontsize=22)
    ax_psf2.imshow(psf[yzslice], cmap=pl.cm.bone)
    ax_psf2.set_xticks([])
    ax_psf2.set_yticks([])

    
    
    ax_zoom = fig.add_axes([0.48, 0.018, 0.45, 0.52])

    
    im = s.image[slicer1]
    sh = np.array(im.shape)
    cx = x.mean()
    cy = y.mean()

    extent = [0,sh[0],0,sh[1]]
    ax_zoom.set_xticks([])
    ax_zoom.set_yticks([])
    ax_zoom.imshow(im, extent=extent, cmap=pl.cm.bone_r)
    ax_zoom.set_xlim(cx-12, cx+12)
    ax_zoom.set_ylim(cy-12, cy+12)
    ax_zoom.set_title(""Sampled positions"", fontsize=24)
    ax_zoom.hexbin(x,y, gridsize=32, mincnt=0, cmap=pl.cm.hot)

    zoom1 = zoomed_inset_axes(ax_zoom, 30, loc=3)
    zoom1.imshow(im, extent=extent, cmap=pl.cm.bone_r)
    zoom1.set_xlim(cx-1.0/6, cx+1.0/6)
    zoom1.set_ylim(cy-1.0/6, cy+1.0/6)
    zoom1.hexbin(x,y,gridsize=32, mincnt=5, cmap=pl.cm.hot)
    zoom1.set_xticks([])
    zoom1.set_yticks([])
    zoom1.hlines(cy-1.0/6 + 1.0/32, cx-1.0/6+5e-2, cx-1.0/6+5e-2+1e-1, lw=3)
    zoom1.text(cx-1.0/6 + 1.0/24, cy-1.0/6+5e-2, '0.1px')
    mark_inset(ax_zoom, zoom1, loc1=2, loc2=4, fc=""none"", ec=""0.0"")

    
    
    
    
    
    
    
    





def examine_unexplained_noise(state, bins=1000, xlim=(-10,10)):
    

    r = state.residuals
    q = np.fft.fftn(r)
    
    calc_sig = lambda x: np.sqrt(np.dot(x,x) / x.size)
    rh, xr = np.histogram(r.ravel() / calc_sig(r.ravel()), bins=bins,
            density=True)
    bigq = np.append(q.real.ravel(), q.imag.ravel())
    qh, xq = np.histogram(bigq / calc_sig(q.real.ravel()), bins=bins,
            density=True)
    xr = 0.5*(xr[1:] + xr[:-1])
    xq = 0.5*(xq[1:] + xq[:-1])

    gauss = lambda t : np.exp(-t*t*0.5) / np.sqrt(2*np.pi)

    plt.figure(figsize=[16,8])
    axes = []
    for a, (x, r, lbl) in enumerate([[xr, rh, 'Real'], [xq, qh, 'Fourier']]):
        ax = plt.subplot(1,2,a+1)
        ax.semilogy(x, r, label='Data')
        ax.plot(x, gauss(x), label='Gauss Fit', scalex=False, scaley=False)
        ax.set_xlabel('Residuals value $r/\sigma$')
        ax.set_ylabel('Probability $P(r/\sigma)$')
        ax.legend(loc='upper right')
        ax.set_title('{}-Space'.format(lbl))
        ax.set_xlim(xlim)
        axes.append(ax)
    return axes

def compare_data_model_residuals(s, tile, data_vmin='calc', data_vmax='calc',
         res_vmin=-0.1, res_vmax=0.1, edgepts='calc', do_imshow=True,
         data_cmap=plt.cm.bone, res_cmap=plt.cm.RdBu):
    

    
    
    residuals = s.residuals[tile.slicer].squeeze()
    data = s.data[tile.slicer].squeeze()
    model = s.model[tile.slicer].squeeze()
    if data.ndim != 2:
        raise ValueError('tile does not give a 2D slice')

    im = np.zeros([data.shape[0], data.shape[1], 4])
    if data_vmin == 'calc':
        data_vmin = 0.5*(data.min() + model.min())
    if data_vmax == 'calc':
        data_vmax = 0.5*(data.max() + model.max())

    
    upper_mask, center_mask, lower_mask = trisect_image(im.shape, edgepts)

    
    gm = data_cmap(center_data(model, data_vmin, data_vmax))
    dt = data_cmap(center_data(data, data_vmin, data_vmax))
    rs = res_cmap(center_data(residuals, res_vmin, res_vmax))

    for a in range(4):
        im[:,:,a][upper_mask] = rs[:,:,a][upper_mask]
        im[:,:,a][center_mask] = gm[:,:,a][center_mask]
        im[:,:,a][lower_mask] = dt[:,:,a][lower_mask]
    if do_imshow:
        return plt.imshow(im)
    else:
        return im

def trisect_image(imshape, edgepts='calc'):
    

    im_x, im_y = np.meshgrid(np.arange(imshape[0]), np.arange(imshape[1]),
            indexing='ij')
    if np.size(edgepts) == 1:
        
        f = np.sqrt(2./3.) if edgepts == 'calc' else edgepts
        
        lower_edge = (imshape[0] * (1-f),  imshape[1] * f)
        upper_edge = (imshape[0] * f,      imshape[1] * (1-f))
    else:
        upper_edge, lower_edge = edgepts

    
    lower_slope = lower_edge[1] / max(float(imshape[0] - lower_edge[0]), 1e-9)
    upper_slope = (imshape[1] - upper_edge[1]) / float(upper_edge[0])
    
    lower_intercept = -lower_slope * lower_edge[0]
    upper_intercept = upper_edge[1]
    lower_mask = im_y < (im_x * lower_slope + lower_intercept)
    upper_mask = im_y > (im_x * upper_slope + upper_intercept)

    center_mask= -(lower_mask | upper_mask)
    return upper_mask, center_mask, lower_mask




def center_data(data, vmin, vmax):
    

    ans = data - vmin
    ans /= (vmax - vmin)
    return np.clip(ans, 0, 1)

def sim_crb_diff(std0, std1, N=10000):
    

    a = std0*np.random.randn(N, len(std0))
    b = std1*np.random.randn(N, len(std1))
    return a - b

def diag_crb_particles(state):
    crbpos = []
    crbrad = []

    for i in np.arange(state.N)[state.state[state.b_typ]==1.]:
        log.info('{}'.format(i))
        bl = state.blocks_particle(i)
        for b in bl[:-1]:
            crbpos.append(np.sqrt(1.0/state.fisher_information(blocks=[b])))
        crbrad.append(np.sqrt(1.0/state.fisher_information(blocks=[bl[-1]])))

    cx, cr = np.array(crbpos).reshape(-1,3), np.squeeze(np.array(crbrad))
    cx[np.isinf(cx)] = 0
    cr[np.isinf(cr)] = 0
    return cx, cr

def crb_compare(state0, samples0, state1, samples1, crb0=None, crb1=None,
        zlayer=None, xlayer=None):
    

    s0 = state0
    s1 = state1
    h0 = np.array(samples0)
    h1 = np.array(samples1)

    slicez = zlayer or s0.image.shape[0]//2
    slicex = xlayer or s0.image.shape[2]//2
    slicer1 = np.s_[slicez,s0.pad:-s0.pad,s0.pad:-s0.pad]
    slicer2 = np.s_[s0.pad:-s0.pad,s0.pad:-s0.pad,slicex]
    center = (slicez, s0.image.shape[1]//2, slicex)

    mu0 = h0.mean(axis=0)
    mu1 = h1.mean(axis=0)

    std0 = h0.std(axis=0)
    std1 = h1.std(axis=0)

    mask0 = (s0.state[s0.b_typ]==1.) & (
        analyze.trim_box(s0, mu0[s0.b_pos].reshape(-1,3)))
    mask1 = (s1.state[s1.b_typ]==1.) & (
        analyze.trim_box(s1, mu1[s1.b_pos].reshape(-1,3)))
    active0 = np.arange(s0.N)[mask0]
    active1 = np.arange(s1.N)[mask1]

    pos0 = mu0[s0.b_pos].reshape(-1,3)[active0]
    pos1 = mu1[s1.b_pos].reshape(-1,3)[active1]
    rad0 = mu0[s0.b_rad][active0]
    rad1 = mu1[s1.b_rad][active1]

    link = analyze.nearest(pos0, pos1)
    dpos = pos0 - pos1[link]
    drad = rad0 - rad1[link]

    drift = dpos.mean(axis=0)
    log.info('drift {}'.format(drift))

    dpos -= drift

    fig = pl.figure(figsize=(24,10))

    
    
    gs0 = ImageGrid(fig, rect=[0.02, 0.4, 0.4, 0.60], nrows_ncols=(2,3), axes_pad=0.1)

    lbl(gs0[0], 'A')
    for i,slicer in enumerate([slicer1, slicer2]):
        ax_real = gs0[3*i+0]
        ax_fake = gs0[3*i+1]
        ax_diff = gs0[3*i+2]

        diff0 = s0.get_model_image() - s0.image
        diff1 = s1.get_model_image() - s1.image
        a = (s0.image - s1.image)
        b = (s0.get_model_image() - s1.get_model_image())
        c = (diff0 - diff1)

        ptp = 0.7*max([np.abs(a).max(), np.abs(b).max(), np.abs(c).max()])
        cmap = pl.cm.RdBu_r
        ax_real.imshow(a[slicer], cmap=cmap, vmin=-ptp, vmax=ptp)
        ax_real.set_xticks([])
        ax_real.set_yticks([])
        ax_fake.imshow(b[slicer], cmap=cmap, vmin=-ptp, vmax=ptp)
        ax_fake.set_xticks([])
        ax_fake.set_yticks([])
        ax_diff.imshow(c[slicer], cmap=cmap, vmin=-ptp, vmax=ptp)
        ax_diff.set_xticks([])
        ax_diff.set_yticks([])

        if i == 0:
            ax_real.set_title(r""$\Delta$ Confocal image"", fontsize=24)
            ax_fake.set_title(r""$\Delta$ Model image"", fontsize=24)
            ax_diff.set_title(r""$\Delta$ Difference"", fontsize=24)
            ax_real.set_ylabel('x-y')
        else:
            ax_real.set_ylabel('x-z')

    
    
    gs1 = GridSpec(1,3, left=0.05, bottom=0.125, right=0.42, top=0.37,
                wspace=0.15, hspace=0.05)

    spos0 = std0[s0.b_pos].reshape(-1,3)[active0]
    spos1 = std1[s1.b_pos].reshape(-1,3)[active1]
    srad0 = std0[s0.b_rad][active0]
    srad1 = std1[s1.b_rad][active1]

    def hist(ax, vals, bins, *args, **kwargs):
        y,x = np.histogram(vals, bins=bins)
        x = (x[1:] + x[:-1])/2
        y /= len(vals)
        ax.plot(x,y, *args, **kwargs)

    def pp(ind, tarr, tsim, tcrb, var='x'):
        bins = 10**np.linspace(-3, 0.0, 30)
        bin2 = 10**np.linspace(-3, 0.0, 100)
        bins = np.linspace(0.0, 0.2, 30)
        bin2 = np.linspace(0.0, 0.2, 100)
        xlim = (0.0, 0.12)
        
        ylim = (1e-2, 30)

        ticks = ticker.FuncFormatter(lambda x, pos: '{:0.0f}'.format(np.log10(x)))
        scaler = lambda x: x 

        ax_crb = pl.subplot(gs1[0,ind])
        ax_crb.hist(scaler(np.abs(tarr)), bins=bins,
                normed=True, alpha=0.7, histtype='stepfilled', lw=1)
        ax_crb.hist(scaler(np.abs(tcrb)).ravel(), bins=bin2,
                normed=True, alpha=1.0, histtype='step', ls='solid', lw=1.5, color='k')
        ax_crb.hist(scaler(np.abs(tsim).ravel()), bins=bin2,
                normed=True, alpha=1.0, histtype='step', lw=3)
        ax_crb.set_xlabel(r""$\Delta = |%s(t_1) - %s(t_0)|$"" % (var,var), fontsize=24)
        
        ax_crb.set_xlim(xlim)
        
        
        
        ax_crb.grid(b=False, which='both', axis='both')

        if ind == 0:
            lbl(ax_crb, 'B')
            ax_crb.set_ylabel(r""$P(\Delta)$"")
        else:
            ax_crb.set_yticks([])

        ax_crb.locator_params(axis='x', nbins=3)

    f,g = 1.5, 1.95
    sim = f*sim_crb_diff(spos0[:,1], spos1[:,1][link])
    crb = g*sim_crb_diff(crb0[0][:,1][active0], crb1[0][:,1][active1][link])
    pp(0, dpos[:,1], sim, crb, 'x')

    sim = f*sim_crb_diff(spos0[:,0], spos1[:,0][link])
    crb = g*sim_crb_diff(crb0[0][:,0][active0], crb1[0][:,0][active1][link])
    pp(1, dpos[:,0], sim, crb, 'z')

    sim = f*sim_crb_diff(srad0, srad1[link])
    crb = g*sim_crb_diff(crb0[1][active0], crb1[1][active1][link])
    pp(2, drad, sim, crb, 'a')

    
    

    
    
    gs2 = GridSpec(2,2, left=0.48, bottom=0.12, right=0.99, top=0.95,
                wspace=0.35, hspace=0.35)

    ax_hist = pl.subplot(gs2[0,0])
    ax_hist.hist(std0[s0.b_pos], bins=np.logspace(-3.0, 0, 50), alpha=0.7, label='POS', histtype='stepfilled')
    ax_hist.hist(std0[s0.b_rad], bins=np.logspace(-3.0, 0, 50), alpha=0.7, label='RAD', histtype='stepfilled')
    ax_hist.set_xlim((10**-3.0, 1))
    ax_hist.semilogx()
    ax_hist.set_xlabel(r""$\bar{\sigma}$"")
    ax_hist.set_ylabel(r""$P(\bar{\sigma})$"")
    ax_hist.legend(loc='upper right')
    lbl(ax_hist, 'C')

    imdiff = ((s0.get_model_image() - s0.image)/s0._sigma_field)[s0.image_mask==1.].ravel()
    mu = imdiff.mean()
    
    
    x = np.linspace(-5,5,10000)

    ax_diff = pl.subplot(gs2[0,1])
    ax_diff.plot(x, 1.0/np.sqrt(2*np.pi) * np.exp(-(x-mu)**2 / 2), '-', alpha=0.7, color='k', lw=2)
    ax_diff.hist(imdiff, bins=1000, histtype='step', alpha=0.7, normed=True)
    ax_diff.semilogy()
    ax_diff.set_ylabel(r""$P(\delta)$"")
    ax_diff.set_xlabel(r""$\delta = (M_i - d_i)/\sigma_i$"")
    ax_diff.locator_params(axis='x', nbins=5)
    ax_diff.grid(b=False, which='minor', axis='y')
    ax_diff.set_xlim(-5, 5)
    ax_diff.set_ylim(1e-4, 1e0)
    lbl(ax_diff, 'D')

    pos = mu0[s0.b_pos].reshape(-1,3)
    rad = mu0[s0.b_rad]
    mask = analyze.trim_box(s0, pos)
    pos = pos[mask]
    rad = rad[mask]

    gx, gy = analyze.gofr(pos, rad, mu0[s0.b_zscale][0], resolution=5e-2,mask_start=0.5)
    mask = gx < 5
    gx = gx[mask]
    gy = gy[mask]
    ax_gofr = pl.subplot(gs2[1,0])
    ax_gofr.plot(gx, gy, '-', lw=1)
    ax_gofr.set_xlabel(r""$r/a$"")
    ax_gofr.set_ylabel(r""$g(r/a)$"")
    ax_gofr.locator_params(axis='both', nbins=5)
    
    lbl(ax_gofr, 'E')

    gx, gy = analyze.gofr(pos, rad, mu0[s0.b_zscale][0], method='surface')
    mask = gx < 5
    gx = gx[mask]
    gy = gy[mask]
    gy[gy <= 0.] = gy[gy>0].min()
    ax_gofrs = pl.subplot(gs2[1,1])
    ax_gofrs.plot(gx, gy, '-', lw=1)
    ax_gofrs.set_xlabel(r""$r/a$"")
    ax_gofrs.set_ylabel(r""$g_{\rm{surface}}(r/a)$"")
    ax_gofrs.locator_params(axis='both', nbins=5)
    ax_gofrs.grid(b=False, which='minor', axis='y')
    
    lbl(ax_gofrs, 'F')

    ylim = ax_gofrs.get_ylim()
    ax_gofrs.set_ylim(gy.min(), ylim[1])
    

def crb_rad(state0, samples0, state1, samples1, crb0, crb1):
    s0 = state0
    s1 = state1
    h0 = np.array(samples0)
    h1 = np.array(samples1)

    mu0 = h0.mean(axis=0)
    mu1 = h1.mean(axis=0)

    std0 = h0.std(axis=0)
    std1 = h1.std(axis=0)

    mask0 = (s0.state[s0.b_typ]==1.) & (
        analyze.trim_box(s0, mu0[s0.b_pos].reshape(-1,3)))
    mask1 = (s1.state[s1.b_typ]==1.) & (
        analyze.trim_box(s1, mu1[s1.b_pos].reshape(-1,3)))
    active0 = np.arange(s0.N)[mask0]
    active1 = np.arange(s1.N)[mask1]

    pos0 = mu0[s0.b_pos].reshape(-1,3)[active0]
    pos1 = mu1[s1.b_pos].reshape(-1,3)[active1]
    rad0 = mu0[s0.b_rad][active0]
    rad1 = mu1[s1.b_rad][active1]

    link = analyze.nearest(pos0, pos1)
    dpos = pos0 - pos1[link]
    drad = rad0 - rad1[link]

    spos0 = std0[s0.b_pos].reshape(-1,3)[active0]
    spos1 = std1[s1.b_pos].reshape(-1,3)[active1]
    srad0 = std0[s0.b_rad][active0]
    srad1 = std1[s1.b_rad][active1]

    def pp(ax, tarr, tsim, tcrb, var='x'):
        bins = 10**np.linspace(-3, 0.0, 30)
        bin2 = 10**np.linspace(-3, 0.0, 100)
        bins = np.linspace(0.0, 0.1, 30)
        bin2 = np.linspace(0.0, 0.1, 100)
        xlim = (0, 0.1)
        
        ylim = (1e-2, 30)

        ticks = ticker.FuncFormatter(lambda x, pos: '{:0.0f}'.format(np.log10(x)))
        scaler = lambda x: x 

        ax_crb = ax
        ax_crb.hist(scaler(np.abs(tarr)), bins=bins,
                normed=True, alpha=0.7, histtype='stepfilled', lw=1, label='Radii differences')

        y,x = np.histogram(np.abs(tcrb).ravel(), bins=bin2, normed=True)
        x = (x[1:] + x[:-1])/2
        ax_crb.step(x, y, lw=3, color='k', ls='solid', label='CRB')

        y,x = np.histogram(np.abs(tsim).ravel(), bins=bin2, normed=True)
        x = (x[1:] + x[:-1])/2
        ax_crb.step(x, y, lw=3, ls='solid', label='Estimated Error')

        ax_crb.set_xlabel(r""$\Delta = |%s(t_1) - %s(t_0)|$"" % (var,var), fontsize=28)
        ax_crb.set_ylabel(r""$P(\Delta)$"", fontsize=28)
        ax_crb.set_xlim(xlim)
        ax_crb.grid(b=False, which='both', axis='both')
        ax_crb.legend(loc='best', ncol=1)

    fig = pl.figure()
    ax = pl.gca()

    f,g = 1.5, 1.85
    sim = f*sim_crb_diff(srad0, srad1[link])
    crb = g*sim_crb_diff(crb0[1][active0], crb1[1][active1][link])
    pp(ax, drad, sim, crb, 'a')


def twoslice(field, center=None, size=6.0, cmap='bone_r', vmin=0, vmax=1,
        orientation='vertical', figpad=1.09, off=0.01):
    

    center = center or [i//2 for i in field.shape]
    slices = []
    for i,c in enumerate(center):
        blank = [np.s_[:]]*len(center)
        blank[i] = c
        slices.append(tuple(blank))

    z,y,x = [float(i) for i in field.shape]
    w = float(x + z)
    h = float(y + z)

    def show(field, ax, slicer, transpose=False):
        tmp = field[slicer] if not transpose else field[slicer].T
        ax.imshow(
            tmp, cmap=cmap, interpolation='nearest',
            vmin=vmin, vmax=vmax
        )
        ax.set_xticks([])
        ax.set_yticks([])
        ax.grid('off')

    if orientation.startswith('v'):
        
        log.info('{} {} {} {} {} {}'.format(x, y, z, w, h, x/h))
        r = x/h
        q = y/h
        f = 1 / (1 + 3*off)
        fig = pl.figure(figsize=(size*r, size*f))
        ax1 = fig.add_axes((off, f*(1-q)+2*off, f, f*q))
        ax2 = fig.add_axes((off, off,           f, f*(1-q)))

        show(field, ax1, slices[0])
        show(field, ax2, slices[1])
    else:
        
        r = y/w
        q = x/w
        f = 1 / (1 + 3*off)
        fig = pl.figure(figsize=(size*f, size*r))
        ax1 = fig.add_axes((off,    off,   f*q, f))
        ax2 = fig.add_axes((2*off+f*q, off, f*(1-q), f))

        show(field, ax1, slices[0])
        show(field, ax2, slices[2], transpose=True)

    return fig, ax1, ax2

def twoslice_overlay(s, zlayer=None, xlayer=None, size=6.0,
        cmap='bone_r', vmin=0, vmax=1, showimage=False, solid=False, pad=None):
    pad = pad or s.pad
    trim = (np.s_[pad:-pad],)*3
    field = s.image[trim]

    slicez = zlayer or field.shape[0]//2
    slicex = xlayer or field.shape[2]//2
    slicer1 = np.s_[slicez,:,:]
    slicer2 = np.s_[:,:,slicex]

    sh = field.shape
    q = float(sh[1]) / (sh[0]+sh[1])
    r = float(sh[1] + sh[0]) / sh[1]

    fig = pl.figure(figsize=(size, size*r*1.05))
    ax1 = fig.add_axes((0, 1-q, 1, q))
    ax2 = fig.add_axes((0, 0, 1, 1-q))

    mu = s.state.copy()
    active = np.arange(s.N)[s.state[s.b_typ]==1.]

    pos = mu[s.b_pos].reshape(-1,3)[active]
    rad = mu[s.b_rad][active]

    def show(ax, slicer):
        talpha = 0.0 if not showimage else 1.0
        ax.imshow(field[slicer], cmap=cmap, interpolation='nearest', vmin=vmin, vmax=vmax, alpha=talpha)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_axis_bgcolor('black')
        ax.grid('off')

    def circles(ax, layer, axis):
        
        talpha = 1.0 if not showimage else 0.8
        cedge = 'white' if not showimage else 'black'
        cface = 'white' if solid else 'none'
        particles = np.arange(len(pos))[np.abs(pos[:,axis] - layer) < rad]

        
        
        for i in particles:
            p = pos[i].copy()
            r = 2*np.sqrt(rad[i]**2 - (p[axis] - layer)**2)
            if axis==0:
                c = Circle((p[2]-pad,p[1]-pad), radius=r/2, fc=cface, ec=cedge, alpha=talpha)
            if axis==2:
                c = Circle((p[1]-pad,p[0]-pad), radius=r/2, fc=cface, ec=cedge, alpha=talpha)
            ax.add_patch(c)

    show(ax1, slicer1)
    show(ax2, slicer2)

    circles(ax1, slicez+pad, 0)
    circles(ax2, slicex+pad, 2)

def deconstruction(s):
    s.model_to_true_image()
    twoslice(s.image, pad=s.pad)
    twoslice(s.get_model_image(), pad=s.pad)
    twoslice(s.ilm.get_field() - s.offset*s.obj.get_field(), pad=s.pad, vmin=None, vmax=None)
    twoslice(1 - s.offset*s.obj.get_field(), pad=s.pad)
    twoslice_overlay(s)

def circles(st, layer, axis, ax=None, talpha=1.0, cedge='white', cface='white'):
    

    pos = st.obj_get_positions()
    rad = st.obj_get_radii()
    shape = st.ishape.shape.tolist()
    shape.pop(axis) 
    if ax is None:
        fig = plt.figure()
        axisbg = 'white' if cface == 'black' else 'black'
        sx, sy = ((1,shape[1]/float(shape[0])) if shape[0] > shape[1] else
                (shape[0]/float(shape[1]), 1))
        ax = fig.add_axes((0,0, sx, sy), axisbg=axisbg)
    
    particles = np.arange(len(pos))[np.abs(pos[:,axis] - layer) < rad]

    
    
    scale = 1.0 
    for i in particles:
        p = pos[i].copy()
        r = 2*np.sqrt(rad[i]**2 - (p[axis] - layer)**2)
        
        if axis==0:
            ix = 1; iy = 2
        elif axis == 1:
            ix = 0; iy = 2
        elif axis==2:
            ix = 0; iy = 1
        c = Circle((p[ix]/scale, p[iy]/scale), radius=r/2/scale, fc=cface,
                ec=cedge, alpha=talpha)
        ax.add_patch(c)
    
    plt.axis('equal') 
    return ax
","


import gevent
import sys
import six
from logging import getLogger

logger = getLogger(__name__)


class SilentGreenletExceptionWrapper(object):
    def __init__(self, exc_info):
        self.exc_info = exc_info

    def get_exc_info(self):
        return self.exc_info


def wrap_uncaught_greenlet_exceptions(func):
    def _func(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except BaseException:
            logger.exception(""uncaught exception in greenlet"")
            return SilentGreenletExceptionWrapper(sys.exc_info())
    _func.__name__ = repr(func)
    return _func


def spawn(func, *args, **kwargs):
    

    return gevent.spawn(wrap_uncaught_greenlet_exceptions(func), *args, **kwargs)


def join(greenlet, timeout=None, raise_error=True):
    value = greenlet.get(block=True, timeout=timeout)
    if isinstance(value, SilentGreenletExceptionWrapper):
        if raise_error:
            six.reraise(*value.get_exc_info())
        else:
            return None
    return value


def joinall(greenlets, timeout=None, raise_error=True):
    
    gevent.joinall(greenlets, timeout=timeout)
    
    if raise_error:
        [join(greenlet) for greenlet in greenlets]
","


















from __future__ import absolute_import, division

__docformat__ = ""restructuredtext en""

import sys
import re
import logging
import hashlib
import hmac

from binascii import a2b_base64
from base64 import standard_b64encode

from .core import ClientAuthenticator, ServerAuthenticator
from .core import Failure, Response, Challenge, Success, Failure
from .core import sasl_mechanism, default_nonce_factory
from .saslprep import SASLPREP

logger = logging.getLogger(""pyxmpp2.sasl.scram"")

HASH_FACTORIES = {
        ""SHA-1"": hashlib.sha1,      
        ""SHA-224"": hashlib.sha224,  
        ""SHA-256"": hashlib.sha256,  
        ""SHA-384"": hashlib.sha384,  
        ""SHA-512"": hashlib.sha512,  
        ""MD-5"": hashlib.md5,        
        }

VALUE_CHARS_RE = re.compile(br""^[\x21-\x2B\x2D-\x7E]+$"")
_QUOTED_VALUE_RE = br""(?:[\x21-\x2B\x2D-\x7E]|=2C|=3D)+""

CLIENT_FIRST_MESSAGE_RE = re.compile(
        br""^(?P<gs2_header>(?:y|n|p=(?P<cb_name>[a-zA-z0-9.-]+)),""
                        br""(?:a=(?P<authzid>"" + _QUOTED_VALUE_RE + br""))?,)""
        br""(?P<client_first_bare>(?P<mext>m=[^\000=]+,)?""
                                br""n=(?P<username>"" + _QUOTED_VALUE_RE + br""),""
                                br""r=(?P<nonce>[\x21-\x2B\x2D-\x7E]+)""
                                br""(?:,.*)?)$""
                                )

SERVER_FIRST_MESSAGE_RE = re.compile(
                                br""^(?P<mext>m=[^\000=]+,)?""
                                br""r=(?P<nonce>[\x21-\x2B\x2D-\x7E]+),""
                                br""s=(?P<salt>[a-zA-Z0-9/+=]+),""
                                br""i=(?P<iteration_count>\d+)""
                                br""(?:,.*)?$""
                                )

CLIENT_FINAL_MESSAGE_RE = re.compile(
        br""(?P<without_proof>c=(?P<cb>[a-zA-Z0-9/+=]+),""
                                br""(?:r=(?P<nonce>[\x21-\x2B\x2D-\x7E]+))""
                                br""(?:,.*)?)""
        br"",p=(?P<proof>[a-zA-Z0-9/+=]+)$""
        )

SERVER_FINAL_MESSAGE_RE = re.compile(
        br""^(?:e=(?P<error>[^,]+)|v=(?P<verifier>[a-zA-Z0-9/+=]+)(?:,.*)?)$"")

class SCRAMOperations(object):
    

    def __init__(self, hash_function_name):
        self.hash_function_name = hash_function_name
        self.hash_factory = HASH_FACTORIES[hash_function_name]
        self.digest_size = self.hash_factory().digest_size

    @staticmethod
    def Normalize(str_):
        

        
        if isinstance(str_, bytes):
            str_ = str_.decode(""utf-8"")
        return SASLPREP.prepare(str_).encode(""utf-8"")

    def HMAC(self, key, str_):
        

        
        return hmac.new(key, str_, self.hash_factory).digest()

    def H(self, str_):
        

        
        return self.hash_factory(str_).digest()

    if sys.version_info.major >= 3:
        @staticmethod
        
        def XOR(str1, str2):
            

            return bytes(a ^ b for a, b in zip(str1, str2))
    else:
        @staticmethod
        
        def XOR(str1, str2):
            

            return """".join(chr(ord(a) ^ ord(b)) for a, b in zip(str1, str2))

    def Hi(self, str_, salt, i):
        

        
        Uj = self.HMAC(str_, salt + b""\000\000\000\001"") 
        result = Uj
        for _ in range(2, i + 1):
            Uj = self.HMAC(str_, Uj)               
            result = self.XOR(result,  Uj)         
        return result

    @staticmethod
    def escape(data):
        

        return data.replace(b'=', b'=3D').replace(b',', b'=2C')

    @staticmethod
    def unescape(data):
        

        return data.replace(b'=2C', b',').replace(b'=3D', b'=')

class SCRAMClientAuthenticator(SCRAMOperations, ClientAuthenticator):
    

    
    def __init__(self, hash_name, channel_binding):
        

        ClientAuthenticator.__init__(self)
        SCRAMOperations.__init__(self, hash_name)
        self.name = ""SCRAM-{0}"".format(hash_name)
        if channel_binding:
            self.name += ""-PLUS""
        self.channel_binding = channel_binding
        self.username = None
        self.password = None
        self.authzid = None
        self._c_nonce = None
        self._server_first_message = False
        self._client_first_message_bare = False
        self._gs2_header = None
        self._finished = False
        self._auth_message = None
        self._salted_password = None
        self._cb_data = None

    @classmethod
    def are_properties_sufficient(cls, properties):
        return ""username"" in properties and ""password"" in properties

    def start(self, properties):
        self.username = properties[""username""]
        self.password = properties[""password""]
        self.authzid = properties.get(""authzid"", u"""")
        c_nonce = properties.get(""nonce_factory"", default_nonce_factory)()
        if not VALUE_CHARS_RE.match(c_nonce):
            c_nonce = standard_b64encode(c_nonce)
        self._c_nonce = c_nonce

        if self.channel_binding:
            cb_data = properties.get(""channel-binding"")
            if not cb_data:
                raise ValueError(""No channel binding data provided"")
            if ""tls-unique"" in cb_data:
                cb_type = ""tls-unique""
            elif ""tls-server-end-point"" in cb_data:
                cb_type = ""tls-server-end-point""
            elif cb_data:
                cb_type = cb_data.keys()[0]
            self._cb_data = cb_data[cb_type]
            cb_flag = b""p="" + cb_type.encode(""utf-8"")
        else:
            plus_name = self.name + ""-PLUS""
            if plus_name in properties.get(""enabled_mechanisms"", []):
                
                
                
                cb_flag = b""y""
            else:
                cb_flag = b""n""

        if self.authzid:
            authzid = b""a="" + self.escape(self.authzid.encode(""utf-8""))
        else:
            authzid = b""""
        gs2_header = cb_flag + b"","" + authzid + b"",""
        self._gs2_header = gs2_header
        nonce = b""r="" + c_nonce
        client_first_message_bare = (b""n="" +
                self.escape(self.username.encode(""utf-8"")) + b"","" + nonce)
        self._client_first_message_bare = client_first_message_bare
        client_first_message = gs2_header + client_first_message_bare
        return Response(client_first_message)

    def challenge(self, challenge):
        

        
        if not challenge:
            logger.debug(""Empty challenge"")
            return Failure(""bad-challenge"")

        if self._server_first_message:
            return self._final_challenge(challenge)

        match = SERVER_FIRST_MESSAGE_RE.match(challenge)
        if not match:
            logger.debug(""Bad challenge syntax: {0!r}"".format(challenge))
            return Failure(""bad-challenge"")

        self._server_first_message = challenge

        mext = match.group(""mext"")
        if mext:
            logger.debug(""Unsupported extension received: {0!r}"".format(mext))
            return Failure(""bad-challenge"")

        nonce = match.group(""nonce"")
        if not nonce.startswith(self._c_nonce):
            logger.debug(""Nonce does not start with our nonce"")
            return Failure(""bad-challenge"")

        salt = match.group(""salt"")
        try:
            salt = a2b_base64(salt)
        except ValueError:
            logger.debug(""Bad base64 encoding for salt: {0!r}"".format(salt))
            return Failure(""bad-challenge"")

        iteration_count = match.group(""iteration_count"")
        try:
            iteration_count = int(iteration_count)
        except ValueError:
            logger.debug(""Bad iteration_count: {0!r}"".format(iteration_count))
            return Failure(""bad-challenge"")

        return self._make_response(nonce, salt, iteration_count)

    def _make_response(self, nonce, salt, iteration_count):
        

        self._salted_password = self.Hi(self.Normalize(self.password), salt,
                                                            iteration_count)
        self.password = None 
        if self.channel_binding:
            channel_binding = b""c="" + standard_b64encode(self._gs2_header +
                                                                self._cb_data)
        else:
            channel_binding = b""c="" + standard_b64encode(self._gs2_header)

        
        client_final_message_without_proof = (channel_binding + b"",r="" + nonce)

        client_key = self.HMAC(self._salted_password, b""Client Key"")
        stored_key = self.H(client_key)
        auth_message = ( self._client_first_message_bare + b"","" +
                                    self._server_first_message + b"","" +
                                        client_final_message_without_proof )
        self._auth_message = auth_message
        client_signature = self.HMAC(stored_key, auth_message)
        client_proof = self.XOR(client_key, client_signature)
        proof = b""p="" + standard_b64encode(client_proof)
        client_final_message = (client_final_message_without_proof + b"","" +
                                                                    proof)
        return Response(client_final_message)

    def _final_challenge(self, challenge):
        

        if self._finished:
            return Failure(""extra-challenge"")

        match = SERVER_FINAL_MESSAGE_RE.match(challenge)
        if not match:
            logger.debug(""Bad final message syntax: {0!r}"".format(challenge))
            return Failure(""bad-challenge"")

        error = match.group(""error"")
        if error:
            logger.debug(""Server returned SCRAM error: {0!r}"".format(error))
            return Failure(u""scram-"" + error.decode(""utf-8""))

        verifier = match.group(""verifier"")
        if not verifier:
            logger.debug(""No verifier value in the final message"")
            return Failure(""bad-succes"")

        server_key = self.HMAC(self._salted_password, b""Server Key"")
        server_signature = self.HMAC(server_key, self._auth_message)
        if server_signature != a2b_base64(verifier):
            logger.debug(""Server verifier does not match"")
            return Failure(""bad-succes"")

        self._finished = True
        return Response(None)

    def finish(self, data):
        

        if not self._server_first_message:
            logger.debug(""Got success too early"")
            return Failure(""bad-success"")
        if self._finished:
            return Success({""username"": self.username, ""authzid"": self.authzid})
        else:
            ret = self._final_challenge(data)
            if isinstance(ret, Failure):
                return ret
            if self._finished:
                return Success({""username"": self.username,
                                                    ""authzid"": self.authzid})
            else:
                logger.debug(""Something went wrong when processing additional""
                                                        "" data with success?"")
                return Failure(""bad-success"")

class SCRAMServerAuthenticator(SCRAMOperations, ServerAuthenticator):
    

    def __init__(self, hash_name, channel_binding, password_database):
        

        ServerAuthenticator.__init__(self, password_database)
        SCRAMOperations.__init__(self, hash_name)
        self.name = ""SCRAM-{0}"".format(hash_name)
        if channel_binding:
            self.name += ""-PLUS""
        self.channel_binding = channel_binding
        self.properties = None
        self.out_properties = None
        self._client_first_message_bare = None
        self._stored_key = None
        self._server_key = None

    def start(self, properties, initial_response):
        self.properties = properties
        self._client_first_message_bare = None
        self.out_properties = {}
        if not initial_response:
            return Challenge(b"""")
        return self.response(initial_response)

    def response(self, response):
        if self._client_first_message_bare:
            logger.debug(""Client final message: {0!r}"".format(response))
            return self._handle_final_response(response)
        else:
            logger.debug(""Client first message: {0!r}"".format(response))
            return self._handle_first_response(response)

    def _handle_first_response(self, response):
        match = CLIENT_FIRST_MESSAGE_RE.match(response)
        if not match:
            logger.debug(""Bad response syntax: {0!r}"".format(response))
            return Failure(""not-authorized"")

        mext = match.group(""mext"")
        if mext:
            logger.debug(""Unsupported extension received: {0!r}"".format(mext))
            return Failure(""not-authorized"")

        gs2_header = match.group(""gs2_header"")
        cb_name = match.group(""cb_name"")
        if self.channel_binding:
            if not cb_name:
                logger.debug(""{0!r} used with no channel-binding""
                                                            .format(self.name))
                return Failure(""not-authorized"")
            cb_name = cb_name.decode(""utf-8"")
            if cb_name not in self.properties[""channel-binding""]:
                logger.debug(""Channel binding data type {0!r} not available""
                                                            .format(cb_name))
                return Failure(""not-authorized"")
        else:
            if gs2_header.startswith(b'y'):
                plus_name = self.name + ""-PLUS""
                if plus_name in self.properties.get(""enabled_mechanisms"", []):
                    logger.warning(""Channel binding downgrade attack detected"")
                    return Failure(""not-authorized"")
            elif gs2_header.startswith(b'p'):
                
                logger.debug(""Channel binding requested for {0!r}""
                                                            .format(self.name))
                return Failure(""not-authorized"")

        authzid = match.group(""authzid"")
        if authzid:
            self.out_properties['authzid'] = self.unescape(authzid
                                                            ).decode(""utf-8"")
        else:
            self.out_properties['authzid'] = None
        username = self.unescape(match.group(""username"")).decode(""utf-8"")
        self.out_properties['username'] = username

        nonce_factory = self.properties.get(""nonce_factory"",
                                                        default_nonce_factory)

        properties = dict(self.properties)
        properties.update(self.out_properties)

        s_pformat = ""SCRAM-{0}-SaltedPassword"".format(self.hash_function_name)
        k_pformat = ""SCRAM-{0}-Keys"".format(self.hash_function_name)
        password, pformat = self.password_database.get_password(username,
                                           (s_pformat, ""plain""), properties)
        if pformat == s_pformat:
            if password is not None:
                salt, iteration_count, salted_password = password
            else:
                logger.debug(""No password for user {0!r}"".format(username))
        elif pformat != k_pformat:
            salt = self.properties.get(""SCRAM-salt"")
            if not salt:
                salt = nonce_factory()
            iteration_count = self.properties.get(""SCRAM-iteration-count"", 4096)
            if pformat == ""plain"" and password is not None:
                salted_password = self.Hi(self.Normalize(password), salt,
                                                            iteration_count)
            else:
                logger.debug(""No password for user {0!r}"".format(username))
                password = None
                
                salted_password = self.Hi(self.Normalize(""""), salt,
                                                            iteration_count)
        if pformat == k_pformat:
            salt, iteration_count, stored_key, server_key = password
        else:
            client_key = self.HMAC(salted_password, b""Client Key"")
            stored_key = self.H(client_key)
            server_key = self.HMAC(salted_password, b""Server Key"")

        if password is not None:
            self._stored_key = stored_key
            self._server_key = server_key
        else:
            self._stored_key = None
            self._server_key = None

        c_nonce = match.group(""nonce"")
        s_nonce = nonce_factory()
        if not VALUE_CHARS_RE.match(s_nonce):
            s_nonce = standard_b64encode(s_nonce)
        nonce = c_nonce + s_nonce
        server_first_message = (
                            b""r="" + nonce
                            + b"",s="" + standard_b64encode(salt)
                            + b"",i="" + str(iteration_count).encode(""utf-8"")
                            )
        self._nonce = nonce
        self._cb_name  = cb_name
        self._gs2_header = gs2_header
        self._client_first_message_bare = match.group(""client_first_bare"")
        self._server_first_message = server_first_message
        return Challenge(server_first_message)

    def _handle_final_response(self, response):
        match = CLIENT_FINAL_MESSAGE_RE.match(response)
        if not match:
            logger.debug(""Bad response syntax: {0!r}"".format(response))
            return Failure(""not-authorized"")
        if match.group(""nonce"") != self._nonce:
            logger.debug(""Bad nonce in the final client response"")
            return Failure(""not-authorized"")
        cb_input = a2b_base64(match.group(""cb""))
        if not cb_input.startswith(self._gs2_header):
            logger.debug(""GS2 header in the final response ({0!r}) doesn't""
                    "" match the one sent in the first message ({1!r})""
                                        .format(cb_input, self._gs2_header))
            return Failure(""not-authorized"")
        if self._cb_name:
            cb_data = cb_input[len(self._gs2_header):]
            if cb_data != self.properties[""channel-binding""][self._cb_name]:
                logger.debug(""Channel binding data doesn't match"")
                return Failure(""not-authorized"")

        proof = a2b_base64(match.group(""proof""))

        auth_message = (self._client_first_message_bare + b"","" +
                                    self._server_first_message + b"","" +
                                        match.group(""without_proof""))
        if self._stored_key is None:
            
            client_signature = self.HMAC(b"""", auth_message)
            client_key = self.XOR(client_signature, proof)
            self.H(client_key)
            logger.debug(""Authentication failed (bad username)"")
            return Failure(""not-authorized"")

        client_signature = self.HMAC(self._stored_key, auth_message)
        client_key = self.XOR(client_signature, proof)
        if self.H(client_key) != self._stored_key:
            logger.debug(""Authentication failed"")
            return Failure(""not-authorized"")

        server_signature = self.HMAC(self._server_key, auth_message)
        server_final_message = b""v="" + standard_b64encode(server_signature)
        return Success(self.out_properties, server_final_message)

@sasl_mechanism(""SCRAM-SHA-1"", 80)
class SCRAM_SHA_1_ClientAuthenticator(SCRAMClientAuthenticator):
    

    
    def __init__(self):
        SCRAMClientAuthenticator.__init__(self, ""SHA-1"", False)

@sasl_mechanism(""SCRAM-SHA-1-PLUS"", 90)
class SCRAM_SHA_1_PLUS_ClientAuthenticator(SCRAMClientAuthenticator):
    

    
    def __init__(self):
        SCRAMClientAuthenticator.__init__(self, ""SHA-1"", True)
    @classmethod
    def are_properties_sufficient(cls, properties):
        ret = super(SCRAM_SHA_1_PLUS_ClientAuthenticator, cls
                                ).are_properties_sufficient(properties)
        if not ret:
            return False
        return bool(properties.get(""channel-binding""))

@sasl_mechanism(""SCRAM-SHA-1"", 80)
class SCRAM_SHA_1_ServerAuthenticator(SCRAMServerAuthenticator):
    

    
    def __init__(self, password_database):
        SCRAMServerAuthenticator.__init__(self, ""SHA-1"", False,
                                                            password_database)

@sasl_mechanism(""SCRAM-SHA-1-PLUS"", 90)
class SCRAM_SHA_1_PLUS_ServerAuthenticator(SCRAMServerAuthenticator):
    

    
    def __init__(self, password_database):
        SCRAMServerAuthenticator.__init__(self, ""SHA-1"", True,
                                                            password_database)
    @classmethod
    def are_properties_sufficient(cls, properties):
        ret = super(SCRAM_SHA_1_PLUS_ServerAuthenticator, cls
                                ).are_properties_sufficient(properties)
        if not ret:
            return False
        return bool(properties.get(""channel-binding""))

","






















import time
import random

from collections import namedtuple
from heronpy.api.tuple import Tuple

HeronTuple = namedtuple('Tuple', Tuple._fields + ('creation_time', 'roots'))



class RootTupleInfo(namedtuple('RootTupleInfo', 'stream_id tuple_id insertion_time key')):
  __slots__ = ()
  def is_expired(self, current_time, timeout_sec):
    return self.insertion_time + timeout_sec - current_time <= 0

class TupleHelper(object):
  

  TICK_TUPLE_ID = ""__tick""
  TICK_SOURCE_COMPONENT = ""__system""

  
  MAX_SFIXED64_RAND_BITS = 61

  @staticmethod
  def make_tuple(stream, tuple_key, values, roots=None):
    

    component_name = stream.component_name
    stream_id = stream.id
    gen_task = roots[0].taskid if roots is not None and len(roots) > 0 else None
    return HeronTuple(id=str(tuple_key), component=component_name, stream=stream_id,
                      task=gen_task, values=values, creation_time=time.time(), roots=roots)
  @staticmethod
  def make_tick_tuple():
    

    return HeronTuple(id=TupleHelper.TICK_TUPLE_ID, component=TupleHelper.TICK_SOURCE_COMPONENT,
                      stream=TupleHelper.TICK_TUPLE_ID, task=None, values=None,
                      creation_time=time.time(), roots=None)

  @staticmethod
  def make_root_tuple_info(stream_id, tuple_id):
    

    key = random.getrandbits(TupleHelper.MAX_SFIXED64_RAND_BITS)
    return RootTupleInfo(stream_id=stream_id, tuple_id=tuple_id,
                         insertion_time=time.time(), key=key)
","




from __future__ import print_function

from . import file_info
from . import core
from .log import logger
from .core import ENCODING_VALS
from .core import is_number
from .core import sox
from .core import play
from .core import SoxError
from .core import SoxiError
from .core import VALID_FORMATS

from .transform import Transformer


COMBINE_VALS = [
    'concatenate', 'merge', 'mix', 'mix-power', 'multiply'
]


class Combiner(Transformer):
    

    def __init__(self):
        super(Combiner, self).__init__()

    def build(self, input_filepath_list, output_filepath, combine_type,
              input_volumes=None):
        

        file_info.validate_input_file_list(input_filepath_list)
        file_info.validate_output_file(output_filepath)
        _validate_combine_type(combine_type)
        _validate_volumes(input_volumes)

        input_format_list = _build_input_format_list(
            input_filepath_list, input_volumes, self.input_format
        )

        try:
            _validate_file_formats(input_filepath_list, combine_type)
        except SoxiError:
            logger.warning(""unable to validate file formats."")

        args = []
        args.extend(self.globals)
        args.extend(['--combine', combine_type])

        input_args = _build_input_args(input_filepath_list, input_format_list)
        args.extend(input_args)

        args.extend(self.output_format)
        args.append(output_filepath)
        args.extend(self.effects)

        status, out, err = sox(args)

        if status != 0:
            raise SoxError(
                ""Stdout: {}\nStderr: {}"".format(out, err)
            )
        else:
            logger.info(
                ""Created %s with combiner %s and  effects: %s"",
                output_filepath,
                combine_type,
                "" "".join(self.effects_log)
            )
            if out is not None:
                logger.info(""[SoX] {}"".format(out))
            return True

    def preview(self, input_filepath_list, combine_type, input_volumes=None):
        

        args = [""play"", ""--no-show-progress""]
        args.extend(self.globals)
        args.extend(['--combine', combine_type])

        input_format_list = _build_input_format_list(
            input_filepath_list, input_volumes, self.input_format
        )
        input_args = _build_input_args(input_filepath_list, input_format_list)
        args.extend(input_args)
        args.extend(self.effects)

        play(args)

    def set_input_format(self, file_type=None, rate=None, bits=None,
                         channels=None, encoding=None, ignore_length=None):
        

        if file_type is not None and not isinstance(file_type, list):
            raise ValueError(""file_type must be a list or None."")

        if file_type is not None:
            if not all([f in VALID_FORMATS for f in file_type]):
                raise ValueError(
                    'file_type elements '
                    'must be one of {}'.format(VALID_FORMATS)
                )
        else:
            file_type = []

        if rate is not None and not isinstance(rate, list):
            raise ValueError(""rate must be a list or None."")

        if rate is not None:
            if not all([is_number(r) and r > 0 for r in rate]):
                raise ValueError('rate elements must be positive floats.')
        else:
            rate = []

        if bits is not None and not isinstance(bits, list):
            raise ValueError(""bits must be a list or None."")

        if bits is not None:
            if not all([isinstance(b, int) and b > 0 for b in bits]):
                raise ValueError('bit elements must be positive ints.')
        else:
            bits = []

        if channels is not None and not isinstance(channels, list):
            raise ValueError(""channels must be a list or None."")

        if channels is not None:
            if not all([isinstance(c, int) and c > 0 for c in channels]):
                raise ValueError('channel elements must be positive ints.')
        else:
            channels = []

        if encoding is not None and not isinstance(encoding, list):
            raise ValueError(""encoding must be a list or None."")

        if encoding is not None:
            if not all([e in ENCODING_VALS for e in encoding]):
                raise ValueError(
                    'elements of encoding must '
                    'be one of {}'.format(ENCODING_VALS)
                )
        else:
            encoding = []

        if ignore_length is not None and not isinstance(ignore_length, list):
            raise ValueError(""ignore_length must be a list or None."")

        if ignore_length is not None:
            if not all([isinstance(l, bool) for l in ignore_length]):
                raise ValueError(""ignore_length elements must be booleans."")
        else:
            ignore_length = []

        max_input_arg_len = max([
            len(file_type), len(rate), len(bits), len(channels),
            len(encoding), len(ignore_length)
        ])

        input_format = []
        for _ in range(max_input_arg_len):
            input_format.append([])

        for i, f in enumerate(file_type):
            input_format[i].extend(['-t', '{}'.format(f)])

        for i, r in enumerate(rate):
            input_format[i].extend(['-r', '{}'.format(r)])

        for i, b in enumerate(bits):
            input_format[i].extend(['-b', '{}'.format(b)])

        for i, c in enumerate(channels):
            input_format[i].extend(['-c', '{}'.format(c)])

        for i, e in enumerate(encoding):
            input_format[i].extend(['-e', '{}'.format(e)])

        for i, l in enumerate(ignore_length):
            if l is True:
                input_format[i].append('--ignore-length')

        self.input_format = input_format
        return self


def _validate_file_formats(input_filepath_list, combine_type):
    

    _validate_sample_rates(input_filepath_list, combine_type)

    if combine_type == 'concatenate':
        _validate_num_channels(input_filepath_list, combine_type)


def _validate_sample_rates(input_filepath_list, combine_type):
    

    sample_rates = [
        file_info.sample_rate(f) for f in input_filepath_list
    ]
    if not core.all_equal(sample_rates):
        raise IOError(
            ""Input files do not have the same sample rate. The {} combine ""
            ""type requires that all files have the same sample rate""
            .format(combine_type)
        )


def _validate_num_channels(input_filepath_list, combine_type):
    

    channels = [
        file_info.channels(f) for f in input_filepath_list
    ]
    if not core.all_equal(channels):
        raise IOError(
            ""Input files do not have the same number of channels. The ""
            ""{} combine type requires that all files have the same ""
            ""number of channels""
            .format(combine_type)
        )


def _build_input_format_list(input_filepath_list, input_volumes=None,
                             input_format=None):
    

    n_inputs = len(input_filepath_list)
    input_format_list = []
    for _ in range(n_inputs):
        input_format_list.append([])

    
    if input_volumes is None:
        vols = [1] * n_inputs
    else:
        n_volumes = len(input_volumes)
        if n_volumes < n_inputs:
            logger.warning(
                'Volumes were only specified for %s out of %s files.'
                'The last %s files will remain at their original volumes.',
                n_volumes, n_inputs, n_inputs - n_volumes
            )
            vols = input_volumes + [1] * (n_inputs - n_volumes)
        elif n_volumes > n_inputs:
            logger.warning(
                '%s volumes were specified but only %s input files exist.'
                'The last %s volumes will be ignored.',
                n_volumes, n_inputs, n_volumes - n_inputs
            )
            vols = input_volumes[:n_inputs]
        else:
            vols = [v for v in input_volumes]

    
    if input_format is None:
        fmts = [[] for _ in range(n_inputs)]
    else:
        n_fmts = len(input_format)
        if n_fmts < n_inputs:
            logger.warning(
                'Input formats were only specified for %s out of %s files.'
                'The last %s files will remain unformatted.',
                n_fmts, n_inputs, n_inputs - n_fmts
            )
            fmts = [f for f in input_format]
            fmts.extend([[] for _ in range(n_inputs - n_fmts)])
        elif n_fmts > n_inputs:
            logger.warning(
                '%s Input formats were specified but only %s input files exist'
                '. The last %s formats will be ignored.',
                n_fmts, n_inputs, n_fmts - n_inputs
            )
            fmts = input_format[:n_inputs]
        else:
            fmts = [f for f in input_format]

    for i, (vol, fmt) in enumerate(zip(vols, fmts)):
        input_format_list[i].extend(['-v', '{}'.format(vol)])
        input_format_list[i].extend(fmt)

    return input_format_list


def _build_input_args(input_filepath_list, input_format_list):
    

    if len(input_format_list) != len(input_filepath_list):
        raise ValueError(
            ""input_format_list & input_filepath_list are not the same size""
        )

    input_args = []
    zipped = zip(input_filepath_list, input_format_list)
    for input_file, input_fmt in zipped:
        input_args.extend(input_fmt)
        input_args.append(input_file)

    return input_args


def _validate_combine_type(combine_type):
    

    if combine_type not in COMBINE_VALS:
        raise ValueError(
            'Invalid value for combine_type. Must be one of {}'.format(
                COMBINE_VALS)
        )


def _validate_volumes(input_volumes):
    

    if not (input_volumes is None or isinstance(input_volumes, list)):
        raise TypeError(""input_volumes must be None or a list."")

    if isinstance(input_volumes, list):
        for vol in input_volumes:
            if not core.is_number(vol):
                raise ValueError(
                    ""Elements of input_volumes must be numbers: found {}""
                    .format(vol)
                )
","







import numpy as np






BJHK = [0.1922, 5.2634, 0.0203, -4.2810]
BJH = [0.7013, 6.2321, -5.2769]
BJK = [0.1935, 5.2676, -4.2650]
BHK = [0.7108, 18.5256, -17.5197]
BJ = [5.5599, 0.6320]
BH = [6.7509, 0.5269]
BK = [7.0739, 0.4971]

VJHK = [-0.0053,3.5326, 1.3141, -3.8331]
VJH = [0.2948, 4.2168, -3.2251]
VJK = [0.0631, 3.7103, -2.7004]
VHK = [1.4044, 12.1719,-11.2331]
VJ = [3.8512, 0.7671]
VH = [4.8834, 0.6895]
VK = [5.1466, 0.6682]

RJHK = [0.0606, 2.7823, 0.8922, -2.6713]
RJH = [0.3678, 3.4181, -2.4434]
RJK = [0.1063, 2.9828, -1.9846]
RHK = [0.4826, 10.3926, -9.4021]
RJ = [2.8217, 0.8101]
RH = [3.9934, 0.7154]
RK = [4.3327, 0.6862]

IJHK = [0.0560, 2.0812, 0.4074, -1.4889]
IJH = [0.2453, 2.4061, -1.4236]
IJK = [0.0994, 2.1576, -1.1622]
IHK = [0.1403, 8.0510, -7.0394]
IJ = [1.5585, 0.8979]
IH = [2.6453, 0.8138]
IK = [2.9959, 0.7839]


SDSSU_JHK = [3.5675, 5.4894, -2.8007, -1.8600]
SDSSU_JH = [4.2568, 5.3802, -4.5899]
SDSSU_JK = [3.8707, 4.6353, -3.8165]
SDSSU_HK = [10.4029, 0.6696, -0.1827]
SDSSU_J = [7.4786, 0.6779]
SDSSU_H = [10.2512, 0.4984]
SDSSU_K = [10.7584, 0.4662]

SDSSG_JHK = [0.9922, 4.3197, -1.6916, -1.6751]
SDSSG_JH = [0.6890, 4.4356, -3.4537]
SDSSG_JK = [1.5487, 4.1286, -3.2193]
SDSSG_HK = [4.3634, 2.7802, -1.9456]
SDSSG_J = [2.4949, 0.9537]
SDSSG_H = [4.7010, 0.8227]
SDSSG_K = [5.2323, 0.7899]

SDSSR_JHK = [0.6975, 2.9782, -0.8809, -1.1230]
SDSSR_JH = [1.0935, 2.9289, -1.9766]
SDSSR_JK = [0.7277, 2.7746, -1.8005]
SDSSR_HK = [5.7039, 1.4010, -0.7037]
SDSSR_J = [3.0033, 0.8713]
SDSSR_H = [5.6142, 0.7069]
SDSSR_K = [5.8755, 0.6913]

SDSSI_JHK = [0.8875, 2.3210, -0.6825, -0.6724]
SDSSI_JH = [0.9052, 2.3750, -1.4074]
SDSSI_JK = [0.8117, 2.1503, -1.1763]
SDSSI_HK = [6.2356, 2.7331, -2.1008]
SDSSI_J = [2.1593, 0.9168]
SDSSI_H = [6.4280, 0.6295]
SDSSI_K = [5.8109, 0.6773]

SDSSZ_JHK = [0.8346, 1.7668, -0.1778, -0.6084]
SDSSZ_JH = [0.9037, 1.8245, -0.8472]
SDSSZ_JK = [0.9220, 1.7158, -0.7411]
SDSSZ_HK = [4.3827, 2.4788, -1.7118]
SDSSZ_J = [1.5408, 0.9557]
SDSSZ_H = [6.1351, 0.6509]
SDSSZ_K = [6.6213, 0.6183]






def convert_constants(jmag, hmag, kmag,
                      cjhk,
                      cjh, cjk, chk,
                      cj, ch, ck):
    


    if jmag is not None:

        if hmag is not None:

            if kmag is not None:

                return cjhk[0] + cjhk[1]*jmag + cjhk[2]*hmag + cjhk[3]*kmag

            else:

                return cjh[0] + cjh[1]*jmag + cjh[2]*hmag

        else:

            if kmag is not None:

                return cjk[0] + cjk[1]*jmag + cjk[2]*kmag

            else:

                return cj[0] + cj[1]*jmag

    else:

        if hmag is not None:

            if kmag is not None:

                return chk[0] + chk[1]*hmag + chk[2]*kmag

            else:

                return ch[0] + ch[1]*hmag

        else:

            if kmag is not None:

                return ck[0] + ck[1]*kmag

            else:

                return np.nan






def jhk_to_bmag(jmag, hmag, kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             BJHK,
                             BJH, BJK, BHK,
                             BJ, BH, BK)



def jhk_to_vmag(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             VJHK,
                             VJH, VJK, VHK,
                             VJ, VH, VK)



def jhk_to_rmag(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             RJHK,
                             RJH, RJK, RHK,
                             RJ, RH, RK)



def jhk_to_imag(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             IJHK,
                             IJH, IJK, IHK,
                             IJ, IH, IK)






def jhk_to_sdssu(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             SDSSU_JHK,
                             SDSSU_JH, SDSSU_JK, SDSSU_HK,
                             SDSSU_J, SDSSU_H, SDSSU_K)



def jhk_to_sdssg(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             SDSSG_JHK,
                             SDSSG_JH, SDSSG_JK, SDSSG_HK,
                             SDSSG_J, SDSSG_H, SDSSG_K)



def jhk_to_sdssr(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             SDSSR_JHK,
                             SDSSR_JH, SDSSR_JK, SDSSR_HK,
                             SDSSR_J, SDSSR_H, SDSSR_K)



def jhk_to_sdssi(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             SDSSI_JHK,
                             SDSSI_JH, SDSSI_JK, SDSSI_HK,
                             SDSSI_J, SDSSI_H, SDSSI_K)



def jhk_to_sdssz(jmag,hmag,kmag):
    


    return convert_constants(jmag,hmag,kmag,
                             SDSSZ_JHK,
                             SDSSZ_JH, SDSSZ_JK, SDSSZ_HK,
                             SDSSZ_J, SDSSZ_H, SDSSZ_K)
















































def absolute_gaia_magnitude(gaia_mag,
                            gaia_parallax_mas,
                            gaia_mag_err=None,
                            gaia_parallax_err_mas=None):
    


    
    
    
    
    d_pc = np.abs(1000.0/gaia_parallax_mas)

    
    if gaia_parallax_err_mas is not None:
        d_pc_err = (
            (1000.0/(gaia_parallax_mas*gaia_parallax_mas)) *
            gaia_parallax_err_mas
        )
    else:
        d_pc_err = None

    
    
    
    
    M_G = 5 - 5.0*np.log10(d_pc) + gaia_mag

    
    if d_pc_err is not None and gaia_mag_err is not None:

        M_G_err = np.sqrt(
            ((5.0/(d_pc * np.log(10.0)))**2 * (d_pc_err)**2) +
            gaia_mag_err*gaia_mag_err
        )
    else:
        M_G_err = None


    if M_G_err is not None:
        return M_G, M_G_err
    else:
        return M_G
","














from neutron_lib import exceptions as n_exc
from oslo_log import log as logging

from quark.db import api as db_api
from quark import exceptions as q_exc
from quark import plugin_views as v

from quark import segment_allocations

SA_REGISTRY = segment_allocations.REGISTRY

LOG = logging.getLogger(__name__)


def get_segment_allocation_range(context, id, fields=None):
    LOG.info(""get_segment_allocation_range %s for tenant %s fields %s"" %
             (id, context.tenant_id, fields))

    if not context.is_admin:
        raise n_exc.NotAuthorized()

    sa_range = db_api.segment_allocation_range_find(
        context, id=id, scope=db_api.ONE)

    if not sa_range:
        raise q_exc.SegmentAllocationRangeNotFound(
            segment_allocation_range_id=id)

    
    allocs = db_api.segment_allocation_find(
        context,
        segment_allocation_range_id=sa_range[""id""],
        deallocated=False).count()
    return v._make_segment_allocation_range_dict(
        sa_range, allocations=allocs)


def get_segment_allocation_ranges(context, **filters):
    LOG.info(""get_segment_allocation_ranges for tenant %s"" % context.tenant_id)
    if not context.is_admin:
        raise n_exc.NotAuthorized()

    sa_ranges = db_api.segment_allocation_range_find(
        context, scope=db_api.ALL, **filters)
    return [v._make_segment_allocation_range_dict(m) for m in sa_ranges]


def create_segment_allocation_range(context, sa_range):
    LOG.info(""create_segment_allocation_range for tenant %s""
             % context.tenant_id)
    if not context.is_admin:
        raise n_exc.NotAuthorized()

    sa_range = sa_range.get(""segment_allocation_range"")
    if not sa_range:
        raise n_exc.BadRequest(resource=""segment_allocation_range"",
                               msg=(""segment_allocation_range not in ""
                                    ""request body.""))

    
    
    
    for k in [""first_id"", ""last_id"", ""segment_id"", ""segment_type""]:
        sa_range[k] = sa_range.get(k, None)
        if sa_range[k] is None:
            raise n_exc.BadRequest(
                resource=""segment_allocation_range"",
                msg=(""Missing required key %s in request body."" % (k)))

    
    for k in [""do_not_use""]:
        sa_range[k] = sa_range.get(k, None)

    
    if not SA_REGISTRY.is_valid_strategy(sa_range[""segment_type""]):
        raise n_exc.BadRequest(
            resource=""segment_allocation_range"",
            msg=(""Unknown segment type '%s'"" % (k)))
    strategy = SA_REGISTRY.get_strategy(sa_range[""segment_type""])

    
    with context.session.begin():
        new_range = strategy.create_range(context, sa_range)

    
    
    
    
    try:
        strategy.populate_range(context, new_range)
    except Exception:
        LOG.exception(""Failed to populate segment allocation range."")
        delete_segment_allocation_range(context, new_range[""id""])
        raise

    return v._make_segment_allocation_range_dict(new_range)


def _delete_segment_allocation_range(context, sa_range):

    allocs = db_api.segment_allocation_find(
        context,
        segment_allocation_range_id=sa_range[""id""],
        deallocated=False).count()

    if allocs:
        raise q_exc.SegmentAllocationRangeInUse(
            segment_allocation_range_id=sa_range[""id""])
    db_api.segment_allocation_range_delete(context, sa_range)


def delete_segment_allocation_range(context, sa_id):
    

    LOG.info(""delete_segment_allocation_range %s for tenant %s"" %
             (sa_id, context.tenant_id))
    if not context.is_admin:
        raise n_exc.NotAuthorized()

    with context.session.begin():
        sa_range = db_api.segment_allocation_range_find(
            context, id=sa_id, scope=db_api.ONE)
        if not sa_range:
            raise q_exc.SegmentAllocationRangeNotFound(
                segment_allocation_range_id=sa_id)
        _delete_segment_allocation_range(context, sa_range)
","




import itertools as itt
from collections import Counter, defaultdict
from typing import Iterable, List, Mapping, Optional, Set, Tuple, TypeVar

from pybel import BELGraph
from pybel.constants import ANNOTATIONS, RELATION
from pybel.dsl import BaseEntity
from pybel.struct.filters.edge_predicates import edge_has_annotation
from pybel.struct.filters.typing import NodePredicate
from pybel.struct.summary import (
    count_annotations, count_pathologies, count_relations, get_annotations, get_unused_annotations,
    get_unused_list_annotation_values, iter_annotation_value_pairs, iter_annotation_values,
)
from .contradictions import pair_has_contradiction

__all__ = [
    'count_relations',
    'get_edge_relations',
    'count_unique_relations',
    'count_annotations',
    'get_annotations',
    'get_annotations_containing_keyword',
    'count_annotation_values',
    'count_annotation_values_filtered',
    'pair_is_consistent',
    'get_consistent_edges',
    'get_contradictory_pairs',
    'count_pathologies',
    'get_unused_annotations',
    'get_unused_list_annotation_values',
]

A = TypeVar('A')
B = TypeVar('B')


def group_dict_set(iterator: Iterable[Tuple[A, B]]) -> Mapping[A, Set[B]]:
    

    d = defaultdict(set)
    for key, value in iterator:
        d[key].add(value)
    return dict(d)


def get_edge_relations(graph: BELGraph) -> Mapping[Tuple[BaseEntity, BaseEntity], Set[str]]:
    

    return group_dict_set(
        ((u, v), d[RELATION])
        for u, v, d in graph.edges(data=True)
    )


def count_unique_relations(graph: BELGraph) -> Counter:
    

    return Counter(itt.chain.from_iterable(get_edge_relations(graph).values()))


def get_annotations_containing_keyword(graph: BELGraph, keyword: str) -> List[Mapping[str, str]]:
    

    return [
        {
            'annotation': annotation,
            'value': value
        }
        for annotation, value in iter_annotation_value_pairs(graph)
        if keyword.lower() in value.lower()
    ]


def count_annotation_values(graph: BELGraph, annotation: str) -> Counter:
    

    return Counter(iter_annotation_values(graph, annotation))


def count_annotation_values_filtered(graph: BELGraph,
                                     annotation: str,
                                     source_predicate: Optional[NodePredicate] = None,
                                     target_predicate: Optional[NodePredicate] = None,
                                     ) -> Counter:
    

    if source_predicate and target_predicate:
        return Counter(
            data[ANNOTATIONS][annotation]
            for u, v, data in graph.edges(data=True)
            if edge_has_annotation(data, annotation) and source_predicate(graph, u) and target_predicate(graph, v)
        )
    elif source_predicate:
        return Counter(
            data[ANNOTATIONS][annotation]
            for u, v, data in graph.edges(data=True)
            if edge_has_annotation(data, annotation) and source_predicate(graph, u)
        )
    elif target_predicate:
        return Counter(
            data[ANNOTATIONS][annotation]
            for u, v, data in graph.edges(data=True)
            if edge_has_annotation(data, annotation) and target_predicate(graph, u)
        )
    else:
        return Counter(
            data[ANNOTATIONS][annotation]
            for u, v, data in graph.edges(data=True)
            if edge_has_annotation(data, annotation)
        )


def pair_is_consistent(graph: BELGraph, u: BaseEntity, v: BaseEntity) -> Optional[str]:
    

    relations = {data[RELATION] for data in graph[u][v].values()}

    if 1 != len(relations):
        return

    return list(relations)[0]


def get_contradictory_pairs(graph: BELGraph) -> Iterable[Tuple[BaseEntity, BaseEntity]]:
    

    for u, v in graph.edges():
        if pair_has_contradiction(graph, u, v):
            yield u, v


def get_consistent_edges(graph: BELGraph) -> Iterable[Tuple[BaseEntity, BaseEntity]]:
    

    for u, v in graph.edges():
        if pair_is_consistent(graph, u, v):
            yield u, v
","























import os

from sigal import signals
from sigal.writer import AbstractWriter
from sigal.utils import url_from_path


class PageWriter(AbstractWriter):
    


    template_file = ""media.html""

    def write(self, album, media_group):
        


        from sigal import __url__ as sigal_link
        file_path = os.path.join(album.dst_path, media_group[0].filename)

        page = self.template.render({
            'album': album,
            'media': media_group[0],
            'previous_media': media_group[-1],
            'next_media': media_group[1],
            'index_title': self.index_title,
            'settings': self.settings,
            'sigal_link': sigal_link,
            'theme': {'name': os.path.basename(self.theme),
                      'url': url_from_path(os.path.relpath(self.theme_path,
                                                           album.dst_path))},
        })

        output_file = ""%s.html"" % file_path

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(page)


def generate_media_pages(gallery):
    


    writer = PageWriter(gallery.settings, index_title=gallery.title)

    for album in gallery.albums.values():
        medias = album.medias
        next_medias = medias[1:] + [None]
        previous_medias = [None] + medias[:-1]

        
        media_groups = zip(medias, next_medias, previous_medias)

        for media_group in media_groups:
            writer.write(album, media_group)


def register(settings):
    signals.gallery_build.connect(generate_media_pages)
","















from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import random
from six.moves import xrange
import numpy as np
from collections import namedtuple

from tensorforce import util, TensorForceError
from tensorforce.core.memories import Memory

_SumRow = namedtuple('SumRow', ['item', 'priority'])




class SumTree(object):
    


    def __init__(self, capacity):
        self._capacity = capacity

        
        self._memory = [0] * (capacity - 1)
        self._position = 0
        self._actual_capacity = 2 * self._capacity - 1

    def put(self, item, priority=None):
        

        if not self._isfull():
            self._memory.append(None)
        position = self._next_position_then_increment()
        old_priority = 0 if self._memory[position] is None \
            else (self._memory[position].priority or 0)
        row = _SumRow(item, priority)
        self._memory[position] = row
        self._update_internal_nodes(
            position, (row.priority or 0) - old_priority)

    def move(self, external_index, new_priority):
        

        index = external_index + (self._capacity - 1)
        return self._move(index, new_priority)

    def _move(self, index, new_priority):
        

        item, old_priority = self._memory[index]
        old_priority = old_priority or 0
        self._memory[index] = _SumRow(item, new_priority)
        self._update_internal_nodes(index, new_priority - old_priority)

    def _update_internal_nodes(self, index, delta):
        

        
        while index > 0:
            index = (index - 1) // 2
            self._memory[index] += delta

    def _isfull(self):
        return len(self) == self._capacity

    def _next_position_then_increment(self):
        

        start = self._capacity - 1
        position = start + self._position
        self._position = (self._position + 1) % self._capacity
        return position

    def _sample_with_priority(self, p):
        

        parent = 0
        while True:
            left = 2 * parent + 1
            if left >= len(self._memory):
                
                return parent

            left_p = self._memory[left] if left < self._capacity - 1 \
                else (self._memory[left].priority or 0)
            if p <= left_p:
                parent = left
            else:
                if left + 1 >= len(self._memory):
                    raise RuntimeError('Right child is expected to exist.')
                p -= left_p
                parent = left + 1

    def sample_minibatch(self, batch_size):
        

        pool_size = len(self)
        if pool_size == 0:
            return []

        delta_p = self._memory[0] / batch_size
        chosen_idx = []
        
        if abs(self._memory[0]) < util.epsilon:
            chosen_idx = np.random.randint(self._capacity - 1, self._capacity - 1 + len(self), size=batch_size).tolist()
        else:
            for i in xrange(batch_size):
                lower = max(i * delta_p, 0)
                upper = min((i + 1) * delta_p, self._memory[0])
                p = random.uniform(lower, upper)
                chosen_idx.append(self._sample_with_priority(p))
        return [(i, self._memory[i]) for i in chosen_idx]

    def __len__(self):
        

        return len(self._memory) - (self._capacity - 1)

    def __getitem__(self, index):
        return self._memory[self._capacity - 1:][index]

    def __getslice__(self, start, end):
        self.memory[self._capacity - 1:][start:end]



class PrioritizedReplay(Memory):
    


    def __init__(self, states_spec, actions_spec, capacity, prioritization_weight=1.0, prioritization_constant=0.0):
        super(PrioritizedReplay, self).__init__(states_spec=states_spec, actions_spec=actions_spec)
        self.capacity = capacity
        self.prioritization_weight = prioritization_weight
        self.prioritization_constant = prioritization_constant
        self.internals_spec = None
        self.batch_indices = None

        
        self.observations = SumTree(capacity)

        
        self.none_priority_index = 0

        
        self.last_observation = None

    def add_observation(self, states, internals, actions, terminal, reward):
        if self.internals_spec is None and internals is not None:
            self.internals_spec = [(internal.shape, internal.dtype) for internal in internals]

        if self.last_observation is not None:
            observation = self.last_observation + (states, internals)

            
            if self.observations._isfull():
                if self.none_priority_index <= 0:
                    raise TensorForceError(
                        ""Trying to replace unseen observations: ""
                        ""Memory is at capacity and contains only unseen observations.""
                    )
                self.none_priority_index -= 1

            self.observations.put(observation, None)

        self.last_observation = (states, internals, actions, terminal, reward)

    def get_batch(self, batch_size, next_states=False):
        

        if batch_size > len(self.observations):
            raise TensorForceError(
                ""Requested batch size is larger than observations in memory: increase config.first_update."")

        
        states = {name: np.zeros((batch_size,) + tuple(state['shape']), dtype=util.np_dtype(
            state['type'])) for name, state in self.states_spec.items()}
        internals = [np.zeros((batch_size,) + shape, dtype)
                     for shape, dtype in self.internals_spec]
        actions = {name: np.zeros((batch_size,) + tuple(action['shape']), dtype=util.np_dtype(action['type'])) for name, action in self.actions_spec.items()}
        terminal = np.zeros((batch_size,), dtype=util.np_dtype('bool'))
        reward = np.zeros((batch_size,), dtype=util.np_dtype('float'))
        if next_states:
            next_states = {name: np.zeros((batch_size,) + tuple(state['shape']), dtype=util.np_dtype(
                state['type'])) for name, state in self.states_spec.items()}
            next_internals = [np.zeros((batch_size,) + shape, dtype)
                              for shape, dtype in self.internals_spec]

        
        unseen_indices = list(xrange(
            self.none_priority_index + self.observations._capacity - 1,
            len(self.observations) + self.observations._capacity - 1)
        )
        self.batch_indices = unseen_indices[:batch_size]

        
        remaining = batch_size - len(self.batch_indices)
        if remaining:
            samples = self.observations.sample_minibatch(remaining)
            sample_indices = [i for i, o in samples]
            self.batch_indices += sample_indices

        
        np.random.shuffle(self.batch_indices)

        
        for n, index in enumerate(self.batch_indices):
            observation, _ = self.observations._memory[index]

            for name, state in states.items():
                state[n] = observation[0][name]
            for k, internal in enumerate(internals):
                internal[n] = observation[1][k]
            for name, action in actions.items():
                action[n] = observation[2][name]
            terminal[n] = observation[3]
            reward[n] = observation[4]
            if next_states:
                for name, next_state in next_states.items():
                    next_state[n] = observation[5][name]
                for k, next_internal in enumerate(next_internals):
                    next_internal[n] = observation[6][k]

        if next_states:
            return dict(
                states=states,
                internals=internals,
                actions=actions,
                terminal=terminal,
                reward=reward,
                next_states=next_states,
                next_internals=next_internals
            )
        else:
            return dict(
                states=states,
                internals=internals,
                actions=actions,
                terminal=terminal,
                reward=reward
            )

    def update_batch(self, loss_per_instance):
        

        if self.batch_indices is None:
            raise TensorForceError(""Need to call get_batch before each update_batch call."")
        
        

        for index, loss in zip(self.batch_indices, loss_per_instance):
            
            new_priority = (np.abs(loss) + self.prioritization_constant) ** self.prioritization_weight
            self.observations._move(index, new_priority)
            self.none_priority_index += 1
","
































import random


__all__ = ['account_number', 'bik', 'inn', 'legal_inn', 'legal_ogrn',
           'ogrn', 'person_inn', 'person_ogrn']

TYPES = ['person', 'legal']


def account_number():
    

    account = [random.randint(1, 9) for _ in range(20)]
    return """".join(map(str, account))


def bik():
    

    return '04' + \
           ''.join([str(random.randint(1, 9)) for _ in range(5)]) + \
           str(random.randint(0, 49) + 50)


def inn(type=""legal""):
    

    if (type in TYPES) and type == 'person':
        return person_inn()
    else:
        return legal_inn()


def legal_inn():
    

    mask = [2, 4, 10, 3, 5, 9, 4, 6, 8]
    inn = [random.randint(1, 9) for _ in range(10)]
    weighted = [v * mask[i] for i, v in enumerate(inn[:-1])]
    inn[9] = sum(weighted) % 11 % 10
    return """".join(map(str, inn))


def legal_ogrn():
    

    ogrn = """".join(map(str, [random.randint(1, 9) for _ in range(12)]))
    ogrn += str((int(ogrn) % 11 % 10))
    return ogrn


def ogrn(type=""legal""):
    

    if (type in TYPES) and type == 'person':
        return person_ogrn()
    else:
        return legal_ogrn()


def person_inn():
    

    mask11 = [7, 2, 4, 10, 3, 5, 9, 4, 6, 8]
    mask12 = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]
    inn = [random.randint(1, 9) for _ in range(12)]

    
    weighted11 = [v * mask11[i] for i, v in enumerate(inn[:-2])]
    inn[10] = sum(weighted11) % 11 % 10

    
    weighted12 = [v * mask12[i] for i, v in enumerate(inn[:-1])]
    inn[11] = sum(weighted12) % 11 % 10

    return """".join(map(str, inn))


def person_ogrn():
    

    ogrn = """".join(map(str, [random.randint(1, 9) for _ in range(14)]))
    ogrn += str((int(ogrn) % 13 % 10))
    return ogrn
","




















from __future__ import absolute_import, unicode_literals, print_function

import io
import os
import re
import sys
import time
import shutil
import tempfile
import textwrap
import subprocess
import webbrowser
from contextlib import contextmanager

try:
    from configparser import ConfigParser, Error as ConfigError
except ImportError:
    from ConfigParser import RawConfigParser as ConfigParser, Error as ConfigError  

import requests

from . import Collection, task
from .. import config
from ..util import notify
from ..util.filesys import pushd
from ..util.shell import capture


def get_pypi_auth(configfile='~/.pypirc'):
    

    pypi_cfg = ConfigParser()
    if pypi_cfg.read(os.path.expanduser(configfile)):
        try:
            user = pypi_cfg.get('pypi', 'username')
            pwd = pypi_cfg.get('pypi', 'password')
            return user, pwd
        except ConfigError:
            notify.warning(""No PyPI credentials in '{}',""
                           "" will fall back to '~/.netrc'..."".format(configfile))
    return None


def watchdogctl(ctx, kill=False, verbose=True):
    

    tries = 40 if kill else 0
    cmd = 'lsof -i TCP:{} -s TCP:LISTEN -S -Fp 2>/dev/null'.format(ctx.rituals.docs.watchdog.port)

    pidno = 0
    pidinfo = capture(cmd, ignore_failures=True)
    while pidinfo:
        pidline = next(filter(None, [re.match(r'^p(\d+)$', x) for x in pidinfo.splitlines()]))
        if not pidline:
            raise ValueError(""Standard lsof output expected (got {!r})"".format(pidinfo))
        pidno = int(pidline.group(1), 10)
        if verbose:
            ctx.run(""ps uw {}"".format(pidno), echo=False)
            verbose = False

        tries -= 1
        if tries <= 0:
            break
        else:
            try:
                os.kill(pidno, 0)
            
            
            except OSError as exc:  
                if exc.errno == 3:
                    break
                raise
            else:
                notify.info(""Killing PID {}"".format(pidno))
                ctx.run(""kill {}"".format(pidno), echo=False)
                time.sleep(.25)

        pid = capture(cmd, ignore_failures=True)

    return pidno


@task(default=True, help={
    'browse': ""Open index page in browser tab"",
    'clean': ""Start with a clean build area"",
    'watchdog': ""Start autobuild watchdog?"",
    'kill': ""Stop autobuild watchdog (and do nothing else)"",
    'status': ""Show autobuild watchdog process state"",
    'opts': ""Extra flags for Sphinx builder"",
})
def sphinx(ctx, browse=False, clean=False, watchdog=False, kill=False, status=False, opts=''):
    

    cfg = config.load()

    if kill or status:
        if not watchdogctl(ctx, kill=kill):
            notify.info(""No process bound to port {}"".format(ctx.rituals.docs.watchdog.port))
        return

    if clean:
        ctx.run(""invoke clean --docs"")

    
    for basename in ('README', 'CONTRIBUTING'):
        markdown = cfg.rootjoin(basename + '.md')
        if os.path.exists(markdown):
            try:
                import pypandoc
            except ImportError as exc:
                notify.warning(""Can't import 'pandoc' ({})"".format(exc))
                break
            else:
                pypandoc.convert(markdown, 'rst', outputfile=os.path.join(ctx.rituals.docs.sources, basename + '.rst'))

    
    if os.path.exists('LICENSE'):
        with io.open('LICENSE', 'r') as inp:
            license_text = inp.read()
            try:
                _, copyright_text = cfg.project['long_description'].split('Copyright', 1)
            except (KeyError, ValueError):
                copyright_text = cfg.project.get('license', 'N/A')
            with io.open(os.path.join(ctx.rituals.docs.sources, 'LICENSE.rst'), 'w') as out:
                out.write(
                    'Software License\n'
                    '================\n'
                    '\n'
                    '    Copyright {}\n'
                    '\n'
                    'Full License Text\n'
                    '-----------------\n'
                    '\n'
                    '::\n'
                    '\n'
                    .format(copyright_text)
                )
                license_text = textwrap.dedent(license_text)
                license_text = '\n    '.join(license_text.splitlines())
                out.write('    {}\n'.format(license_text))

    
    if cfg.project.get('packages') and str(ctx.rituals.docs.apidoc).lower()[:1] in 't1y':
        cmd = ['sphinx-apidoc', '-o', 'api', '-f', '-M']
        for package in cfg.project.packages:
            if '.' not in package:
                cmd.append(cfg.srcjoin(package))
        with pushd(ctx.rituals.docs.sources):
            ctx.run(' '.join(cmd))

    
    cmd = ['sphinx-build', '-b', 'html']
    if opts:
        cmd.append(opts)
    cmd.extend(['.', ctx.rituals.docs.build])
    index_url = index_file = os.path.join(ctx.rituals.docs.sources, ctx.rituals.docs.build, 'index.html')
    if watchdog:
        watchdogctl(ctx, kill=True)
        cmd[0:1] = ['nohup', 'sphinx-autobuild']
        cmd.extend([
               '-H', ctx.rituals.docs.watchdog.host,
               '-p', '{}'.format(ctx.rituals.docs.watchdog.port),
               ""-i'{}'"".format('*~'),
               ""-i'{}'"".format('.*'),
               ""-i'{}'"".format('*.log'),
               "">watchdog.log"", ""2>&1"", ""&"",
        ])
        index_url = ""http://{}:{}/"".format(ctx.rituals.docs.watchdog.host, ctx.rituals.docs.watchdog.port)

    
    notify.info(""Starting Sphinx {}build..."".format('auto' if watchdog else ''))
    with pushd(ctx.rituals.docs.sources):
        ctx.run(' '.join(cmd), pty=not watchdog)

    
    if watchdog:
        def activity(what=None, i=None):
            ""Helper""
            if i is None:
                sys.stdout.write(what + '\n')
            else:
                sys.stdout.write(' {}  Waiting for {}\r'.format(r'\|/-'[i % 4], what or 'something'))
            sys.stdout.flush()

        for i in range(60):
            activity('server start', i)
            if watchdogctl(ctx):
                activity('OK')
                break
            time.sleep(1)
        else:
            activity('ERR')

        
        if os.path.exists(os.path.join(ctx.rituals.docs.sources, 'index.rst')):
            os.utime(os.path.join(ctx.rituals.docs.sources, 'index.rst'), None)

        for i in range(60):
            activity('HTML index file', i)
            if os.path.exists(index_file):
                activity('OK')
                break
            time.sleep(1)
        else:
            activity('ERR')

    
    if browse:
        time.sleep(1)
        webbrowser.open_new_tab(index_url)


@task(help={
    'no-publish': ""Do not publish to Confluence, just build"",
    'clean': ""Start with a clean build area"",
    'opts': ""Extra flags for Sphinx builder"",
})
def confluence(ctx, no_publish=False, clean=False, opts=''):
    

    cfg = config.load()

    if clean:
        ctx.run(""invoke clean --docs"")

    cmd = ['sphinx-build', '-b', 'confluence']
    cmd.extend(['-E', '-a'])  
    if opts:
        cmd.append(opts)
    cmd.extend(['.', ctx.rituals.docs.build + '_cf'])
    if no_publish:
        cmd.extend(['-Dconfluence_publish=False'])

    
    notify.info(""Starting Sphinx build..."")
    with pushd(ctx.rituals.docs.sources):
        ctx.run(' '.join(cmd), pty=True)

try:
    import sphinxcontrib.confluencebuilder
except ImportError:
    del confluence


class DocsUploader(object):
    


    def __init__(self, ctx, cfg, target):
        self.ctx = ctx
        self.cfg = cfg
        self.target = target or ctx.rituals.docs.upload.method
        self.params = getattr(ctx.rituals.docs.upload.targets, self.target, None)

        if self.params is None:
            notify.failure(""Unknown upload target '{}'!"".format(self.target))
        if not self.params.get('url'):
            notify.failure(""You must provide an upload URL for target '{}', e.g. via the environment:\n""
                           ""    export INVOKE_RITUALS_DOCS_UPLOAD_TARGETS_{}_URL='http://.../{{name}}-{{version}}.zip'""
                           .format(self.target, self.target.upper()))

    @contextmanager
    def _zipped(self, docs_base):
        

        with pushd(docs_base):
            with tempfile.NamedTemporaryFile(prefix='pythonhosted-', delete=False) as ziphandle:
                pass
            zip_name = shutil.make_archive(ziphandle.name, 'zip')

        notify.info(""Uploading {:.1f} MiB from '{}' to '{}'...""
                    .format(os.path.getsize(zip_name) / 1024.0, zip_name, self.target))
        with io.open(zip_name, 'rb') as zipread:
            try:
                yield zipread
            finally:
                os.remove(ziphandle.name)
                os.remove(ziphandle.name + '.zip')

    def _to_pypi(self, docs_base, release):
        

        url = None
        with self._zipped(docs_base) as handle:
            reply = requests.post(self.params['url'], auth=get_pypi_auth(), allow_redirects=False,
                                  files=dict(content=(self.cfg.project.name + '.zip', handle, 'application/zip')),
                                  data={':action': 'doc_upload', 'name': self.cfg.project.name})
            if reply.status_code in range(200, 300):
                notify.info(""{status_code} {reason}"".format(**vars(reply)))
            elif reply.status_code == 301:
                url = reply.headers['location']
            else:
                data = self.cfg.copy()
                data.update(self.params)
                data.update(vars(reply))
                notify.error(""{status_code} {reason} for POST to {url}"".format(**data))
        return url

    def _to_webdav(self, docs_base, release):
        

        try:
            git_path = subprocess.check_output('git remote get-url origin 2>/dev/null', shell=True)
        except subprocess.CalledProcessError:
            git_path = ''
        else:
            git_path = git_path.decode('ascii').strip()
            git_path = git_path.replace('http://', '').replace('https://', '').replace('ssh://', '')
            git_path = re.search(r'[^:/]+?[:/](.+)', git_path)
            git_path = git_path.group(1).replace('.git', '') if git_path else ''
        url = None
        with self._zipped(docs_base) as handle:
            url_ns = dict(name=self.cfg.project.name, version=release, git_path=git_path)
            reply = requests.put(self.params['url'].format(**url_ns),
                                 data=handle.read(), headers={'Accept': 'application/json'})
            if reply.status_code in range(200, 300):
                notify.info(""{status_code} {reason}"".format(**vars(reply)))
                try:
                    data = reply.json()
                except ValueError as exc:
                    notify.warning(""Didn't get a JSON response! ({})"".format(exc))
                else:
                    if 'downloadUri' in data:  
                        url = data['downloadUri'] + '!/index.html'
            elif reply.status_code == 301:
                url = reply.headers['location']
            else:
                data = self.cfg.copy()
                data.update(self.params)
                data.update(vars(reply))
                notify.error(""{status_code} {reason} for PUT to {url}"".format(**data))

        if not url:
            notify.warning(""Couldn't get URL from upload response!"")
        return url

    def upload(self, docs_base, release):
        

        return getattr(self, '_to_' + self.target)(docs_base, release)


@task(help={
    'browse': ""Open index page on successful upload"",
    'target': ""Upload target name (default: pypi)"",
    'release': ""Version for upload path (default: latest)"",
})
def upload(ctx, browse=False, target=None, release='latest'):
    

    cfg = config.load()
    uploader = DocsUploader(ctx, cfg, target)

    html_dir = os.path.join(ctx.rituals.docs.sources, ctx.rituals.docs.build)
    if not os.path.isdir(html_dir):
        notify.failure(""No HTML docs dir found at '{}'!"".format(html_dir))

    url = uploader.upload(html_dir, release)
    notify.info(""Uploaded docs to '{url}'!"".format(url=url or 'N/A'))
    if url and browse:  
        webbrowser.open_new_tab(url)


namespace = Collection.from_module(sys.modules[__name__], name='docs', config={'rituals': dict(
    docs = dict(
        sources = 'docs',
        build = '_build',
        apidoc = True,
        watchdog = dict(
            host = '127.0.0.1',
            port = 8840,
        ),
        upload = dict(
            method = 'pypi',
            targets = dict(
                pypi = dict(url='https://pypi.python.org/pypi'),
                webdav = dict(url=None),  
            ),
        ),
    ),
)})
","











from __future__ import print_function
import os
import sys
import time
import glob
import shutil
import datetime
import pandas as pd
import multiprocessing
import subprocess as sps
import ipyparallel as ipp
from ipyrad.assemble.util import IPyradWarningExit, progressbar



MISSING_IMPORTS = 


ACCESSION_ID = 



class SRA(object):
    

    def __init__(self, 
        accession,
        workdir=""sra-fastq-data"",
        ):

        
        for binary in ['fastq-dump', 'esearch']:
            if not sps.call(""type "" + binary, 
                        shell=True,
                        stdout=sps.PIPE,
                        stderr=sps.PIPE) == 0:
                raise IPyradWarningExit(MISSING_IMPORTS)

        
        self.accession = accession
        self.workdir = os.path.abspath(os.path.expanduser(workdir))
        self.is_sample = False
        self.is_project = False
        self._oldtmpdir = None

        
        self._ipcluster = {
            ""cluster_id"": """", 
            ""profile"": ""default"",
            ""engines"": ""Local"", 
            ""quiet"": 0, 
            ""timeout"": 60, 
            ""cores"": 0, 
            ""threads"" : 2,
            ""pids"": {},
            }

        
        if any([i in self.accession for i in [""SRR"", ""ERR"", ""DRR""]]):        
            self.is_sample = True
        elif any([i in self.accession for i in [""SRP"", ""ERP"", ""DRP""]]):
            self.is_project = True
        else:
            raise IPyradWarningExit(ACCESSION_ID)



    
    def run(self, 
        force=False, 
        ipyclient=None, 
        name_fields=30, 
        name_separator=""_"", 
        dry_run=False):
        


        
        
        try:
            
            if not os.path.exists(self.workdir):
                os.makedirs(self.workdir)

            
            
            self._set_vdbconfig_path()

            
            if ipyclient:
                self._ipcluster[""pids""] = {}
                for eid in ipyclient.ids:
                    engine = ipyclient[eid]
                    if not engine.outstanding:
                        pid = engine.apply(os.getpid).get()
                        self._ipcluster[""pids""][eid] = pid               

            
            self._submit_jobs(
                force=force, 
                ipyclient=ipyclient, 
                name_fields=name_fields, 
                name_separator=name_separator,
                dry_run=dry_run,
                )

        except IPyradWarningExit as inst:
            print(inst)
        
        except KeyboardInterrupt:
            print(""keyboard interrupt..."")
        except Exception as inst:
            print(""Exception in run() - {}"".format(inst))
        finally:
            
            self._restore_vdbconfig_path()

            
            
            
            sradir = os.path.join(self.workdir, ""sra"")
            if os.path.exists(sradir) and (not os.listdir(sradir)):
                shutil.rmtree(sradir)
            else:
                
                try:
                    print(FAILED_DOWNLOAD.format(os.listdir(sradir)))
                except OSError as inst:
                    
                    raise IPyradWarningExit(""Download failed. Exiting."")
                
                for srr in os.listdir(sradir):
                    isrr = srr.split(""."")[0]
                    ipath = os.path.join(self.workdir, ""*_{}*.gz"".format(isrr))
                    ifile = glob.glob(ipath)[0]
                    if os.path.exists(ifile):
                        os.remove(ifile)
                
                shutil.rmtree(sradir)

            
            if ipyclient:
                
                try:
                    ipyclient.abort()
                    time.sleep(0.5)
                    for engine_id, pid in self._ipcluster[""pids""].items():
                        if ipyclient.queue_status()[engine_id][""tasks""]:
                            os.kill(pid, 2)
                        time.sleep(0.1)
                except ipp.NoEnginesRegistered:
                    pass
                
                if not ipyclient.outstanding:
                    ipyclient.purge_everything()
                
                else:
                    ipyclient.shutdown(hub=True, block=False)
                    ipyclient.close()
                    print(""\nwarning: ipcluster shutdown and must be restarted"")
                    


    def _submit_jobs(self, 
        force, 
        ipyclient, 
        name_fields, 
        name_separator, 
        dry_run):
        


        
        df = self.fetch_runinfo(range(31), quiet=True)
        sys.stdout.flush()

        
        if ipyclient:
            lb = ipyclient.load_balanced_view()

        
        
        if name_fields:
            
            fields = [int(i)-1 for i in fields_checker(name_fields)]
            
            df['Accession'] = pd.Series(df[df.columns[fields[0]]], index=df.index)
            for field in fields[1:]:
                df.Accession += name_separator + df[df.columns[field]]
            df.Accession = [i.replace("" "", ""_"") for i in df.Accession]    
            
            if not df.Accession.shape[0] == df.Accession.unique().shape[0]:
                raise IPyradWarningExit(""names are not unique:\n{}""\
                    .format(df.Accession))

        
        else:
            if len(set(df.SampleName)) != len(df.SampleName):
                accs = (i+""-""+j for i, j in zip(df.SampleName, df.Run))
                df.Accession = accs
            else:
                df.Accession = df.SampleName

        if dry_run:
            print(""\rThe following files will be written to: {}"".format(self.workdir))
            print(""{}\n"".format(df.Accession))
        else:
            
            asyncs = []
            for idx in df.index:

                
                srr = df.Run[idx]
                outname = df.Accession[idx]
                paired = df.spots_with_mates.values.astype(int).nonzero()[0].any()
                fpath = os.path.join(self.workdir, outname+"".fastq.gz"")

                
                skip = False
                if force:
                    if os.path.exists(fpath):
                        os.remove(fpath)
                else:
                    if os.path.exists(fpath):                
                        skip = True
                        sys.stdout.flush()
                        print(""[skip] file already exists: {}"".format(fpath))

                
                tidx = df.Accession.shape[0]
                
                    

                
                if not skip:
                    args = (self, srr, outname, paired)
                    if ipyclient:
                        async = lb.apply_async(call_fastq_dump_on_SRRs, *args)
                        asyncs.append(async)
                    else:
                        print(""Downloading file {}/{}: {}"".format(idx+1, tidx, fpath))
                        call_fastq_dump_on_SRRs(*args)
                        sys.stdout.flush()

            
            if ipyclient:
                tots = df.Accession.shape[0]
                printstr = "" Downloading fastq files | {} | ""
                start = time.time()
                while 1:
                    elapsed = datetime.timedelta(seconds=int(time.time()-start))
                    ready = sum([i.ready() for i in asyncs])
                    progressbar(tots, ready, printstr.format(elapsed), spacer="""")
                    time.sleep(0.1)
                    if tots == ready:
                        print("""")
                        break
                self._report(tots)

                
                for async in asyncs:
                    if not async.successful():
                        raise IPyradWarningExit(async.result())



    def _report(self, N):
        print(""{} fastq files downloaded to {}"".format(N, self.workdir))


    @property
    def fetch_fields(self):
        fields = pd.DataFrame(data=[COLNAMES, range(1, len(COLNAMES)+1)]).T
        fields.columns=['field', 'index']
        return fields


    def fetch_runinfo(self, fields=None, quiet=False):
        

        if not quiet:
            print(""\rFetching project data..."", end="""")

        
        if fields == None:  
            fields = range(30)
        fields = fields_checker(fields)

        
        es_cmd = [
            ""esearch"", 
            ""-db"", ""sra"", 
            ""-query"", self.accession,
        ]

        ef_cmd = [
            ""efetch"", 
            ""--format"", ""runinfo"",
        ]

        cut_cmd = [
            ""cut"", 
            ""-d"", "","", 
            ""-f"", "","".join(fields),
        ]

        
        proc1 = sps.Popen(es_cmd, stderr=sps.STDOUT, stdout=sps.PIPE)
        proc2 = sps.Popen(ef_cmd, stdin=proc1.stdout, stderr=sps.STDOUT, stdout=sps.PIPE)
        proc3 = sps.Popen(cut_cmd, stdin=proc2.stdout, stderr=sps.STDOUT, stdout=sps.PIPE)
        o, e = proc3.communicate()
        proc2.stdout.close()
        proc1.stdout.close()
        
        if o:
            vals = o.strip().split(""\n"")
            names = vals[0].split("","")
            items = [i.split("","") for i in vals[1:] if i not in ["""", vals[0]]]
            return pd.DataFrame(items, columns=names)
        else:
            raise IPyradWarningExit(""no samples found in {}"".format(self.accession))


    def _set_vdbconfig_path(self):

        
        proc = sps.Popen(
            ['vdb-config', '-p'], 
            stderr=sps.STDOUT, stdout=sps.PIPE)
        o, e = proc.communicate()
        self._oldtmpdir = o.split(""root>"")[1][:-2]

        
        proc = sps.Popen(
            ['vdb-config', '-s', 
            'repository/user/main/public/root='+self.workdir], 
            stderr=sps.STDOUT, stdout=sps.PIPE)
        o, e = proc.communicate()
        


    def _restore_vdbconfig_path(self):
        
        if not self._oldtmpdir:
            self._oldtmpdir = os.path.join(os.path.expanduser(""~""), ""ncbi"")
        proc = sps.Popen(
            ['vdb-config', '-s', 
            'repository/user/main/public/root='+self._oldtmpdir],
            stderr=sps.STDOUT, stdout=sps.PIPE)
        o, e = proc.communicate()
        



def call_fastq_dump_on_SRRs(self, srr, outname, paired):
    


    
    fd_cmd = [
        ""fastq-dump"", srr,
        ""--accession"", outname,
        ""--outdir"", self.workdir, 
        ""--gzip"",
        ]
    if paired:
        fd_cmd += [""--split-files""]

    
    proc = sps.Popen(fd_cmd, stderr=sps.STDOUT, stdout=sps.PIPE)
    o, e = proc.communicate()

    
    
    
    srafile = os.path.join(self.workdir, ""sra"", srr+"".sra"")
    if os.path.exists(srafile):
        os.remove(srafile)



def fields_checker(fields):
    

    
    if isinstance(fields, int):
        fields = str(fields)
    if isinstance(fields, str):
        if "","" in fields:
            fields = [str(i) for i in fields.split("","")]
        else:
            fields = [str(fields)]
    elif isinstance(fields, (tuple, list)):
        fields = [str(i) for i in fields]
    else:
        raise IPyradWarningExit(""fields not properly formatted"")

    
    fields = [i for i in fields if i != '0']

    return fields


FAILED_DOWNLOAD = 



COLNAMES = [
 'Run',
 'ReleaseDate',
 'LoadDate',
 'spots',
 'bases',
 'spots_with_mates',
 'avgLength',
 'size_MB',
 'AssemblyName',
 'download_path',
 'Experiment',
 'LibraryName',
 'LibraryStrategy',
 'LibrarySelection',
 'LibrarySource',
 'LibraryLayout',
 'InsertSize',
 'InsertDev',
 'Platform',
 'Model',
 'SRAStudy',
 'BioProject',
 'Study_Pubmed_id',
 'ProjectID',
 'Sample',
 'BioSample',
 'SampleType',
 'TaxID',
 'ScientificName',
 'SampleName',
 'g1k_pop_code',
 'source',
 'g1k_analysis_group',
 'Subject_ID',
 'Sex',
 'Disease',
 'Tumor',
 'Affection_Status',
 'Analyte_Type',
 'Histological_Type',
 'Body_Site',
 'CenterName',
 'Submission',
 'dbgap_study_accession',
 'Consent',
 'RunHash',
 'ReadHash',
 ]

","

from json import dumps
from threading import Thread
from time import sleep

from cbexchange.market import MarketClient
from cbexchange.websock import WSClient


class OrderBook(object):
  

  WS_URI = 'wss://ws-feed.exchange.coinbase.com'
  API_URI = 'https://api.exchange.coinbase.com'
  PRODUCT_ID = 'BTC-USD'

  def __init__(self, ws_uri=None, api_uri=None, product_id=None):
    self.API_URI = api_uri or self.API_URI
    self.WS_URI = ws_uri or self.WS_URI
    self.PRODUCT_ID = product_id or self.PRODUCT_ID
    self.ws_client = WSClient(ws_uri=self.WS_URI, ws_product_id=self.PRODUCT_ID)
    self.ws_client.connect()
    client = MarketClient(api_uri=self.API_URI)
    self.book = client.get_product_order_book(3, self.PRODUCT_ID)
    self.sequence = self.book['sequence']
    self.die = False
    self.pause = False

    asks = {}
    for entry in self.book['asks']:
      asks[entry[2]] = {'price':entry[0], 'size':entry[1]}
    self.book['asks'] = asks

    bids = {}
    for entry in self.book['bids']:
      bids[entry[2]] = {'price':entry[0], 'size':entry[1]}
    self.book['bids'] = bids

    Thread(target=self._real_time_thread, args=[]).start()

  def _get_key(self, message):
    if message['side'] == 'buy':
      return 'bids'
    else:
      return 'asks'

  def _get_order_id(self, message):
    return message['order_id']

  def _handle_open(self, message):
    key = self._get_key(message)
    self.book[key][self._get_order_id(message)] = {
      'price':message['price'],
      'size':message['remaining_size']
    }

  def _handle_match(self, message):
    maker = message['maker_order_id']
    key = self._get_key(message)
    if maker in self.book[key]:
      left = float(self.book[key][maker]['size'])
      right = float(message['size'])
      self.book[key][maker]['size'] = str(left - right)

  def _handle_done(self, message):
    if message['order_type'] == 'market':
      return
    key = self._get_key(message)
    self.book[key].pop(self._get_order_id(message), None)

  def _handle_change(self, message):
    if message['price'] is None: 
      return
    if 'new_funds' in message:   
      return
    key = self._get_key(message)
    order_id = self._get_order_id(message)
    if order_id in self.book[key]:
      self.book[key][order_id]['size'] = message['remaining_size']

  def _real_time_thread(self):
    

    while self.ws_client.connected():
      if self.die:
        break
      
      if self.pause:
        sleep(5)
        continue

      message = self.ws_client.receive()

      if message is None:
        break

      message_type = message['type']

      if message_type  == 'error':
        continue
      if message['sequence'] <= self.sequence:
        continue

      if message_type == 'open':
        self._handle_open(message)
      elif message_type == 'match':
        self._handle_match(message)
      elif message_type == 'done':
        self._handle_done(message)
      elif message_type == 'change':
        self._handle_change(message)
      else:
        continue

    self.ws_client.disconnect()

  def __enter__(self):
    return self
  
  def __exit__(self, type, value, traceback):
    self.end()

  def __str__(self):
    if self.book:
      return dumps(self.book, indent=2, sort_keys=True)
    else:
      return str(self.book)

  def end(self):
    

    self.die = True

  def pause(self):
    

    self.pause = True
  
  def resume(self):
    

    self.pause = False

  def get_order_book(self):
    

    return self.book
","





import logging


from yotta.lib import version

from yotta.lib import access_common

logger = logging.getLogger('access')





class GitCloneVersion(version.Version):
    def __init__(self, semver, tag, working_copy):
        self.working_copy = working_copy
        self.tag = tag
        super(GitCloneVersion, self).__init__(semver)

    def unpackInto(self, directory):
        
        from yotta.lib import vcs
        
        from yotta.lib import fsutils
        logger.debug('unpack version %s from git repo %s to %s' % (self.version, self.working_copy.directory, directory))
        tag = self.tag
        fsutils.rmRf(directory)
        vcs.Git.cloneToDirectory(self.working_copy.directory, directory, tag)

        
        self.working_copy.remove()

class GitWorkingCopy(object):
    def __init__(self, vcs):
        self.vcs = vcs
        self.directory = vcs.workingDirectory()

    def remove(self):
        self.vcs.remove()
        self.directory = None

    def availableVersions(self):
        

        r = []
        for t in self.vcs.tags():
            logger.debug(""available version tag: %s"", t)
            
            if not len(t.strip()):
                continue
            try:
                r.append(GitCloneVersion(t, t, self))
            except ValueError:
                logger.debug('invalid version tag: %s', t)
        return r

    def availableTags(self):
        

        return [GitCloneVersion('', t, self) for t in self.vcs.tags()]

    def availableBranches(self):
        

        return [GitCloneVersion('', b, self) for b in self.vcs.branches()]


    def tipVersion(self):
        return GitCloneVersion('', '', self)

    def commitVersion(self, spec):
        

        import re

        commit_match = re.match('^[a-f0-9]{7,40}$', spec, re.I)
        if commit_match:
            return GitCloneVersion('', spec, self)

        return None

class GitComponent(access_common.RemoteComponent):
    def __init__(self, url, tag_or_branch=None, semantic_spec=None):
        logging.debug('create git component for url:%s version spec:%s' % (url, semantic_spec or tag_or_branch))
        self.url = url
        
        self.spec = semantic_spec
        self.tag_or_branch = tag_or_branch

    @classmethod
    def createFromSource(cls, vs, name=None):
        

        return GitComponent(vs.location, vs.spec, vs.semantic_spec)

    def versionSpec(self):
        return self.spec

    def tagOrBranchSpec(self):
        return self.tag_or_branch

    
    
    
    
    
    
    def clone(self):
        
        from yotta.lib import vcs
        clone = vcs.Git.cloneToTemporaryDir(self.url)
        clone.fetchAllBranches()
        return GitWorkingCopy(clone)

    @classmethod
    def remoteType(cls):
        return 'git'
","













from __future__ import division

from collections import OrderedDict
from functools import partial

import empyrical as ep
import numpy as np
import pandas as pd
import scipy as sp
import scipy.stats as stats
from sklearn import linear_model

from .deprecate import deprecated
from .interesting_periods import PERIODS
from .txn import get_turnover
from .utils import APPROX_BDAYS_PER_MONTH, APPROX_BDAYS_PER_YEAR
from .utils import DAILY

DEPRECATION_WARNING = (""Risk functions in pyfolio.timeseries are deprecated ""
                       ""and will be removed in a future release. Please ""
                       ""install the empyrical package instead."")


def var_cov_var_normal(P, c, mu=0, sigma=1):
    


    alpha = sp.stats.norm.ppf(1 - c, mu, sigma)
    return P - P * (alpha + 1)


@deprecated(msg=DEPRECATION_WARNING)
def max_drawdown(returns):
    


    return ep.max_drawdown(returns)


@deprecated(msg=DEPRECATION_WARNING)
def annual_return(returns, period=DAILY):
    


    return ep.annual_return(returns, period=period)


@deprecated(msg=DEPRECATION_WARNING)
def annual_volatility(returns, period=DAILY):
    


    return ep.annual_volatility(returns, period=period)


@deprecated(msg=DEPRECATION_WARNING)
def calmar_ratio(returns, period=DAILY):
    


    return ep.calmar_ratio(returns, period=period)


@deprecated(msg=DEPRECATION_WARNING)
def omega_ratio(returns, annual_return_threshhold=0.0):
    


    return ep.omega_ratio(returns,
                          required_return=annual_return_threshhold)


@deprecated(msg=DEPRECATION_WARNING)
def sortino_ratio(returns, required_return=0, period=DAILY):
    


    return ep.sortino_ratio(returns, required_return=required_return)


@deprecated(msg=DEPRECATION_WARNING)
def downside_risk(returns, required_return=0, period=DAILY):
    


    return ep.downside_risk(returns,
                            required_return=required_return,
                            period=period)


@deprecated(msg=DEPRECATION_WARNING)
def sharpe_ratio(returns, risk_free=0, period=DAILY):
    


    return ep.sharpe_ratio(returns, risk_free=risk_free, period=period)


@deprecated(msg=DEPRECATION_WARNING)
def alpha_beta(returns, factor_returns):
    


    return ep.alpha_beta(returns, factor_returns=factor_returns)


@deprecated(msg=DEPRECATION_WARNING)
def alpha(returns, factor_returns):
    


    return ep.alpha(returns, factor_returns=factor_returns)


@deprecated(msg=DEPRECATION_WARNING)
def beta(returns, factor_returns):
    


    return ep.beta(returns, factor_returns)


@deprecated(msg=DEPRECATION_WARNING)
def stability_of_timeseries(returns):
    


    return ep.stability_of_timeseries(returns)


@deprecated(msg=DEPRECATION_WARNING)
def tail_ratio(returns):
    


    return ep.tail_ratio(returns)


def common_sense_ratio(returns):
    


    return ep.tail_ratio(returns) * \
        (1 + ep.annual_return(returns))


def normalize(returns, starting_value=1):
    


    return starting_value * (returns / returns.iloc[0])


@deprecated(msg=DEPRECATION_WARNING)
def cum_returns(returns, starting_value=0):
    


    return ep.cum_returns(returns, starting_value=starting_value)


@deprecated(msg=DEPRECATION_WARNING)
def aggregate_returns(returns, convert_to):
    


    return ep.aggregate_returns(returns, convert_to=convert_to)


def rolling_beta(returns, factor_returns,
                 rolling_window=APPROX_BDAYS_PER_MONTH * 6):
    


    if factor_returns.ndim > 1:
        
        return factor_returns.apply(partial(rolling_beta, returns),
                                    rolling_window=rolling_window)
    else:
        out = pd.Series(index=returns.index)
        for beg, end in zip(returns.index[0:-rolling_window],
                            returns.index[rolling_window:]):
            out.loc[end] = ep.beta(
                returns.loc[beg:end],
                factor_returns.loc[beg:end])

        return out


def rolling_regression(returns, factor_returns,
                       rolling_window=APPROX_BDAYS_PER_MONTH * 6,
                       nan_threshold=0.1):
    


    
    ret_no_na = returns.dropna()

    columns = ['alpha'] + factor_returns.columns.tolist()
    rolling_risk = pd.DataFrame(columns=columns,
                                index=ret_no_na.index)

    rolling_risk.index.name = 'dt'

    for beg, end in zip(ret_no_na.index[:-rolling_window],
                        ret_no_na.index[rolling_window:]):
        returns_period = ret_no_na[beg:end]
        factor_returns_period = factor_returns.loc[returns_period.index]

        if np.all(factor_returns_period.isnull().mean()) < nan_threshold:
            factor_returns_period_dnan = factor_returns_period.dropna()
            reg = linear_model.LinearRegression(fit_intercept=True).fit(
                factor_returns_period_dnan,
                returns_period.loc[factor_returns_period_dnan.index])
            rolling_risk.loc[end, factor_returns.columns] = reg.coef_
            rolling_risk.loc[end, 'alpha'] = reg.intercept_

    return rolling_risk


def gross_lev(positions):
    


    exposure = positions.drop('cash', axis=1).abs().sum(axis=1)
    return exposure / positions.sum(axis=1)


def value_at_risk(returns, period=None, sigma=2.0):
    

    if period is not None:
        returns_agg = ep.aggregate_returns(returns, period)
    else:
        returns_agg = returns.copy()

    value_at_risk = returns_agg.mean() - sigma * returns_agg.std()
    return value_at_risk


SIMPLE_STAT_FUNCS = [
    ep.annual_return,
    ep.cum_returns_final,
    ep.annual_volatility,
    ep.sharpe_ratio,
    ep.calmar_ratio,
    ep.stability_of_timeseries,
    ep.max_drawdown,
    ep.omega_ratio,
    ep.sortino_ratio,
    stats.skew,
    stats.kurtosis,
    ep.tail_ratio,
    value_at_risk
]

FACTOR_STAT_FUNCS = [
    ep.alpha,
    ep.beta,
]

STAT_FUNC_NAMES = {
    'annual_return': 'Annual return',
    'cum_returns_final': 'Cumulative returns',
    'annual_volatility': 'Annual volatility',
    'sharpe_ratio': 'Sharpe ratio',
    'calmar_ratio': 'Calmar ratio',
    'stability_of_timeseries': 'Stability',
    'max_drawdown': 'Max drawdown',
    'omega_ratio': 'Omega ratio',
    'sortino_ratio': 'Sortino ratio',
    'skew': 'Skew',
    'kurtosis': 'Kurtosis',
    'tail_ratio': 'Tail ratio',
    'common_sense_ratio': 'Common sense ratio',
    'value_at_risk': 'Daily value at risk',
    'alpha': 'Alpha',
    'beta': 'Beta',
}


def perf_stats(returns, factor_returns=None, positions=None,
               transactions=None, turnover_denom='AGB'):
    


    stats = pd.Series()
    for stat_func in SIMPLE_STAT_FUNCS:
        stats[STAT_FUNC_NAMES[stat_func.__name__]] = stat_func(returns)

    if positions is not None:
        stats['Gross leverage'] = gross_lev(positions).mean()
        if transactions is not None:
            stats['Daily turnover'] = get_turnover(positions,
                                                   transactions,
                                                   turnover_denom).mean()
    if factor_returns is not None:
        for stat_func in FACTOR_STAT_FUNCS:
            res = stat_func(returns, factor_returns)
            stats[STAT_FUNC_NAMES[stat_func.__name__]] = res

    return stats


def perf_stats_bootstrap(returns, factor_returns=None, return_stats=True,
                         **kwargs):
    


    bootstrap_values = OrderedDict()

    for stat_func in SIMPLE_STAT_FUNCS:
        stat_name = STAT_FUNC_NAMES[stat_func.__name__]
        bootstrap_values[stat_name] = calc_bootstrap(stat_func,
                                                     returns)

    if factor_returns is not None:
        for stat_func in FACTOR_STAT_FUNCS:
            stat_name = STAT_FUNC_NAMES[stat_func.__name__]
            bootstrap_values[stat_name] = calc_bootstrap(
                stat_func,
                returns,
                factor_returns=factor_returns)

    bootstrap_values = pd.DataFrame(bootstrap_values)

    if return_stats:
        stats = bootstrap_values.apply(calc_distribution_stats)
        return stats.T[['mean', 'median', '5%', '95%']]
    else:
        return bootstrap_values


def calc_bootstrap(func, returns, *args, **kwargs):
    


    n_samples = kwargs.pop('n_samples', 1000)
    out = np.empty(n_samples)

    factor_returns = kwargs.pop('factor_returns', None)

    for i in range(n_samples):
        idx = np.random.randint(len(returns), size=len(returns))
        returns_i = returns.iloc[idx].reset_index(drop=True)
        if factor_returns is not None:
            factor_returns_i = factor_returns.iloc[idx].reset_index(drop=True)
            out[i] = func(returns_i, factor_returns_i,
                          *args, **kwargs)
        else:
            out[i] = func(returns_i,
                          *args, **kwargs)

    return out


def calc_distribution_stats(x):
    


    return pd.Series({'mean': np.mean(x),
                      'median': np.median(x),
                      'std': np.std(x),
                      '5%': np.percentile(x, 5),
                      '25%': np.percentile(x, 25),
                      '75%': np.percentile(x, 75),
                      '95%': np.percentile(x, 95),
                      'IQR': np.subtract.reduce(
                          np.percentile(x, [75, 25])),
                      })


def get_max_drawdown_underwater(underwater):
    


    valley = np.argmin(underwater)  
    
    peak = underwater[:valley][underwater[:valley] == 0].index[-1]
    
    try:
        recovery = underwater[valley:][underwater[valley:] == 0].index[0]
    except IndexError:
        recovery = np.nan  
    return peak, valley, recovery


def get_max_drawdown(returns):
    


    returns = returns.copy()
    df_cum = cum_returns(returns, 1.0)
    running_max = np.maximum.accumulate(df_cum)
    underwater = df_cum / running_max - 1
    return get_max_drawdown_underwater(underwater)


def get_top_drawdowns(returns, top=10):
    


    returns = returns.copy()
    df_cum = ep.cum_returns(returns, 1.0)
    running_max = np.maximum.accumulate(df_cum)
    underwater = df_cum / running_max - 1

    drawdowns = []
    for t in range(top):
        peak, valley, recovery = get_max_drawdown_underwater(underwater)
        
        if not pd.isnull(recovery):
            underwater.drop(underwater[peak: recovery].index[1:-1],
                            inplace=True)
        else:
            
            underwater = underwater.loc[:peak]

        drawdowns.append((peak, valley, recovery))
        if (len(returns) == 0) or (len(underwater) == 0):
            break

    return drawdowns


def gen_drawdown_table(returns, top=10):
    


    df_cum = ep.cum_returns(returns, 1.0)
    drawdown_periods = get_top_drawdowns(returns, top=top)
    df_drawdowns = pd.DataFrame(index=list(range(top)),
                                columns=['Net drawdown in %',
                                         'Peak date',
                                         'Valley date',
                                         'Recovery date',
                                         'Duration'])

    for i, (peak, valley, recovery) in enumerate(drawdown_periods):
        if pd.isnull(recovery):
            df_drawdowns.loc[i, 'Duration'] = np.nan
        else:
            df_drawdowns.loc[i, 'Duration'] = len(pd.date_range(peak,
                                                                recovery,
                                                                freq='B'))
        df_drawdowns.loc[i, 'Peak date'] = (peak.to_pydatetime()
                                            .strftime('%Y-%m-%d'))
        df_drawdowns.loc[i, 'Valley date'] = (valley.to_pydatetime()
                                              .strftime('%Y-%m-%d'))
        if isinstance(recovery, float):
            df_drawdowns.loc[i, 'Recovery date'] = recovery
        else:
            df_drawdowns.loc[i, 'Recovery date'] = (recovery.to_pydatetime()
                                                    .strftime('%Y-%m-%d'))
        df_drawdowns.loc[i, 'Net drawdown in %'] = (
            (df_cum.loc[peak] - df_cum.loc[valley]) / df_cum.loc[peak]) * 100

    df_drawdowns['Peak date'] = pd.to_datetime(df_drawdowns['Peak date'])
    df_drawdowns['Valley date'] = pd.to_datetime(df_drawdowns['Valley date'])
    df_drawdowns['Recovery date'] = pd.to_datetime(
        df_drawdowns['Recovery date'])

    return df_drawdowns


def rolling_volatility(returns, rolling_vol_window):
    


    return returns.rolling(rolling_vol_window).std() \
        * np.sqrt(APPROX_BDAYS_PER_YEAR)


def rolling_sharpe(returns, rolling_sharpe_window):
    


    return returns.rolling(rolling_sharpe_window).mean() \
        / returns.rolling(rolling_sharpe_window).std() \
        * np.sqrt(APPROX_BDAYS_PER_YEAR)


def simulate_paths(is_returns, num_days,
                   starting_value=1, num_samples=1000, random_seed=None):
    


    samples = np.empty((num_samples, num_days))
    seed = np.random.RandomState(seed=random_seed)
    for i in range(num_samples):
        samples[i, :] = is_returns.sample(num_days, replace=True,
                                          random_state=seed)

    return samples


def summarize_paths(samples, cone_std=(1., 1.5, 2.), starting_value=1.):
    


    cum_samples = ep.cum_returns(samples.T,
                                 starting_value=starting_value).T

    cum_mean = cum_samples.mean(axis=0)
    cum_std = cum_samples.std(axis=0)

    if isinstance(cone_std, (float, int)):
        cone_std = [cone_std]

    cone_bounds = pd.DataFrame(columns=pd.Float64Index([]))
    for num_std in cone_std:
        cone_bounds.loc[:, float(num_std)] = cum_mean + cum_std * num_std
        cone_bounds.loc[:, float(-num_std)] = cum_mean - cum_std * num_std

    return cone_bounds


def forecast_cone_bootstrap(is_returns, num_days, cone_std=(1., 1.5, 2.),
                            starting_value=1, num_samples=1000,
                            random_seed=None):
    


    samples = simulate_paths(
        is_returns=is_returns,
        num_days=num_days,
        starting_value=starting_value,
        num_samples=num_samples,
        random_seed=random_seed
    )

    cone_bounds = summarize_paths(
        samples=samples,
        cone_std=cone_std,
        starting_value=starting_value
    )

    return cone_bounds


def extract_interesting_date_ranges(returns):
    


    returns_dupe = returns.copy()
    returns_dupe.index = returns_dupe.index.map(pd.Timestamp)
    ranges = OrderedDict()
    for name, (start, end) in PERIODS.items():
        try:
            period = returns_dupe.loc[start:end]
            if len(period) == 0:
                continue
            ranges[name] = period
        except BaseException:
            continue

    return ranges
","


import hashlib
import re


class TamperEvidentFile(object):
    


    def __init__(self, filename):
        self.filename = filename

    def write(self, text, hashline=b""
        u

        if not text.endswith(b""\n""):
            text += b""\n""

        actual_hash = hashlib.sha1(text).hexdigest()

        with open(self.filename, ""wb"") as f:
            f.write(text)
            f.write(hashline.decode(""utf8"").format(actual_hash).encode(""utf8""))
            f.write(b""\n"")

    def validate(self):
        


        with open(self.filename, ""rb"") as f:
            text = f.read()

        start_last_line = text.rfind(b""\n"", 0, -1)
        if start_last_line == -1:
            return False

        original_text = text[:start_last_line+1]
        last_line = text[start_last_line+1:]

        expected_hash = hashlib.sha1(original_text).hexdigest().encode('utf8')
        match = re.search(b""[0-9a-f]{40}"", last_line)
        if not match:
            return False
        actual_hash = match.group(0)
        return actual_hash == expected_hash
","

import ShirtsIO
import yaml
import os
import code

def new_user(yaml_path):
    


    print 'Retrieve API Key from https://www.shirts.io/accounts/api_console/'
    api_key = raw_input('Shirts.io API Key: ')

    tokens = {
        'api_key': api_key,
    }

    yaml_file = open(yaml_path, 'w+')
    yaml.dump(tokens, yaml_file, indent=2)
    yaml_file.close()

    return tokens


if __name__ == '__main__':
    yaml_path = os.path.expanduser('~') + '/.pyshirtsio'

    if not os.path.exists(yaml_path):
        tokens = new_user(yaml_path)
    else:
        yaml_file = open(yaml_path, ""r"")
        tokens = yaml.safe_load(yaml_file)
        yaml_file.close()

    client = ShirtsIO.ShirtsIOClient(tokens['api_key'])

    print 'You may run PyShirtsIO commands prefixed with ""client."".""'
    print 'client = ShirtsIOClient()\n'

    code.interact(local=dict(globals(), **{'client': client}))
","















import os
import textwrap

from . import job_model

_LOCALIZE_COMMAND_MAP = {
    job_model.P_GCS: 'gsutil -m rsync -r',
    job_model.P_LOCAL: 'rsync -r',
}




























DATA_MOUNT_POINT = '/mnt/data'

SCRIPT_DIR = '%s/script' % DATA_MOUNT_POINT
TMP_DIR = '%s/tmp' % DATA_MOUNT_POINT
WORKING_DIR = '%s/workingdir' % DATA_MOUNT_POINT


def get_file_environment_variables(file_params):
  

  env = {}
  for param in file_params:
    
    
    
    
    env[param.name] = os.path.join(
        DATA_MOUNT_POINT, param.docker_path.rstrip('/')) if param.value else ''
  return env


def build_recursive_localize_env(destination, inputs):
  

  export_input_dirs = '\n'.join([
      'export {0}={1}/{2}'.format(var.name, destination.rstrip('/'),
                                  var.docker_path.rstrip('/'))
      for var in inputs
      if var.recursive and var.docker_path
  ])
  return export_input_dirs


def build_recursive_localize_command(destination, inputs, file_provider):
  

  command = _LOCALIZE_COMMAND_MAP[file_provider]
  filtered_inputs = [
      var for var in inputs
      if var.recursive and var.file_provider == file_provider
  ]

  copy_input_dirs = '\n'.join([
      textwrap.dedent(
).format(
          command=command,
          source_uri=var.uri,
          data_mount=destination.rstrip('/'),
          docker_path=var.docker_path) for var in filtered_inputs
  ])
  return copy_input_dirs


def build_recursive_gcs_delocalize_env(source, outputs):
  

  filtered_outs = [
      var for var in outputs
      if var.recursive and var.file_provider == job_model.P_GCS
  ]
  return '\n'.join([
      'export {0}={1}/{2}'.format(var.name,
                                  source.rstrip('/'),
                                  var.docker_path.rstrip('/'))
      for var in filtered_outs
  ])


def build_recursive_delocalize_command(source, outputs, file_provider):
  

  command = _LOCALIZE_COMMAND_MAP[file_provider]
  filtered_outputs = [
      var for var in outputs
      if var.recursive and var.file_provider == file_provider
  ]

  return '\n'.join([
      textwrap.dedent(
).format(
          command=command,
          data_mount=source.rstrip('/'),
          docker_path=var.docker_path,
          destination_uri=var.uri) for var in filtered_outputs
  ])


def get_task_metadata(job_metadata, task_id):
  

  task_metadata = job_metadata.copy()
  task_metadata['task-id'] = task_id

  return task_metadata


def build_mount_env(source, mounts):
  

  return '\n'.join([
      'export {0}={1}/{2}'.format(var.name, source.rstrip('/'),
                                  var.docker_path.rstrip('/')) for var in mounts
  ])


def get_job_and_task_param(job_params, task_params, field):
  

  return job_params.get(field, set()) | task_params.get(field, set())
","


__all__ = ['get_close_matches', 'ndiff', 'restore', 'SequenceMatcher',
           'Differ','IS_CHARACTER_JUNK', 'IS_LINE_JUNK', 'context_diff',
           'unified_diff', 'HtmlDiff', 'Match']

import heapq


import functools
reduce = functools.reduce

import operator
_itemgetter = operator.itemgetter
_property = property
_tuple = tuple

def setdefault(d, k, default=None):
    if k not in d:
        d[k] = default
    return d[k]


class Match(tuple):
    'Match(a, b, size)'

    __slots__ = ()

    _fields = ('a', 'b', 'size')

    def __new__(_cls, a, b, size):
        'Create new instance of Match(a, b, size)'
        return _tuple.__new__(_cls, (a, b, size))

    
    def _make(cls, iterable, new=tuple.__new__, len=len):
        'Make a new Match object from a sequence or iterable'
        result = new(cls, iterable)
        if len(result) != 3:
            raise TypeError('Expected 3 arguments, got %d' % len(result))
        return result
    _make = classmethod(_make)

    def __repr__(self):
        'Return a nicely formatted representation string'
        return 'Match(a=%r, b=%r, size=%r)' % self

    def _asdict(self):
        'Return a new OrderedDict which maps field names to their values'
        return OrderedDict(zip(self._fields, self))

    def _replace(_self, **kwds):
        'Return a new Match object replacing specified fields with new values'
        result = _self._make(map(kwds.pop, ('a', 'b', 'size'), _self))
        if kwds:
            raise ValueError('Got unexpected field names: %r' % kwds.keys())
        return result

    def __getnewargs__(self):
        'Return self as a plain tuple.  Used by copy and pickle.'
        return tuple(self)

    __dict__ = _property(_asdict)

    def __getstate__(self):
        'Exclude the OrderedDict from pickling'
        pass

    a = _property(_itemgetter(0), doc='Alias for field number 0')

    b = _property(_itemgetter(1), doc='Alias for field number 1')

    size = _property(_itemgetter(2), doc='Alias for field number 2')

def _calculate_ratio(matches, length):
    if length:
        return 2.0 * matches / length
    return 1.0

class SequenceMatcher(object):

    


    def __init__(self, isjunk=None, a='', b='', autojunk=True):
        


        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

        self.isjunk = isjunk
        self.a = self.b = None
        self.autojunk = autojunk
        self.set_seqs(a, b)

    def set_seqs(self, a, b):
        


        self.set_seq1(a)
        self.set_seq2(b)

    def set_seq1(self, a):
        


        if a is self.a:
            return
        self.a = a
        self.matching_blocks = self.opcodes = None

    def set_seq2(self, b):
        


        if b is self.b:
            return
        self.b = b
        self.matching_blocks = self.opcodes = None
        self.fullbcount = None
        self.__chain_b()

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    def __chain_b(self):
        
        
        
        
        
        
        
        
        
        
        b = self.b
        self.b2j = b2j = {}

        for i, elt in enumerate(b):
            indices = setdefault(b2j, elt, [])
            
            indices.append(i)

        
        junk = set()
        isjunk = self.isjunk
        if isjunk:
            for elt in list(b2j.keys()):  
                if isjunk(elt):
                    junk.add(elt)
                    del b2j[elt]

        
        popular = set()
        n = len(b)
        if self.autojunk and n >= 200:
            ntest = n // 100 + 1
            for elt, idxs in list(b2j.items()):
                if len(idxs) > ntest:
                    popular.add(elt)
                    del b2j[elt]

        
        
        
        
        self.isbjunk = junk.__contains__
        self.isbpopular = popular.__contains__

    def find_longest_match(self, alo, ahi, blo, bhi):
        


        
        
        
        
        
        
        
        
        
        
        

        a, b, b2j, isbjunk = self.a, self.b, self.b2j, self.isbjunk
        besti, bestj, bestsize = alo, blo, 0
        
        
        
        j2len = {}
        nothing = []
        for i in xrange(alo, ahi):
            
            
            j2lenget = j2len.get
            newj2len = {}
            for j in b2j.get(a[i], nothing):
                
                if j < blo:
                    continue
                if j >= bhi:
                    break
                k = newj2len[j] = j2lenget(j-1, 0) + 1
                if k > bestsize:
                    besti, bestj, bestsize = i-k+1, j-k+1, k
            j2len = newj2len

        
        
        
        
        while besti > alo and bestj > blo and \
              not isbjunk(b[bestj-1]) and \
              a[besti-1] == b[bestj-1]:
            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1
        while besti+bestsize < ahi and bestj+bestsize < bhi and \
              not isbjunk(b[bestj+bestsize]) and \
              a[besti+bestsize] == b[bestj+bestsize]:
            bestsize += 1

        
        
        
        
        
        
        
        while besti > alo and bestj > blo and \
              isbjunk(b[bestj-1]) and \
              a[besti-1] == b[bestj-1]:
            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1
        while besti+bestsize < ahi and bestj+bestsize < bhi and \
              isbjunk(b[bestj+bestsize]) and \
              a[besti+bestsize] == b[bestj+bestsize]:
            bestsize = bestsize + 1

        return Match(besti, bestj, bestsize)

    def get_matching_blocks(self):
        


        if self.matching_blocks is not None:
            return self.matching_blocks
        la, lb = len(self.a), len(self.b)

        
        
        
        
        
        
        queue = [(0, la, 0, lb)]
        matching_blocks = []
        while queue:
            alo, ahi, blo, bhi = queue.pop()
            i, j, k = x = self.find_longest_match(alo, ahi, blo, bhi)
            
            
            
            if k:   
                matching_blocks.append(x)
                if alo < i and blo < j:
                    queue.append((alo, i, blo, j))
                if i+k < ahi and j+k < bhi:
                    queue.append((i+k, ahi, j+k, bhi))
        matching_blocks.sort()

        
        
        
        i1 = j1 = k1 = 0
        non_adjacent = []
        for i2, j2, k2 in matching_blocks:
            
            if i1 + k1 == i2 and j1 + k1 == j2:
                
                
                
                k1 += k2
            else:
                
                
                
                if k1:
                    non_adjacent.append((i1, j1, k1))
                i1, j1, k1 = i2, j2, k2
        if k1:
            non_adjacent.append((i1, j1, k1))

        non_adjacent.append( (la, lb, 0) )
        self.matching_blocks = map(Match._make, non_adjacent)
        return self.matching_blocks

    def get_opcodes(self):
        


        if self.opcodes is not None:
            return self.opcodes
        i = j = 0
        self.opcodes = answer = []
        for ai, bj, size in self.get_matching_blocks():
            
            
            
            
            
            tag = ''
            if i < ai and j < bj:
                tag = 'replace'
            elif i < ai:
                tag = 'delete'
            elif j < bj:
                tag = 'insert'
            if tag:
                answer.append( (tag, i, ai, j, bj) )
            i, j = ai+size, bj+size
            
            
            if size:
                answer.append( ('equal', ai, i, bj, j) )
        return answer

    def get_grouped_opcodes(self, n=3):
        


        codes = self.get_opcodes()
        if not codes:
            codes = [(""equal"", 0, 1, 0, 1)]
        
        if codes[0][0] == 'equal':
            tag, i1, i2, j1, j2 = codes[0]
            codes[0] = tag, max(i1, i2-n), i2, max(j1, j2-n), j2
        if codes[-1][0] == 'equal':
            tag, i1, i2, j1, j2 = codes[-1]
            codes[-1] = tag, i1, min(i2, i1+n), j1, min(j2, j1+n)

        nn = n + n
        group = []
        for tag, i1, i2, j1, j2 in codes:
            
            
            if tag == 'equal' and i2-i1 > nn:
                group.append((tag, i1, min(i2, i1+n), j1, min(j2, j1+n)))
                yield group
                group = []
                i1, j1 = max(i1, i2-n), max(j1, j2-n)
            group.append((tag, i1, i2, j1 ,j2))
        if group and not (len(group)==1 and group[0][0] == 'equal'):
            yield group

    def ratio(self):
        


        matches = reduce(lambda sum, triple: sum + triple[-1],
                         self.get_matching_blocks(), 0)
        return _calculate_ratio(matches, len(self.a) + len(self.b))

    def quick_ratio(self):
        


        
        
        
        if self.fullbcount is None:
            self.fullbcount = fullbcount = {}
            for elt in self.b:
                fullbcount[elt] = fullbcount.get(elt, 0) + 1
        fullbcount = self.fullbcount
        
        
        avail = {}
        availhas, matches = avail.__contains__, 0
        for elt in self.a:
            if availhas(elt):
                numb = avail[elt]
            else:
                numb = fullbcount.get(elt, 0)
            avail[elt] = numb - 1
            if numb > 0:
                matches = matches + 1
        return _calculate_ratio(matches, len(self.a) + len(self.b))

    def real_quick_ratio(self):
        


        la, lb = len(self.a), len(self.b)
        
        
        return _calculate_ratio(min(la, lb), la + lb)

def get_close_matches(word, possibilities, n=3, cutoff=0.6):
    


    if not n >  0:
        raise ValueError(""n must be > 0: %r"" % (n,))
    if not 0.0 <= cutoff <= 1.0:
        raise ValueError(""cutoff must be in [0.0, 1.0]: %r"" % (cutoff,))
    result = []
    s = SequenceMatcher()
    s.set_seq2(word)
    for x in possibilities:
        s.set_seq1(x)
        if s.real_quick_ratio() >= cutoff and \
           s.quick_ratio() >= cutoff and \
           s.ratio() >= cutoff:
            result.append((s.ratio(), x))

    
    result = heapq.nlargest(n, result)
    
    return [x for score, x in result]

def _count_leading(line, ch):
    


    i, n = 0, len(line)
    while i < n and line[i] == ch:
        i += 1
    return i

class Differ(object):
    r
  1. Beautiful is better than ugly.
    ...   2. Explicit is better than implicit.
    ...   3. Simple is better than complex.
    ...   4. Complex is better than complicated.
    ... 
  1. Beautiful is better than ugly.
    ...   3.   Simple is better than complex.
    ...   4. Complicated is better than complex.
    ...   5. Flat is better than nested.
    ... 


    def __init__(self, linejunk=None, charjunk=None):
        


        self.linejunk = linejunk
        self.charjunk = charjunk

    def compare(self, a, b):
        r


        cruncher = SequenceMatcher(self.linejunk, a, b)
        for tag, alo, ahi, blo, bhi in cruncher.get_opcodes():
            if tag == 'replace':
                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)
            elif tag == 'delete':
                g = self._dump('-', a, alo, ahi)
            elif tag == 'insert':
                g = self._dump('+', b, blo, bhi)
            elif tag == 'equal':
                g = self._dump(' ', a, alo, ahi)
            else:
                raise ValueError, 'unknown tag %r' % (tag,)

            for line in g:
                yield line

    def _dump(self, tag, x, lo, hi):
        

        for i in xrange(lo, hi):
            yield '%s %s' % (tag, x[i])

    def _plain_replace(self, a, alo, ahi, b, blo, bhi):
        assert alo < ahi and blo < bhi
        
        
        if bhi - blo < ahi - alo:
            first  = self._dump('+', b, blo, bhi)
            second = self._dump('-', a, alo, ahi)
        else:
            first  = self._dump('-', a, alo, ahi)
            second = self._dump('+', b, blo, bhi)

        for g in first, second:
            for line in g:
                yield line

    def _fancy_replace(self, a, alo, ahi, b, blo, bhi):
        r


        
        
        best_ratio, cutoff = 0.74, 0.75
        cruncher = SequenceMatcher(self.charjunk)
        eqi, eqj = None, None   

        
        
        
        for j in xrange(blo, bhi):
            bj = b[j]
            cruncher.set_seq2(bj)
            for i in xrange(alo, ahi):
                ai = a[i]
                if ai == bj:
                    if eqi is None:
                        eqi, eqj = i, j
                    continue
                cruncher.set_seq1(ai)
                
                
                
                
                
                
                if cruncher.real_quick_ratio() > best_ratio and \
                      cruncher.quick_ratio() > best_ratio and \
                      cruncher.ratio() > best_ratio:
                    best_ratio, best_i, best_j = cruncher.ratio(), i, j
        if best_ratio < cutoff:
            
            if eqi is None:
                
                for line in self._plain_replace(a, alo, ahi, b, blo, bhi):
                    yield line
                return
            
            best_i, best_j, best_ratio = eqi, eqj, 1.0
        else:
            
            eqi = None

        
        

        
        for line in self._fancy_helper(a, alo, best_i, b, blo, best_j):
            yield line

        
        aelt, belt = a[best_i], b[best_j]
        if eqi is None:
            
            atags = btags = """"
            cruncher.set_seqs(aelt, belt)
            for tag, ai1, ai2, bj1, bj2 in cruncher.get_opcodes():
                la, lb = ai2 - ai1, bj2 - bj1
                if tag == 'replace':
                    atags += '^' * la
                    btags += '^' * lb
                elif tag == 'delete':
                    atags += '-' * la
                elif tag == 'insert':
                    btags += '+' * lb
                elif tag == 'equal':
                    atags += ' ' * la
                    btags += ' ' * lb
                else:
                    raise ValueError, 'unknown tag %r' % (tag,)
            for line in self._qformat(aelt, belt, atags, btags):
                yield line
        else:
            
            yield '  ' + aelt

        
        for line in self._fancy_helper(a, best_i+1, ahi, b, best_j+1, bhi):
            yield line

    def _fancy_helper(self, a, alo, ahi, b, blo, bhi):
        g = []
        if alo < ahi:
            if blo < bhi:
                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)
            else:
                g = self._dump('-', a, alo, ahi)
        elif blo < bhi:
            g = self._dump('+', b, blo, bhi)

        for line in g:
            yield line

    def _qformat(self, aline, bline, atags, btags):
        r


        
        common = min(_count_leading(aline, ""\t""),
                     _count_leading(bline, ""\t""))
        common = min(common, _count_leading(atags[:common], "" ""))
        common = min(common, _count_leading(btags[:common], "" ""))
        atags = atags[common:].rstrip()
        btags = btags[common:].rstrip()

        yield ""- "" + aline
        if atags:
            yield ""? %s%s\n"" % (""\t"" * common, atags)

        yield ""+ "" + bline
        if btags:
            yield ""? %s%s\n"" % (""\t"" * common, btags)


















import re

def IS_LINE_JUNK(line, pat=re.compile(r""\s*
    r


    return pat(line) is not None

def IS_CHARACTER_JUNK(ch, ws="" \t""):
    r


    return ch in ws






def _format_range_unified(start, stop):
    'Convert range to the ""ed"" format'
    
    beginning = start + 1     
    length = stop - start
    if length == 1:
        
        return '%s' % (beginning)
    if not length:
        beginning -= 1        
    return '%s,%s' % (beginning, length)

def unified_diff(a, b, fromfile='', tofile='', fromfiledate='',
                 tofiledate='', n=3, lineterm='\n'):
    r


    started = False
    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):
        if not started:
            started = True
            
            fromdate = '\t%s' % (fromfiledate) if fromfiledate else ''
            
            todate = '\t%s' % (tofiledate) if tofiledate else ''
            
            yield '--- %s%s%s' % (fromfile, fromdate, lineterm)
            
            yield '+++ %s%s%s' % (tofile, todate, lineterm)

        first, last = group[0], group[-1]
        file1_range = _format_range_unified(first[1], last[2])
        file2_range = _format_range_unified(first[3], last[4])
        
        yield '@@ -%s +%s @@%s' % (file1_range, file2_range, lineterm)

        for tag, i1, i2, j1, j2 in group:
            if tag == 'equal':
                for line in a[i1:i2]:
                    yield ' ' + line
                continue
            if tag in ('replace', 'delete'):
                for line in a[i1:i2]:
                    yield '-' + line
            if tag in ('replace', 'insert'):
                for line in b[j1:j2]:
                    yield '+' + line






def _format_range_context(start, stop):
    'Convert range to the ""ed"" format'
    
    beginning = start + 1     
    length = stop - start
    if not length:
        beginning -= 1        
    if length <= 1:
        
        return '%s' % (beginning)
    
    return '%s,%s' % (beginning, beginning + length - 1)


def context_diff(a, b, fromfile='', tofile='',
                 fromfiledate='', tofiledate='', n=3, lineterm='\n'):
    r


    prefix = dict(insert='+ ', delete='- ', replace='! ', equal='  ')
    started = False
    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):
        if not started:
            started = True
            
            fromdate = '\t%s' % (fromfiledate) if fromfiledate else ''
            
            todate = '\t%s' % (tofiledate) if tofiledate else ''
            
            yield '*** %s%s%s' % (fromfile, fromdate, lineterm)
            
            yield '--- %s%s%s' % (tofile, todate, lineterm)

        first, last = group[0], group[-1]
        yield '***************' + lineterm

        file1_range = _format_range_context(first[1], last[2])
        
        yield '*** %s ****%s' % (file1_range, lineterm)

        if any(tag in ('replace', 'delete') for tag, _, _, _, _ in group):
            for tag, i1, i2, _, _ in group:
                if tag != 'insert':
                    for line in a[i1:i2]:
                        yield prefix[tag] + line

        file2_range = _format_range_context(first[3], last[4])
        
        yield '--- %s ----%s' % (file2_range, lineterm)

        if any(tag in ('replace', 'insert') for tag, _, _, _, _ in group):
            for tag, _, _, j1, j2 in group:
                if tag != 'delete':
                    for line in b[j1:j2]:
                        yield prefix[tag] + line

def ndiff(a, b, linejunk=None, charjunk=IS_CHARACTER_JUNK):
    r

    return Differ(linejunk, charjunk).compare(a, b)

def _mdiff(fromlines, tolines, context=None, linejunk=None,
           charjunk=IS_CHARACTER_JUNK):
    r

    import re

    
    change_re = re.compile('(\++|\-+|\^+)')

    
    diff_lines_iterator = ndiff(fromlines,tolines,linejunk,charjunk)

    def _make_line(lines, format_key, side, num_lines=[0,0]):
        

        num_lines[side] += 1
        
        
        if format_key is None:
            return (num_lines[side],lines.pop(0)[2:])
        
        if format_key == '?':
            text, markers = lines.pop(0), lines.pop(0)
            
            sub_info = []
            def record_sub_info(match_object,sub_info=sub_info):
                sub_info.append([match_object.group(1)[0],match_object.span()])
                return match_object.group(1)
            change_re.sub(record_sub_info,markers)
            
            
            for key,(begin,end) in sub_info[::-1]:
                text = text[0:begin]+'\0'+key+text[begin:end]+'\1'+text[end:]
            text = text[2:]
        
        else:
            text = lines.pop(0)[2:]
            
            
            if not text:
                text = ' '
            
            text = '\0' + format_key + text + '\1'
        
        
        
        return (num_lines[side],text)

    def _line_iterator():
        

        lines = []
        num_blanks_pending, num_blanks_to_yield = 0, 0
        while True:
            
            
            
            while len(lines) < 4:
                try:
                    lines.append(diff_lines_iterator.next())
                except StopIteration:
                    lines.append('X')
            s = ''.join([line[0] for line in lines])
            if s.startswith('X'):
                
                
                
                num_blanks_to_yield = num_blanks_pending
            elif s.startswith('-?+?'):
                
                yield _make_line(lines,'?',0), _make_line(lines,'?',1), True
                continue
            elif s.startswith('--++'):
                
                
                num_blanks_pending -= 1
                yield _make_line(lines,'-',0), None, True
                continue
            elif s.startswith(('--?+', '--+', '- ')):
                
                
                from_line,to_line = _make_line(lines,'-',0), None
                num_blanks_to_yield,num_blanks_pending = num_blanks_pending-1,0
            elif s.startswith('-+?'):
                
                yield _make_line(lines,None,0), _make_line(lines,'?',1), True
                continue
            elif s.startswith('-?+'):
                
                yield _make_line(lines,'?',0), _make_line(lines,None,1), True
                continue
            elif s.startswith('-'):
                
                num_blanks_pending -= 1
                yield _make_line(lines,'-',0), None, True
                continue
            elif s.startswith('+--'):
                
                
                num_blanks_pending += 1
                yield None, _make_line(lines,'+',1), True
                continue
            elif s.startswith(('+ ', '+-')):
                
                from_line, to_line = None, _make_line(lines,'+',1)
                num_blanks_to_yield,num_blanks_pending = num_blanks_pending+1,0
            elif s.startswith('+'):
                
                num_blanks_pending += 1
                yield None, _make_line(lines,'+',1), True
                continue
            elif s.startswith(' '):
                
                yield _make_line(lines[:],None,0),_make_line(lines,None,1),False
                continue
            
            
            while(num_blanks_to_yield < 0):
                num_blanks_to_yield += 1
                yield None,('','\n'),True
            while(num_blanks_to_yield > 0):
                num_blanks_to_yield -= 1
                yield ('','\n'),None,True
            if s.startswith('X'):
                raise StopIteration
            else:
                yield from_line,to_line,True

    def _line_pair_iterator():
        

        line_iterator = _line_iterator()
        fromlines,tolines=[],[]
        while True:
            
            while (len(fromlines)==0 or len(tolines)==0):
                from_line, to_line, found_diff =line_iterator.next()
                if from_line is not None:
                    fromlines.append((from_line,found_diff))
                if to_line is not None:
                    tolines.append((to_line,found_diff))
            
            from_line, fromDiff = fromlines.pop(0)
            to_line, to_diff = tolines.pop(0)
            yield (from_line,to_line,fromDiff or to_diff)

    
    
    line_pair_iterator = _line_pair_iterator()
    if context is None:
        while True:
            yield line_pair_iterator.next()
    
    
    else:
        context += 1
        lines_to_write = 0
        while True:
            
            
            
            index, contextLines = 0, [None]*(context)
            found_diff = False
            while(found_diff is False):
                from_line, to_line, found_diff = line_pair_iterator.next()
                i = index % context
                contextLines[i] = (from_line, to_line, found_diff)
                index += 1
            
            
            if index > context:
                yield None, None, None
                lines_to_write = context
            else:
                lines_to_write = index
                index = 0
            while(lines_to_write):
                i = index % context
                index += 1
                yield contextLines[i]
                lines_to_write -= 1
            
            lines_to_write = context-1
            while(lines_to_write):
                from_line, to_line, found_diff = line_pair_iterator.next()
                
                if found_diff:
                    lines_to_write = context-1
                else:
                    lines_to_write -= 1
                yield from_line, to_line, found_diff


_file_template = 


_styles = 


_table_template = 


_legend = 


class HtmlDiff(object):
    


    _file_template = _file_template
    _styles = _styles
    _table_template = _table_template
    _legend = _legend
    _default_prefix = 0

    def __init__(self,tabsize=8,wrapcolumn=None,linejunk=None,
                 charjunk=IS_CHARACTER_JUNK):
        

        self._tabsize = tabsize
        self._wrapcolumn = wrapcolumn
        self._linejunk = linejunk
        self._charjunk = charjunk

    def make_file(self,fromlines,tolines,fromdesc='',todesc='',context=False,
                  numlines=5):
        


        return self._file_template % dict(
            styles = self._styles,
            legend = self._legend,
            table = self.make_table(fromlines,tolines,fromdesc,todesc,
                                    context=context,numlines=numlines))

    def _tab_newline_replace(self,fromlines,tolines):
        

        def expand_tabs(line):
            
            line = line.replace(' ','\0')
            
            line = line.expandtabs(self._tabsize)
            
            
            line = line.replace(' ','\t')
            return line.replace('\0',' ').rstrip('\n')
        fromlines = [expand_tabs(line) for line in fromlines]
        tolines = [expand_tabs(line) for line in tolines]
        return fromlines,tolines

    def _split_line(self,data_list,line_num,text):
        

        
        if not line_num:
            data_list.append((line_num,text))
            return

        
        size = len(text)
        max = self._wrapcolumn
        if (size <= max) or ((size -(text.count('\0')*3)) <= max):
            data_list.append((line_num,text))
            return

        
        
        i = 0
        n = 0
        mark = ''
        while n < max and i < size:
            if text[i] == '\0':
                i += 1
                mark = text[i]
                i += 1
            elif text[i] == '\1':
                i += 1
                mark = ''
            else:
                i += 1
                n += 1

        
        line1 = text[:i]
        line2 = text[i:]

        
        
        
        if mark:
            line1 = line1 + '\1'
            line2 = '\0' + mark + line2

        
        data_list.append((line_num,line1))

        
        self._split_line(data_list,'>',line2)

    def _line_wrapper(self,diffs):
        


        
        for fromdata,todata,flag in diffs:
            
            if flag is None:
                yield fromdata,todata,flag
                continue
            (fromline,fromtext),(toline,totext) = fromdata,todata
            
            
            fromlist,tolist = [],[]
            self._split_line(fromlist,fromline,fromtext)
            self._split_line(tolist,toline,totext)
            
            
            while fromlist or tolist:
                if fromlist:
                    fromdata = fromlist.pop(0)
                else:
                    fromdata = ('',' ')
                if tolist:
                    todata = tolist.pop(0)
                else:
                    todata = ('',' ')
                yield fromdata,todata,flag

    def _collect_lines(self,diffs):
        


        fromlist,tolist,flaglist = [],[],[]
        
        for fromdata,todata,flag in diffs:
            try:
                
                fromlist.append(self._format_line(0,flag,*fromdata))
                tolist.append(self._format_line(1,flag,*todata))
            except TypeError:
                
                fromlist.append(None)
                tolist.append(None)
            flaglist.append(flag)
        return fromlist,tolist,flaglist

    def _format_line(self,side,flag,linenum,text):
        

        try:
            linenum = '%d' % linenum
            id = ' id=""%s%s""' % (self._prefix[side],linenum)
        except TypeError:
            
            id = ''
        
        text=text.replace(""&"",""&amp;"").replace("">"",""&gt;"").replace(""<"",""&lt;"")

        
        text = text.replace(' ','&nbsp;').rstrip()

        return '<td class=""diff_header""%s>%s</td><td nowrap=""nowrap"">%s</td>' \
               % (id,linenum,text)

    def _make_prefix(self):
        


        
        
        fromprefix = ""from%d_"" % HtmlDiff._default_prefix
        toprefix = ""to%d_"" % HtmlDiff._default_prefix
        HtmlDiff._default_prefix += 1
        
        self._prefix = [fromprefix,toprefix]

    def _convert_flags(self,fromlist,tolist,flaglist,context,numlines):
        


        
        toprefix = self._prefix[1]

        
        next_id = ['']*len(flaglist)
        next_href = ['']*len(flaglist)
        num_chg, in_change = 0, False
        last = 0
        for i,flag in enumerate(flaglist):
            if flag:
                if not in_change:
                    in_change = True
                    last = i
                    
                    
                    
                    i = max([0,i-numlines])
                    next_id[i] = ' id=""difflib_chg_%s_%d""' % (toprefix,num_chg)
                    
                    
                    num_chg += 1
                    next_href[last] = '<a href=""
                         toprefix,num_chg)
            else:
                in_change = False
        
        if not flaglist:
            flaglist = [False]
            next_id = ['']
            next_href = ['']
            last = 0
            if context:
                fromlist = ['<td></td><td>&nbsp;No Differences Found&nbsp;</td>']
                tolist = fromlist
            else:
                fromlist = tolist = ['<td></td><td>&nbsp;Empty File&nbsp;</td>']
        
        if not flaglist[0]:
            next_href[0] = '<a href=""
        
        next_href[last] = '<a href=""

        return fromlist,tolist,flaglist,next_href,next_id

    def make_table(self,fromlines,tolines,fromdesc='',todesc='',context=False,
                   numlines=5):
        


        
        
        self._make_prefix()

        
        
        fromlines,tolines = self._tab_newline_replace(fromlines,tolines)

        
        if context:
            context_lines = numlines
        else:
            context_lines = None
        diffs = _mdiff(fromlines,tolines,context_lines,linejunk=self._linejunk,
                      charjunk=self._charjunk)

        
        if self._wrapcolumn:
            diffs = self._line_wrapper(diffs)

        
        fromlist,tolist,flaglist = self._collect_lines(diffs)

        
        fromlist,tolist,flaglist,next_href,next_id = self._convert_flags(
            fromlist,tolist,flaglist,context,numlines)

        s = []
        fmt = '            <tr><td class=""diff_next""%s>%s</td>%s' + \
              '<td class=""diff_next"">%s</td>%s</tr>\n'
        for i in range(len(flaglist)):
            if flaglist[i] is None:
                
                
                if i > 0:
                    s.append('        </tbody>        \n        <tbody>\n')
            else:
                s.append( fmt % (next_id[i],next_href[i],fromlist[i],
                                           next_href[i],tolist[i]))
        if fromdesc or todesc:
            header_row = '<thead><tr>%s%s%s%s</tr></thead>' % (
                '<th class=""diff_next""><br /></th>',
                '<th colspan=""2"" class=""diff_header"">%s</th>' % fromdesc,
                '<th class=""diff_next""><br /></th>',
                '<th colspan=""2"" class=""diff_header"">%s</th>' % todesc)
        else:
            header_row = ''

        table = self._table_template % dict(
            data_rows=''.join(s),
            header_row=header_row,
            prefix=self._prefix[1])

        return table.replace('\0+','<span class=""diff_add"">'). \
                     replace('\0-','<span class=""diff_sub"">'). \
                     replace('\0^','<span class=""diff_chg"">'). \
                     replace('\1','</span>'). \
                     replace('\t','&nbsp;')

del re

def restore(delta, which):
    r

    try:
        tag = {1: ""- "", 2: ""+ ""}[int(which)]
    except KeyError:
        raise ValueError, ('unknown delta choice (must be 1 or 2): %r'
                           % which)
    prefixes = (""  "", tag)
    for line in delta:
        if line[:2] in prefixes:
            yield line[2:]







","import functools, queue, traceback
from .. util import log


class EditQueue(queue.Queue):
    


    def put_edit(self, f, *args, **kwds):
        

        self.put_nowait(functools.partial(f, *args, **kwds))

    def get_and_run_edits(self):
        

        if self.empty():
            return

        edits = []
        while True:
            try:
                edits.append(self.get_nowait())
            except queue.Empty:
                break

        for e in edits:
            try:
                e()
            except:
                log.error('Error on edit %s', e)
                traceback.print_exc()
","import asyncio
import json
import logging

import websockets

from .Events import TrackStuckEvent, TrackExceptionEvent, TrackEndEvent, StatsUpdateEvent

log = logging.getLogger(__name__)


class WebSocket:
    def __init__(self, lavalink, host, password, ws_port, ws_retry, shard_count):
        self._lavalink = lavalink

        self._ws = None
        self._queue = []

        self._ws_retry = ws_retry
        self._password = password
        self._host = host
        self._port = ws_port
        self._uri = 'ws://{}:{}'.format(self._host, self._port)
        self._shards = shard_count

        self._shutdown = False

        self._loop = self._lavalink.loop
        self._loop.create_task(self.connect())

    @property
    def connected(self):
        

        return self._ws is not None and self._ws.open

    async def connect(self):
        

        await self._lavalink.bot.wait_until_ready()

        if self._ws and self._ws.open:
            log.debug('WebSocket still open, closing...')
            await self._ws.close()

        user_id = self._lavalink.bot.user.id
        shard_count = self._lavalink.bot.shard_count or self._shards

        headers = {
            'Authorization': self._password,
            'Num-Shards': shard_count,
            'User-Id': str(user_id)
        }
        log.debug('Preparing to connect to Lavalink')
        log.debug('    with URI: {}'.format(self._uri))
        log.debug('    with headers: {}'.format(str(headers)))
        log.info('Connecting to Lavalink...')

        try:
            self._ws = await websockets.connect(self._uri, loop=self._loop, extra_headers=headers)
        except OSError as error:
            log.exception('Failed to connect to Lavalink: {}'.format(str(error)))
        else:
            log.info('Connected to Lavalink!')
            self._loop.create_task(self.listen())
            version = self._ws.response_headers.get('Lavalink-Major-Version', 2)
            try:
                self._lavalink._server_version = int(version)
            except ValueError:
                self._lavalink._server_version = 2
            log.info('Lavalink server version is {}'.format(version))
            if self._queue:
                log.info('Replaying {} queued events...'.format(len(self._queue)))
                for task in self._queue:
                    await self.send(**task)

    async def _attempt_reconnect(self):
        

        log.info('Connection closed; attempting to reconnect in 30 seconds')
        for a in range(0, self._ws_retry):
            await asyncio.sleep(30)
            log.info('Reconnecting... (Attempt {})'.format(a + 1))
            await self.connect()

            if self._ws.open:
                return True
        return False

    async def listen(self):
        

        while not self._shutdown:
            try:
                data = json.loads(await self._ws.recv())
            except websockets.ConnectionClosed as error:
                log.warning('Disconnected from Lavalink: {}'.format(str(error)))
                for g in self._lavalink.players._players.copy().keys():
                    ws = self._lavalink.bot._connection._get_websocket(int(g))
                    await ws.voice_state(int(g), None)

                self._lavalink.players.clear()

                if self._shutdown:
                    break

                if await self._attempt_reconnect():
                    return

                log.warning('Unable to reconnect to Lavalink!')
                break

            op = data.get('op', None)
            log.debug('Received WebSocket data {}'.format(str(data)))

            if not op:
                return log.debug('Received WebSocket message without op {}'.format(str(data)))

            if op == 'event':
                log.debug('Received event of type {}'.format(data['type']))
                player = self._lavalink.players[int(data['guildId'])]
                event = None

                if data['type'] == 'TrackEndEvent':
                    event = TrackEndEvent(player, data['track'], data['reason'])
                elif data['type'] == 'TrackExceptionEvent':
                    event = TrackExceptionEvent(player, data['track'], data['error'])
                elif data['type'] == 'TrackStuckEvent':
                    event = TrackStuckEvent(player, data['track'], data['thresholdMs'])

                if event:
                    await self._lavalink.dispatch_event(event)
            elif op == 'playerUpdate':
                await self._lavalink.update_state(data)
            elif op == 'stats':
                self._lavalink.stats._update(data)
                await self._lavalink.dispatch_event(StatsUpdateEvent(self._lavalink.stats))

        log.debug('Closing WebSocket...')
        await self._ws.close()

    async def send(self, **data):
        

        if self._ws and self._ws.open:
            log.debug('Sending payload {}'.format(str(data)))
            await self._ws.send(json.dumps(data))
        else:
            self._queue.append(data)
            log.debug('Send called before WebSocket ready; queueing payload {}'.format(str(data)))

    def destroy(self):
        self._shutdown = True
","



from __future__ import division

__all__ = ['CRC_Permittivity_data', 'permittivity_IAPWS', 'Permittivity']

import os
import numpy as np
import pandas as pd
from thermo.utils import N_A, epsilon_0, k
from thermo.utils import TDependentProperty

folder = os.path.join(os.path.dirname(__file__), 'Electrolytes')


CRC_Permittivity_data = pd.read_csv(os.path.join(folder, 'Permittivity (Dielectric Constant) of Liquids.tsv'),
                                    sep='\t', index_col=0)
_CRC_Permittivity_data_values = CRC_Permittivity_data.values


def permittivity_IAPWS(T, rho):
    r

    dipole = 6.138E-30 
    polarizability = 1.636E-40 
    MW = 0.018015268 
    ih = [1, 1, 1, 2, 3, 3, 4, 5, 6, 7, 10]
    jh = [0.25, 1, 2.5, 1.5, 1.5, 2.5, 2, 2, 5, 0.5, 10]
    Nh = [0.978224486826, -0.957771379375, 0.237511794148, 0.714692244396,
          -0.298217036956, -0.108863472196, 0.949327488264E-1, 
          -.980469816509E-2, 0.165167634970E-4, 0.937359795772E-4, 
          -0.12317921872E-9]
    
    delta = rho/322.
    tau = 647.096/T
    
    g = (1 + sum([Nh[h]*delta**ih[h]*tau**jh[h] for h in range(11)])
        + 0.196096504426E-2*delta*(T/228. - 1)**-1.2)
    
    A = N_A*dipole**2*(rho/MW)*g/epsilon_0/k/T
    B = N_A*polarizability*(rho/MW)/3./epsilon_0
    epsilon = (1. + A + 5.*B + (9. + 2.*A + 18.*B + A**2 + 10.*A*B + 9.*B**2
        )**0.5)/(4. - 4.*B)
    return epsilon


CRC = 'CRC'
CRC_CONSTANT = 'CRC_CONSTANT'
permittivity_methods = [CRC, CRC_CONSTANT]




class Permittivity(TDependentProperty):
    r

    name = 'relative permittivity'
    units = '-'
    interpolation_T = None
    

    interpolation_property = None
    

    interpolation_property_inv = None
    

    tabular_extrapolation_permitted = True
    

    property_min = 1
    

    property_max = 1000
    


    ranked_methods = [CRC, CRC_CONSTANT]
    


    def __init__(self, CASRN=''):
        self.CASRN = CASRN

        self.Tmin = None
        

        self.Tmax = None
        


        self.tabular_data = {}
        

        self.tabular_data_interpolators = {}
        


        self.sorted_valid_methods = []
        

        self.user_methods = []
        


        self.all_methods = set()
        


        self.load_all_methods()

    def load_all_methods(self):
        r

        methods = []
        Tmins, Tmaxs = [], []
        if self.CASRN in CRC_Permittivity_data.index:
            methods.append(CRC_CONSTANT)
            _, self.CRC_CONSTANT_T, self.CRC_permittivity, A, B, C, D, Tmin, Tmax = _CRC_Permittivity_data_values[CRC_Permittivity_data.index.get_loc(self.CASRN)].tolist()
            self.CRC_Tmin = Tmin
            self.CRC_Tmax = Tmax
            self.CRC_coeffs = [0 if np.isnan(x) else x for x in [A, B, C, D] ]
            if not np.isnan(Tmin):
                Tmins.append(Tmin); Tmaxs.append(Tmax)
            if self.CRC_coeffs[0]:
                methods.append(CRC)
        self.all_methods = set(methods)
        if Tmins and Tmaxs:
            self.Tmin = min(Tmins)
            self.Tmax = max(Tmaxs)

    def calculate(self, T, method):
        r

        if method == CRC:
            A, B, C, D = self.CRC_coeffs
            epsilon = A + B*T + C*T**2 + D*T**3
        elif method == CRC_CONSTANT:
            epsilon = self.CRC_permittivity
        elif method in self.tabular_data:
            epsilon = self.interpolate(T, method)
        return epsilon

    def test_method_validity(self, T, method):
        r

        validity = True
        if method == CRC:
            if T < self.CRC_Tmin or T > self.CRC_Tmax:
                validity = False
        elif method == CRC_CONSTANT:
            
            if T < self.CRC_CONSTANT_T - 20 or T > self.CRC_CONSTANT_T + 20:
                validity = False
        elif method in self.tabular_data:
            
            if not self.tabular_extrapolation_permitted:
                Ts, properties = self.tabular_data[method]
                if T < Ts[0] or T > Ts[-1]:
                    validity = False
        else:
            raise Exception('Method not valid')
        return validity






















","














import tensorflow as tf
from tensorforce import util
import tensorforce.core.optimizers.solvers


class Solver(object):
    


    def __init__(self):
        

        
        self.solve = tf.make_template(name_='solver', func_=self.tf_solve)

    def tf_solve(self, fn_x, *args):
        

        raise NotImplementedError

    @staticmethod
    def from_config(config, kwargs=None):
        

        return util.get_object(
            obj=config,
            predefined=tensorforce.core.optimizers.solvers.solvers,
            kwargs=kwargs
        )
","

from __future__ import print_function

import argparse
import os
import re
import subprocess
import sys
if sys.version_info.major == 3:
    from builtins import object
from collections import OrderedDict

from .optionsfile_parser import OptionsFileParser
from .version import __version__


DEFAULT_OPTIONS = {
    ""date_format"": ""%Y-%m-%d"",
    ""exclude_labels"": [],
    ""git_remote"": ""origin"",
    ""github_api"": ""api.github.com"",
    ""github_site"": ""github.com"",
    ""header"": ""
    ""issue_prefix"": ""**Closed issues:**"",
    ""max_issues"": sys.maxsize,
    ""max_simultaneous_requests"": 10,
    ""merge_prefix"": ""**Merged pull requests:**"",
    ""options_file"": "".pygcgen"",
    ""output"": ""CHANGELOG.md"",
    ""unreleased_label"": ""Unreleased"",
}


class DebugHelp(argparse.Action):
    
    

    def __call__(self, parser, namespace, values, option_string=None):
        import platform
        print(self.help)
        print(""\n{} v{}"".format(parser.prog, __version__))
        print(""Python"", sys.version)
        print(platform.platform())
        exit()


class OptionsParser(object):
    def __init__(self, options=None):
        self.options = self.parse_options(options)

    def parse_options(self, options):
        parser = argparse.ArgumentParser(
            description='Fully automate changelog generation.',
        )

        parser.add_argument(
            ""-u"", ""--user"",
            help=""Username of the owner of target GitHub repo""
        )
        parser.add_argument(
            ""-p"", ""--project"",
            help=""Name of project on GitHub""
        )
        parser.add_argument(
            ""-t"", ""--token"",
            help=""To make more than 50 requests per hour your GitHub token ""
                 ""is required. You can generate it at: ""
                 ""https://github.com/settings/tokens/new""
        )
        parser.add_argument(
            ""--options-file"", metavar=""FILE"",
            default=DEFAULT_OPTIONS[""options_file""],
            help=""Read options from file. Those will overwrite the ones from ""
                 ""the command line.""
        )
        parser.add_argument(
            ""-f"", ""--date-format"",
            default=DEFAULT_OPTIONS[""date_format""],
            help=""The date format to use in changelog. Default is: %%Y-%%m-%%d""
        )
        parser.add_argument(
            ""-o"", ""--output"", metavar=""FILE"",
            default=DEFAULT_OPTIONS[""output""],
            help=""Output file. Default is CHANGELOG.md""
        )
        parser.add_argument(
            ""--no-overwrite"", action='store_true',
            help=""Don't overwrite the output file if it exists ""
                 ""(add a number instead).""
        )
        parser.add_argument(
            ""-b"", ""--base"", metavar=""FILE"",
            help=""Optional base file to append to generated changelog.""
        )
        parser.add_argument(
            ""-s"", ""--section"", action=""append"", nargs=""*"",
            metavar=('PREFIX', 'LABEL'),
            help=""Add a new section to the changelog with the prefix ""
                 ""'PREFIX'. All issues that match one of the LABEL's ""
                 ""will be listed in this section.""
        )
        lst = [
            [""--header-label"", ""header"", ""Setup custom header label.""],
            [""--issues-label"", ""issue_prefix"",
             ""Setup custom label for closed-issues section.""],
            [""--pr-label"", ""merge_prefix"",
             ""Setup custom label for pull requests section.""]
        ]
        for opt, dest, hlp in lst:
            parser.add_argument(
                opt, dest=dest,
                default=DEFAULT_OPTIONS[dest],
                help=""{} Default is: {}"".format(hlp, DEFAULT_OPTIONS[dest])
            )
        parser.add_argument(
            ""--front-matter"", metavar=""JSON"", dest=""frontmatter"",
            help=""Add YAML front matter. Formatted as JSON because it's ""
                 ""easier to add on the command line.""
        )
        parser.add_argument(
            ""--no-issues"", action=""store_false"", dest='issues',
            help=""Don't include closed issues in changelog.""
        )
        parser.add_argument(
            ""--no-issues-wo-labels"",
            action=""store_true"", dest=""add_issues_wo_labels"",
            help=""Don't include closed issues without labels in changelog.""
        )
        parser.add_argument(
            ""--no-pr-wo-labels"",
            action=""store_false"", dest='add_pr_wo_labels',
            help=""Don't include pull requests without labels in changelog.""
        )
        parser.add_argument(
            ""--no-pull-requests"",
            action=""store_false"", dest='include_pull_request',
            help=""Don't include pull-requests in changelog.""
        )
        parser.add_argument(
            ""--no-filter-by-milestone"",
            action=""store_false"", dest=""filter_issues_by_milestone"",
            help=""Don't use milestone to detect when issue was resolved.""
        )
        parser.add_argument(
            ""--no-author"",
            action=""store_false"", dest=""author"",
            help=""Don't add author of pull-request in the end.""
        )
        parser.add_argument(
            ""--author-link-as-tag"",
            action='store_true', dest=""username_as_tag"",
            help=""Use GitHub tags instead of Markdown links for the ""
                 ""author of an issue or pull-request.""
        )
        parser.add_argument(
            ""--with-unreleased"",
            action='store_true', dest=""with_unreleased"",
            help=""Include unreleased closed issues in log.""
        )
        parser.add_argument(
            ""--unreleased-only"",
            action='store_true', dest=""unreleased_only"",
            help=""Generate log from unreleased closed issues only.""
        )
        parser.add_argument(
            ""--unreleased-label"",
            default=DEFAULT_OPTIONS[""unreleased_label""],
            help=""Label for unreleased closed issues. ""
                 ""Default is: {0}"".format(DEFAULT_OPTIONS[""unreleased_label""])
        )
        parser.add_argument(
            ""--unreleased-with-date"", action='store_true',
            help=""Add actual date to unreleased label.""
        )
        parser.add_argument(
            ""--no-compare-link"", action='store_false', dest=""compare_link"",
            help=""Don't include compare link (Full Changelog) between older ""
                 ""version and newer version.""
        )
        parser.add_argument(
            ""--include-labels"", metavar=""LABEL"", nargs='*',
            help=""Only issues with the specified labels will be ""
                 ""included in the changelog.""
        )
        parser.add_argument(
            ""--exclude-labels"", metavar=""LABEL"",
            nargs='*', default=DEFAULT_OPTIONS[""exclude_labels""],
            help=""Issues with the specified labels will always be ""
                 ""excluded from changelog. ""
                 ""Default labels: {0}"".format(
                DEFAULT_OPTIONS[""exclude_labels""]
            )
        )
        parser.add_argument(
            ""--tag-separator"",  metavar=""SEPARATOR"",
            help=""The SEPARATOR will be inserted in the log between tags.""
        )
        parser.add_argument(
            ""--between-tags"",  metavar=""TAG"", nargs='*',
            help=""Changelog will be filled only between specified tags.""
        )
        parser.add_argument(
            ""--exclude-tags"", metavar=""TAG"", nargs='*',
            help=""Change log will exclude specified tags.""
        )
        parser.add_argument(
            ""--exclude-tags-regex"",
            help='Apply a regular expression on tag names so that they can be '
            'excluded, for example: --exclude-tags-regex "".*\+\d{1,}""'
        )
        parser.add_argument(
            ""--since-tag"", metavar=""TAG"",
            help=""Change log will start after specified tag.""
        )
        parser.add_argument(
            ""--due-tag"", metavar=""TAG"",
            help=""Change log will end before specified tag.""
        )
        parser.add_argument(
            ""--max-issues"", metavar=""NUMBER"",
            type=int, default=DEFAULT_OPTIONS[""max_issues""],
            help=""Max number of issues to fetch from GitHub. ""
                 ""Default is unlimited.""
        )
        parser.add_argument(
            ""--release-url"", metavar=""URL"",
            help=""The URL to point to for release links, in printf format ""
                 ""(with the tag as variable).""
        )
        parser.add_argument(
            ""--github-api"", metavar=""URL"",
            dest=""github_endpoint"", default=DEFAULT_OPTIONS[""github_api""],
            help=""The enterprise endpoint to use for your Github API.""
        )
        parser.add_argument(
            ""--github-site"", metavar=""URL"",
            dest=""github_site"", default=DEFAULT_OPTIONS[""github_site""],
            help=""The Enterprise Github site on which your project is hosted.""
        )
        parser.add_argument(
            ""--simple-list"", action='store_true',
            help=""Create simple list from issues and pull requests. ""
        )
        parser.add_argument(
            ""--future-release"", metavar=""RELEASE_VERSION"",
            help=""Put the unreleased changes in the specified release number.""
        )
        parser.add_argument(
            ""--release-branch"",
            help=""Limit pull requests to the release branch, ""
                 ""such as master or release.""
        )
        parser.add_argument(
            ""--origin"", dest=""git_remote"",
            default=DEFAULT_OPTIONS[""git_remote""],
            help=""If you named the origin of your repo other than origin.""
        )
        parser.add_argument(
            ""-v"", ""--verbose"", action='count', default=False,
            help=""Run verbosely.""
        )
        parser.add_argument(
            ""-q"", ""--quiet"", action='store_true',
            help=""Don't output progress information.""
        )
        parser.add_argument(
            ""--version"",
            action='version',
            version=""%(prog)s v{0}"".format(__version__),
            help=""Print version number""
        )
        parser.add_argument(
            ""--support"", action=DebugHelp, nargs=""?"",
            help=""If you have an issue with pygcgen, got to ""
                 ""https://github.com/topic2k/pygcgen/issues and add a ""
                 ""new issue. \nAdd this information to your issue discription.""
        )
        parser.add_argument(
            ""--max-simultaneous-requests"", metavar=""NUMBER"",
            type=int, default=DEFAULT_OPTIONS[""max_simultaneous_requests""],
            help=
            ""Max number of events to fetch simultaneous from GitHub. ""
            ""Default is %d."" % DEFAULT_OPTIONS[""max_simultaneous_requests""]
        )

        opts = parser.parse_args(options)

        if os.path.exists(opts.options_file):
            OptionsFileParser(options=opts).parse()
        if not opts.user or not opts.project:
            self.fetch_user_and_project(opts)

        sections = OrderedDict()
        if opts.section:
            for s in opts.section:
                labels = []
                for l in s[1:]:
                    
                    
                    if l:
                        labels.append(l)
                sections.update({s[0]: labels})

        opts.sections = sections
        del opts.section

        return opts

    def fetch_user_and_project(self, options):
        user, project = self.user_and_project_from_git(options)
        if not options.user:
            options.user = user
        if not options.project:
            options.project = project

    def user_and_project_from_git(self, options, arg0=None, arg1=None):
        

        user, project = self.user_project_from_option(options, arg0, arg1)
        if user and project:
            return user, project

        try:
            remote = subprocess.check_output(
                [
                    'git', 'config', '--get',
                    'remote.{0}.url'.format(options.git_remote)
                ]
            )
        except subprocess.CalledProcessError:
            return None, None
        except WindowsError:
            print(""git binary not found."")
            exit(1)
        else:
            return self.user_project_from_remote(remote)

    @staticmethod
    def user_project_from_option(options, arg0, arg1):
        


        site = options.github_site
        if arg0 and not arg1:
            
            
            
            
            
            match = re.match(
                ""(?:.+{site}/)?(.+)/(.+)"".format(site=site),
                arg0
            )
            if not match:
                print(""Can't detect user and name from first ""
                      ""parameter: '{arg0}' -> exit'"".format(arg0=arg0))
                exit(1)
            return match.groups()
        return None, None

    @staticmethod
    def user_project_from_remote(remote):
        


        
        
        
        regex1 = br"".*(?:[:/])(?P<user>(-|\w|\.)*)/"" \
                 br""(?P<project>(-|\w|\.)*)(\.git).*""
        match = re.match(regex1, remote)
        if match:
            return match.group(""user""), match.group(""project"")

        
        
        
        regex2 = r"".*/((?:-|\w|\.)*)/((?:-|\w|\.)*).*""
        match = re.match(regex2, remote)
        if match:
            return match.group(""user""), match.group(""project"")

        return None, None
","




















import numpy
import os

try:
  import capnp
except ImportError:
  capnp = None

from nupic.bindings.regions.PyRegion import PyRegion

from nupic.algorithms import (anomaly, backtracking_tm, backtracking_tm_cpp,
                              backtracking_tm_shim)
if capnp:
  from nupic.regions.tm_region_capnp import TMRegionProto

from nupic.support import getArgumentDescriptions



gDefaultTemporalImp = 'py'



def _getTPClass(temporalImp):
  


  if temporalImp == 'py':
    return backtracking_tm.BacktrackingTM
  elif temporalImp == 'cpp':
    return backtracking_tm_cpp.BacktrackingTMCPP
  elif temporalImp == 'tm_py':
    return backtracking_tm_shim.TMShim
  elif temporalImp == 'tm_cpp':
    return backtracking_tm_shim.TMCPPShim
  elif temporalImp == 'monitored_tm_py':
    return backtracking_tm_shim.MonitoredTMShim
  else:
    raise RuntimeError(""Invalid temporalImp '%s'. Legal values are: 'py', ""
              ""'cpp', 'tm_py', 'monitored_tm_py'"" % (temporalImp))



def _buildArgs(f, self=None, kwargs={}):
  

  
  argTuples = getArgumentDescriptions(f)
  argTuples = argTuples[1:]  

  
  
  
  
  
  init = TMRegion.__init__
  ourArgNames = [t[0] for t in getArgumentDescriptions(init)]
  
  
  ourArgNames += [
    'numberOfCols',    
  ]
  for argTuple in argTuples[:]:
    if argTuple[0] in ourArgNames:
      argTuples.remove(argTuple)

  
  if self:
    for argTuple in argTuples:
      argName = argTuple[0]
      if argName in kwargs:
        
        argValue = kwargs.pop(argName)
      else:
        
        
        if len(argTuple) == 2:
          
          raise TypeError(""Must provide '%s'"" % argName)
        argValue = argTuple[2]
      
      setattr(self, argName, argValue)

  return argTuples


def _getAdditionalSpecs(temporalImp, kwargs={}):
  

  typeNames = {int: 'UInt32', float: 'Real32', str: 'Byte', bool: 'bool', tuple: 'tuple'}

  def getArgType(arg):
    t = typeNames.get(type(arg), 'Byte')
    count = 0 if t == 'Byte' else 1
    if t == 'tuple':
      t = typeNames.get(type(arg[0]), 'Byte')
      count = len(arg)
    if t == 'bool':
      t = 'UInt32'
    return (t, count)

  def getConstraints(arg):
    t = typeNames.get(type(arg), 'Byte')
    if t == 'Byte':
      return 'multiple'
    elif t == 'bool':
      return 'bool'
    else:
      return ''

  
  TemporalClass = _getTPClass(temporalImp)
  tArgTuples = _buildArgs(TemporalClass.__init__)
  temporalSpec = {}
  for argTuple in tArgTuples:
    d = dict(
      description=argTuple[1],
      accessMode='ReadWrite',
      dataType=getArgType(argTuple[2])[0],
      count=getArgType(argTuple[2])[1],
      constraints=getConstraints(argTuple[2]))
    temporalSpec[argTuple[0]] = d

  
  temporalSpec.update(dict(
    columnCount=dict(
      description='Total number of columns.',
      accessMode='Read',
      dataType='UInt32',
      count=1,
      constraints=''),

    cellsPerColumn=dict(
      description='Number of cells per column.',
      accessMode='Read',
      dataType='UInt32',
      count=1,
      constraints=''),

    inputWidth=dict(
      description='Number of inputs to the TM.',
      accessMode='Read',
      dataType='UInt32',
      count=1,
      constraints=''),

    predictedSegmentDecrement=dict(
      description='Predicted segment decrement',
      accessMode='Read',
      dataType='Real',
      count=1,
      constraints=''),

    orColumnOutputs=dict(
      description=
,
      accessMode='Read',
      dataType='Bool',
      count=1,
      constraints='bool'),

    cellsSavePath=dict(
      description=
,
      accessMode='ReadWrite',
      dataType='Byte',
      count=0,
      constraints=''),

    temporalImp=dict(
      description=
,
      accessMode='ReadWrite',
      dataType='Byte',
      count=0,
      constraints='enum: py, cpp'),

  ))

  
  otherSpec = dict(
    learningMode=dict(
      description='True if the node is learning (default True).',
      accessMode='ReadWrite',
      dataType='Bool',
      count=1,
      defaultValue=True,
      constraints='bool'),

    inferenceMode=dict(
      description='True if the node is inferring (default False).',
      accessMode='ReadWrite',
      dataType='Bool',
      count=1,
      defaultValue=False,
      constraints='bool'),

    computePredictedActiveCellIndices=dict(
      description='True if active and predicted active indices should be computed',
      accessMode='Create',
      dataType='Bool',
      count=1,
      defaultValue=False,
      constraints='bool'),

    anomalyMode=dict(
      description='True if an anomaly score is being computed',
      accessMode='Create',
      dataType='Bool',
      count=1,
      defaultValue=False,
      constraints='bool'),

    topDownMode=dict(
      description='True if the node should do top down compute on the next call '
                  'to compute into topDownOut (default False).',
      accessMode='ReadWrite',
      dataType='Bool',
      count=1,
      defaultValue=False,
      constraints='bool'),

    activeOutputCount=dict(
      description='Number of active elements in bottomUpOut output.',
      accessMode='Read',
      dataType='UInt32',
      count=1,
      constraints=''),

    storeDenseOutput=dict(
      description=
,
      accessMode='ReadWrite',
      dataType='UInt32',
      count=1,
      constraints='bool'),

    logPathOutput=dict(
      description='Optional name of output log file. If set, every output vector'
                  ' will be logged to this file as a sparse vector.',
      accessMode='ReadWrite',
      dataType='Byte',
      count=0,
      constraints=''),

  )

  return temporalSpec, otherSpec



class TMRegion(PyRegion):

  


  def __init__(self,

               columnCount,   
               inputWidth,    
               cellsPerColumn, 

               
               
               

               orColumnOutputs=False,
               cellsSavePath='',
               temporalImp=gDefaultTemporalImp,
               anomalyMode=False,
               computePredictedActiveCellIndices=False,

               **kwargs):
    
    TemporalClass = _getTPClass(temporalImp)

    
    
    
    tArgTuples = _buildArgs(TemporalClass.__init__, self, kwargs)

    self._temporalArgNames = [t[0] for t in tArgTuples]

    self.learningMode   = True      
    self.inferenceMode  = False
    self.anomalyMode    = anomalyMode
    self.computePredictedActiveCellIndices = computePredictedActiveCellIndices
    self.topDownMode    = False
    self.columnCount    = columnCount
    self.inputWidth     = inputWidth
    self.outputWidth    = columnCount * cellsPerColumn
    self.cellsPerColumn = cellsPerColumn

    PyRegion.__init__(self, **kwargs)

    
    
    self._loaded = False
    self._initialize()

    
    self.breakPdb = False
    self.breakKomodo = False

    
    self.orColumnOutputs = orColumnOutputs
    self.temporalImp = temporalImp

    
    self.storeDenseOutput = False
    self.logPathOutput = ''
    self.cellsSavePath = cellsSavePath
    self._fpLogTPOutput = None

    
    self._tfdr = None  


  
  
  
  
  


  def _initialize(self):
    


    for attrName in self._getEphemeralMembersBase():
      if attrName != ""_loaded"":
        if hasattr(self, attrName):
          if self._loaded:
            
            
            
            pass
          else:
            print self.__class__.__name__, ""contains base class member '%s'"" % \
                attrName
    if not self._loaded:
      for attrName in self._getEphemeralMembersBase():
        if attrName != ""_loaded"":
          
          
          assert not hasattr(self, attrName)
        else:
          assert hasattr(self, attrName)

    
    self._profileObj = None
    self._iterations = 0

    
    self._initEphemerals()
    self._checkEphemeralMembers()


  def initialize(self):
    

    
    
    autoArgs = dict((name, getattr(self, name))
                    for name in self._temporalArgNames)

    if self._tfdr is None:
      tpClass = _getTPClass(self.temporalImp)

      if self.temporalImp in ['py', 'cpp', 'r',
                              'tm_py', 'tm_cpp',
                              'monitored_tm_py',]:
        self._tfdr = tpClass(
             numberOfCols=self.columnCount,
             cellsPerColumn=self.cellsPerColumn,
             **autoArgs)
      else:
        raise RuntimeError(""Invalid temporalImp"")


  
  
  
  
  


  
  def compute(self, inputs, outputs):
    


    
    

    
    
    
    
    if False and self.learningMode \
        and self._iterations > 0 and self._iterations <= 10:

      import hotshot
      if self._iterations == 10:
        print ""\n  Collecting and sorting internal node profiling stats generated by hotshot...""
        stats = hotshot.stats.load(""hotshot.stats"")
        stats.strip_dirs()
        stats.sort_stats('time', 'calls')
        stats.print_stats()

      
      
      if self._profileObj is None:
        print ""\n  Preparing to capture profile using hotshot...""
        if os.path.exists('hotshot.stats'):
          
          os.remove('hotshot.stats')
        self._profileObj = hotshot.Profile(""hotshot.stats"", 1, 1)
                                          
      self._profileObj.runcall(self._compute, *[inputs, outputs])
    else:
      self._compute(inputs, outputs)

  def _compute(self, inputs, outputs):
    


    
    
    

    if self._tfdr is None:
      raise RuntimeError(""TM has not been initialized"")

    
    self._conditionalBreak()

    self._iterations += 1

    
    buInputVector = inputs['bottomUpIn']

    
    resetSignal = False
    if 'resetIn' in inputs:
      assert len(inputs['resetIn']) == 1
      if inputs['resetIn'][0] != 0:
        self._tfdr.reset()
        self._sequencePos = 0  

    if self.computePredictedActiveCellIndices:
      prevPredictedState = self._tfdr.getPredictedState().reshape(-1).astype('float32')

    if self.anomalyMode:
      prevPredictedColumns = self._tfdr.topDownCompute().copy().nonzero()[0]

    
    tpOutput = self._tfdr.compute(buInputVector, self.learningMode, self.inferenceMode)
    self._sequencePos += 1

    
    if self.orColumnOutputs:
      tpOutput= tpOutput.reshape(self.columnCount,
                                     self.cellsPerColumn).max(axis=1)

    
    if self._fpLogTPOutput:
      output = tpOutput.reshape(-1)
      outputNZ = tpOutput.nonzero()[0]
      outStr = "" "".join([""%d"" % int(token) for token in outputNZ])
      print >>self._fpLogTPOutput, output.size, outStr

    
    outputs['bottomUpOut'][:] = tpOutput.flat

    if self.topDownMode:
      
      outputs['topDownOut'][:] = self._tfdr.topDownCompute().copy()

    
    if self.anomalyMode:
      activeLearnCells = self._tfdr.getLearnActiveStateT()
      size = activeLearnCells.shape[0] * activeLearnCells.shape[1]
      outputs['lrnActiveStateT'][:] = activeLearnCells.reshape(size)

      activeColumns = buInputVector.nonzero()[0]
      outputs['anomalyScore'][:] = anomaly.computeRawAnomalyScore(
        activeColumns, prevPredictedColumns)

    if self.computePredictedActiveCellIndices:
      
      activeState = self._tfdr._getActiveState().reshape(-1).astype('float32')
      activeIndices = numpy.where(activeState != 0)[0]
      predictedIndices= numpy.where(prevPredictedState != 0)[0]
      predictedActiveIndices = numpy.intersect1d(activeIndices, predictedIndices)
      outputs[""activeCells""].fill(0)
      outputs[""activeCells""][activeIndices] = 1
      outputs[""predictedActiveCells""].fill(0)
      outputs[""predictedActiveCells""][predictedActiveIndices] = 1


  
  
  
  
  

  
  @classmethod
  def getBaseSpec(cls):
    

    spec = dict(
      description=TMRegion.__doc__,
      singleNodeOnly=True,
      inputs=dict(
        bottomUpIn=dict(
          description=
,
          dataType='Real32',
          count=0,
          required=True,
          regionLevel=False,
          isDefaultInput=True,
          requireSplitterMap=False),

        resetIn=dict(
          description=
,
          dataType='Real32',
          count=1,
          required=False,
          regionLevel=True,
          isDefaultInput=False,
          requireSplitterMap=False),

        sequenceIdIn=dict(
          description=""Sequence ID"",
          dataType='UInt64',
          count=1,
          required=False,
          regionLevel=True,
          isDefaultInput=False,
          requireSplitterMap=False),

      ),

      outputs=dict(
        bottomUpOut=dict(
          description=
,
          dataType='Real32',
          count=0,
          regionLevel=True,
          isDefaultOutput=True),

        topDownOut=dict(
          description=
,
          dataType='Real32',
          count=0,
          regionLevel=True,
          isDefaultOutput=False),

        activeCells=dict(
          description=""The cells that are active"",
          dataType='Real32',
          count=0,
          regionLevel=True,
          isDefaultOutput=False),

        predictedActiveCells=dict(
          description=""The cells that are active and predicted"",
          dataType='Real32',
          count=0,
          regionLevel=True,
          isDefaultOutput=False),

        anomalyScore = dict(
          description=
,
          dataType='Real32',
          count=1,
          regionLevel=True,
          isDefaultOutput=False),

        lrnActiveStateT = dict(
          description=
,
          dataType='Real32',
          count=0,
          regionLevel=True,
          isDefaultOutput=False),

      ),

      parameters=dict(
        breakPdb=dict(
          description='Set to 1 to stop in the pdb debugger on the next compute',
          dataType='UInt32',
          count=1,
          constraints='bool',
          defaultValue=0,
          accessMode='ReadWrite'),

        breakKomodo=dict(
          description='Set to 1 to stop in the Komodo debugger on the next compute',
          dataType='UInt32',
          count=1,
          constraints='bool',
          defaultValue=0,
          accessMode='ReadWrite'),

      ),
      commands = {}
    )

    return spec

  @classmethod
  def getSpec(cls):
    

    spec = cls.getBaseSpec()
    t, o = _getAdditionalSpecs(temporalImp=gDefaultTemporalImp)
    spec['parameters'].update(t)
    spec['parameters'].update(o)

    return spec


  def getAlgorithmInstance(self):
    

    return self._tfdr


  def getParameter(self, parameterName, index=-1):
    

    if parameterName in self._temporalArgNames:
      return getattr(self._tfdr, parameterName)
    else:
      return PyRegion.getParameter(self, parameterName, index)


  def setParameter(self, parameterName, index, parameterValue):
    

    if parameterName in self._temporalArgNames:
      setattr(self._tfdr, parameterName, parameterValue)

    elif parameterName == ""logPathOutput"":
      self.logPathOutput = parameterValue
      
      if self._fpLogTPOutput is not None:
        self._fpLogTPOutput.close()
        self._fpLogTPOutput = None

      
      if parameterValue:
        self._fpLogTPOutput = open(self.logPathOutput, 'w')

    elif hasattr(self, parameterName):
      setattr(self, parameterName, parameterValue)

    else:
      raise Exception('Unknown parameter: ' + parameterName)


  
  
  
  
  

  def resetSequenceStates(self):
    

    self._tfdr.reset()
    self._sequencePos = 0  
    return

  def finishLearning(self):
    

    if self._tfdr is None:
      raise RuntimeError(""Temporal memory has not been initialized"")

    if hasattr(self._tfdr, 'finishLearning'):
      self.resetSequenceStates()
      self._tfdr.finishLearning()

  
  
  
  
  


  @staticmethod
  def getSchema():
    

    return TMRegionProto


  def writeToProto(self, proto):
    

    proto.temporalImp = self.temporalImp
    proto.columnCount = self.columnCount
    proto.inputWidth = self.inputWidth
    proto.cellsPerColumn = self.cellsPerColumn
    proto.learningMode = self.learningMode
    proto.inferenceMode = self.inferenceMode
    proto.anomalyMode = self.anomalyMode
    proto.topDownMode = self.topDownMode
    proto.computePredictedActiveCellIndices = (
      self.computePredictedActiveCellIndices)
    proto.orColumnOutputs = self.orColumnOutputs

    if self.temporalImp == ""py"":
      tmProto = proto.init(""backtrackingTM"")
    elif self.temporalImp == ""cpp"":
      tmProto = proto.init(""backtrackingTMCpp"")
    elif self.temporalImp == ""tm_py"":
      tmProto = proto.init(""temporalMemory"")
    elif self.temporalImp == ""tm_cpp"":
      tmProto = proto.init(""temporalMemory"")
    else:
      raise TypeError(
          ""Unsupported temporalImp for capnp serialization: {}"".format(
              self.temporalImp))

    self._tfdr.write(tmProto)


  @classmethod
  def readFromProto(cls, proto):
    

    instance = cls(proto.columnCount, proto.inputWidth, proto.cellsPerColumn)

    instance.temporalImp = proto.temporalImp
    instance.learningMode = proto.learningMode
    instance.inferenceMode = proto.inferenceMode
    instance.anomalyMode = proto.anomalyMode
    instance.topDownMode = proto.topDownMode
    instance.computePredictedActiveCellIndices = (
      proto.computePredictedActiveCellIndices)
    instance.orColumnOutputs = proto.orColumnOutputs

    if instance.temporalImp == ""py"":
      tmProto = proto.backtrackingTM
    elif instance.temporalImp == ""cpp"":
      tmProto = proto.backtrackingTMCpp
    elif instance.temporalImp == ""tm_py"":
      tmProto = proto.temporalMemory
    elif instance.temporalImp == ""tm_cpp"":
      tmProto = proto.temporalMemory
    else:
      raise TypeError(
          ""Unsupported temporalImp for capnp serialization: {}"".format(
              instance.temporalImp))

    instance._tfdr = _getTPClass(proto.temporalImp).read(tmProto)

    return instance


  def __getstate__(self):
    

    state = self.__dict__.copy()
    
    for ephemeralMemberName in self._getEphemeralMembersAll():
      state.pop(ephemeralMemberName, None)
    return state

  def serializeExtraData(self, filePath):
    

    if self._tfdr is not None:
      self._tfdr.saveToFile(filePath)

  def deSerializeExtraData(self, filePath):
    

    if self._tfdr is not None:
      self._tfdr.loadFromFile(filePath)


  def __setstate__(self, state):
    


    if not hasattr(self, 'storeDenseOutput'):
      self.storeDenseOutput = False

    if not hasattr(self, 'computePredictedActiveCellIndices'):
      self.computePredictedActiveCellIndices = False

    self.__dict__.update(state)
    self._loaded = True
    
    
    self._initialize()


  def _initEphemerals(self):
    


    self._sequencePos = 0
    self._fpLogTPOutput = None
    self.logPathOutput = None


  def _getEphemeralMembers(self):
    


    return ['_sequencePos', '_fpLogTPOutput', 'logPathOutput',]


  def _getEphemeralMembersBase(self):
    

    return [
        '_loaded',
        '_profileObj',
        '_iterations',
      ]


  def _getEphemeralMembersAll(self):
    

    return self._getEphemeralMembersBase() + self._getEphemeralMembers()


  def _checkEphemeralMembers(self):
    for attrName in self._getEphemeralMembersBase():
      if not hasattr(self, attrName):
        print ""Missing base class member:"", attrName
    for attrName in self._getEphemeralMembers():
      if not hasattr(self, attrName):
        print ""Missing derived class member:"", attrName

    for attrName in self._getEphemeralMembersBase():
      assert hasattr(self, attrName)
    for attrName in self._getEphemeralMembers():
      assert hasattr(self, attrName), ""Node missing attr '%s'."" % attrName

  
  
  
  
  


  def _conditionalBreak(self):
    if self.breakKomodo:
      import dbgp.client; dbgp.client.brk()
    if self.breakPdb:
      import pdb; pdb.set_trace()


  
  
  
  
  


  def getOutputElementCount(self, name):
    

    if name == 'bottomUpOut':
      return self.outputWidth
    elif name == 'topDownOut':
      return self.columnCount
    elif name == 'lrnActiveStateT':
      return self.outputWidth
    elif name == ""activeCells"":
      return self.outputWidth
    elif name == ""predictedActiveCells"":
      return self.outputWidth
    else:
      raise Exception(""Invalid output name specified"")


  
  
  def getParameterArrayCount(self, name, index):
    

    p = self.getParameter(name)
    if (not hasattr(p, '__len__')):
      raise Exception(""Attempt to access parameter '%s' as an array but it is not an array"" % name)
    return len(p)


  
  
  def getParameterArray(self, name, index, a):
    

    p = self.getParameter(name)
    if (not hasattr(p, '__len__')):
      raise Exception(""Attempt to access parameter '%s' as an array but it is not an array"" % name)

    if len(p) >  0:
      a[:] = p[:]
","from . channel_order import ChannelOrder
from .. colors import gamma as _gamma
from .. project import attributes, clock, data_maker, fields
import threading, time


class DriverBase(object):
    


    
    
    
    
    
    set_device_brightness = None

    pre_recursion = fields.default_converter

    @classmethod
    def construct(cls, project, **desc):
        

        return cls(maker=project.maker, **desc)

    def __init__(self, num=0, width=0, height=0, c_order=""RGB"",
                 gamma=None, maker=data_maker.MAKER, **kwds):
        attributes.set_reserved(self, 'driver', **kwds)

        if num == 0:
            num = width * height
            if num == 0:
                raise ValueError(""Either num, or width and height are needed!"")

        self.numLEDs = num
        gamma = gamma or _gamma.DEFAULT
        self.gamma = gamma

        if isinstance(c_order, str):
            c_order = ChannelOrder.make(c_order)

        self.c_order = c_order
        self.perm = ChannelOrder.ORDERS.index(c_order)

        self.pixel_positions = None

        self.width = width
        self.height = height
        self.maker = maker
        self._buf = self._make_buffer()

        self.lastUpdate = 0
        self.brightness_lock = threading.Lock()
        self._brightness = 255
        self._waiting_brightness = None
        self.clock = clock.Clock()

    def set_pixel_positions(self, pixel_positions):
        

        pass

    def set_colors(self, colors, pos):
        

        self._colors = colors
        self._pos = pos

        end = self._pos + self.numLEDs
        if end > len(self._colors):
            raise ValueError('Needed %d colors but found %d' % (
                end, len(self._colors)))

    def set_project(self, project):
        self.clock = project.clock

    def start(self):
        


    def stop(self):
        


    def cleanup(self):
        


    def join(self, timeout=None):
        


    def bufByteCount(self):
        

        return 3 * self.numLEDs

    def sync(self):
        


    def _compute_packet(self):
        


    def _send_packet(self):
        


    def update_colors(self):
        

        start = self.clock.time()

        with self.brightness_lock:
            
            brightness, self._waiting_brightness = (
                self._waiting_brightness, None)

        if brightness is not None:
            self._brightness = brightness
            if self.set_device_brightness:
                self.set_device_brightness(brightness)

        self._compute_packet()
        self._send_packet()

        self.lastUpdate = self.clock.time() - start

    def set_brightness(self, brightness):
        

        with self.brightness_lock:
            self._waiting_brightness = brightness

    def _render(self):
        

        if self.set_device_brightness:
            level = 1.0
        else:
            level = self._brightness / 255.0
        gam, (r, g, b) = self.gamma.get, self.c_order
        for i in range(min(self.numLEDs, len(self._buf) / 3)):
            c = [int(level * x) for x in self._colors[i + self._pos]]
            self._buf[i * 3:(i + 1) * 3] = gam(c[r]), gam(c[g]), gam(c[b])

    def _make_buffer(self):
        return self.maker.bytes(self.bufByteCount())
",,"


import tensorflow as tf

from tensorlayer.layers.core import Layer
from tensorlayer.layers.core import LayersConfig

from tensorlayer import logging

from tensorlayer.decorators import deprecated_alias
from tensorlayer.decorators import private_method

__all__ = [
    'DeformableConv2d',
]


class DeformableConv2d(Layer):
    


    @deprecated_alias(layer='prev_layer', end_support_version=1.9)  
    def __init__(
            self,
            prev_layer,
            offset_layer=None,
            
            n_filter=32,
            filter_size=(3, 3),
            act=None,
            name='deformable_conv_2d',
            W_init=tf.truncated_normal_initializer(stddev=0.02),
            b_init=tf.constant_initializer(value=0.0),
            W_init_args=None,
            b_init_args=None
    ):

        super(DeformableConv2d, self
             ).__init__(prev_layer=prev_layer, act=act, W_init_args=W_init_args, b_init_args=b_init_args, name=name)

        logging.info(
            ""DeformableConv2d %s: n_filter: %d, filter_size: %s act: %s"" %
            (self.name, n_filter, str(filter_size), self.act.__name__ if self.act is not None else 'No Activation')
        )

        self.offset_layer = offset_layer

        try:
            pre_channel = int(prev_layer.outputs.get_shape()[-1])
        except Exception:  
            pre_channel = 1
            logging.info(""[warnings] unknow input channels, set to 1"")
        shape = (filter_size[0], filter_size[1], pre_channel, n_filter)

        with tf.variable_scope(name):
            offset = self.offset_layer.outputs

            if offset.get_shape()[-1] != 2 * shape[0] * shape[1]:
                raise AssertionError(""offset.get_shape()[-1] is not equal to: %d"" % 2 * shape[0] * shape[1])

            
            input_h = int(self.inputs.get_shape()[1])
            input_w = int(self.inputs.get_shape()[2])
            kernel_n = shape[0] * shape[1]
            initial_offsets = tf.stack(
                tf.meshgrid(tf.range(shape[0]), tf.range(shape[1]), indexing='ij')
            )  
            initial_offsets = tf.reshape(initial_offsets, (-1, 2))  
            initial_offsets = tf.expand_dims(initial_offsets, 0)  
            initial_offsets = tf.expand_dims(initial_offsets, 0)  
            initial_offsets = tf.tile(initial_offsets, [input_h, input_w, 1, 1])  
            initial_offsets = tf.cast(initial_offsets, 'float32')
            grid = tf.meshgrid(
                tf.range(-int((shape[0] - 1) / 2.0), int(input_h - int((shape[0] - 1) / 2.0)), 1),
                tf.range(-int((shape[1] - 1) / 2.0), int(input_w - int((shape[1] - 1) / 2.0)), 1), indexing='ij'
            )

            grid = tf.stack(grid, axis=-1)
            grid = tf.cast(grid, 'float32')  
            grid = tf.expand_dims(grid, 2)  
            grid = tf.tile(grid, [1, 1, kernel_n, 1])  
            grid_offset = grid + initial_offsets  

            input_deform = self._tf_batch_map_offsets(self.inputs, offset, grid_offset)

            W = tf.get_variable(
                name='W_deformableconv2d', shape=[1, 1, shape[0] * shape[1], shape[-2], shape[-1]], initializer=W_init,
                dtype=LayersConfig.tf_dtype, **self.W_init_args
            )

            _tensor = tf.nn.conv3d(input_deform, W, strides=[1, 1, 1, 1, 1], padding='VALID', name=None)

            if b_init:
                b = tf.get_variable(
                    name='b_deformableconv2d', shape=(shape[-1]), initializer=b_init, dtype=LayersConfig.tf_dtype,
                    **self.b_init_args
                )

                _tensor = tf.nn.bias_add(_tensor, b, name='bias_add')

            self.outputs = tf.reshape(
                tensor=self._apply_activation(_tensor), shape=[tf.shape(self.inputs)[0], input_h, input_w, shape[-1]]
            )

        self._add_layers(self.outputs)

        if b_init:
            self._add_params([W, b])
        else:
            self._add_params(W)

    @private_method
    def _to_bc_h_w(self, x, x_shape):
        

        x = tf.transpose(x, [0, 3, 1, 2])
        x = tf.reshape(x, (-1, x_shape[1], x_shape[2]))
        return x

    @private_method
    def _to_b_h_w_n_c(self, x, x_shape):
        

        x = tf.reshape(x, (-1, x_shape[4], x_shape[1], x_shape[2], x_shape[3]))
        x = tf.transpose(x, [0, 2, 3, 4, 1])
        return x

    @private_method
    def tf_flatten(self, a):
        

        return tf.reshape(a, [-1])

    @private_method
    def _get_vals_by_coords(self, inputs, coords, idx, out_shape):
        indices = tf.stack(
            [idx, self.tf_flatten(coords[:, :, :, :, 0]),
             self.tf_flatten(coords[:, :, :, :, 1])], axis=-1
        )
        vals = tf.gather_nd(inputs, indices)
        vals = tf.reshape(vals, out_shape)
        return vals

    @private_method
    def _tf_repeat(self, a, repeats):
        

        

        if len(a.get_shape()) != 1:
            raise AssertionError(""This is not a 1D Tensor"")

        a = tf.expand_dims(a, -1)
        a = tf.tile(a, [1, repeats])
        a = self.tf_flatten(a)
        return a

    @private_method
    def _tf_batch_map_coordinates(self, inputs, coords):
        

        input_shape = inputs.get_shape()
        coords_shape = coords.get_shape()
        batch_channel = tf.shape(inputs)[0]
        input_h = int(input_shape[1])
        input_w = int(input_shape[2])
        kernel_n = int(coords_shape[3])
        n_coords = input_h * input_w * kernel_n

        coords_lt = tf.cast(tf.floor(coords), 'int32')
        coords_rb = tf.cast(tf.ceil(coords), 'int32')
        coords_lb = tf.stack([coords_lt[:, :, :, :, 0], coords_rb[:, :, :, :, 1]], axis=-1)
        coords_rt = tf.stack([coords_rb[:, :, :, :, 0], coords_lt[:, :, :, :, 1]], axis=-1)

        idx = self._tf_repeat(tf.range(batch_channel), n_coords)

        vals_lt = self._get_vals_by_coords(inputs, coords_lt, idx, (batch_channel, input_h, input_w, kernel_n))
        vals_rb = self._get_vals_by_coords(inputs, coords_rb, idx, (batch_channel, input_h, input_w, kernel_n))
        vals_lb = self._get_vals_by_coords(inputs, coords_lb, idx, (batch_channel, input_h, input_w, kernel_n))
        vals_rt = self._get_vals_by_coords(inputs, coords_rt, idx, (batch_channel, input_h, input_w, kernel_n))

        coords_offset_lt = coords - tf.cast(coords_lt, 'float32')

        vals_t = vals_lt + (vals_rt - vals_lt) * coords_offset_lt[:, :, :, :, 0]
        vals_b = vals_lb + (vals_rb - vals_lb) * coords_offset_lt[:, :, :, :, 0]
        mapped_vals = vals_t + (vals_b - vals_t) * coords_offset_lt[:, :, :, :, 1]

        return mapped_vals

    @private_method
    def _tf_batch_map_offsets(self, inputs, offsets, grid_offset):
        

        input_shape = inputs.get_shape()
        batch_size = tf.shape(inputs)[0]
        kernel_n = int(int(offsets.get_shape()[3]) / 2)
        input_h = input_shape[1]
        input_w = input_shape[2]
        channel = input_shape[3]

        
        inputs = self._to_bc_h_w(inputs, input_shape)

        
        offsets = tf.reshape(offsets, (batch_size, input_h, input_w, kernel_n, 2))
        
        

        coords = tf.expand_dims(grid_offset, 0)  
        coords = tf.tile(coords, [batch_size, 1, 1, 1, 1]) + offsets  

        
        coords = tf.stack(
            [
                tf.clip_by_value(coords[:, :, :, :, 0], 0.0, tf.cast(input_h - 1, 'float32')),
                tf.clip_by_value(coords[:, :, :, :, 1], 0.0, tf.cast(input_w - 1, 'float32'))
            ], axis=-1
        )
        coords = tf.tile(coords, [channel, 1, 1, 1, 1])

        mapped_vals = self._tf_batch_map_coordinates(inputs, coords)
        
        mapped_vals = self._to_b_h_w_n_c(mapped_vals, [batch_size, input_h, input_w, kernel_n, channel])

        return mapped_vals
","





from wsgidav import __version__, compat, util

import logging
import socket
import sys
import threading
import time
import traceback


__docformat__ = ""reStructuredText""


try:
    from http import client as http_client  
except ImportError:
    import httplib as http_client

try:
    from http import server as BaseHTTPServer  
except ImportError:
    import BaseHTTPServer

try:
    import socketserver  
except ImportError:
    import SocketServer as socketserver


_logger = util.get_module_logger(__name__)

_version = 1.0

SERVER_ERROR = 



class ExtHandler(BaseHTTPServer.BaseHTTPRequestHandler):

    _SUPPORTED_METHODS = [
        ""HEAD"",
        ""GET"",
        ""PUT"",
        ""POST"",
        ""OPTIONS"",
        ""TRACE"",
        ""DELETE"",
        ""PROPFIND"",
        ""PROPPATCH"",
        ""MKCOL"",
        ""COPY"",
        ""MOVE"",
        ""LOCK"",
        ""UNLOCK"",
    ]

    
    protocol_version = ""HTTP/1.1""

    server_version = ""WsgiDAV/{} ExtServer/{} {} Python {}"".format(
        __version__,
        _version,
        BaseHTTPServer.BaseHTTPRequestHandler.server_version,
        util.PYTHON_VERSION,
    )

    def log_message(self, *args):
        pass

    

    def log_request(self, *args):
        pass

    

    def getApp(self):
        
        _protocol, _host, path, _parameters, query, _fragment = compat.urlparse(
            ""http://dummyhost{}"".format(self.path), allow_fragments=False
        )
        
        for appPath, app in self.server.wsgiApplications:
            if path.startswith(appPath):
                
                path_info = path[len(appPath) :]
                if len(path_info) > 0:
                    if not path_info.startswith(""/""):
                        path_info = ""/"" + path_info
                if appPath.endswith(""/""):
                    script_name = appPath[:-1]
                else:
                    script_name = appPath
                
                return app, script_name, path_info, query
        return None, None, None, None

    def handlerFunctionClosure(self, name):
        def handlerFunction(*args, **kwargs):
            self.do_method()

        return handlerFunction

    def do_method(self):
        app, script_name, path_info, query = self.getApp()
        if not app:
            self.send_error(404, ""Application not found."")
            return
        self.runWSGIApp(app, script_name, path_info, query)

    def __getattr__(self, name):
        if len(name) > 3 and name[0:3] == ""do_"" and name[3:] in self._SUPPORTED_METHODS:
            return self.handlerFunctionClosure(name)
        elif name == ""_headers_buffer"":
            
            raise AttributeError
        self.send_error(501, ""Method Not Implemented."")
        return

    def runWSGIApp(self, application, script_name, path_info, query):
        
        

        if self.command == ""PUT"":
            pass  

        env = {
            ""wsgi.version"": (1, 0),
            ""wsgi.url_scheme"": ""http"",
            ""wsgi.input"": self.rfile,
            ""wsgi.errors"": sys.stderr,
            ""wsgi.multithread"": 1,
            ""wsgi.multiprocess"": 0,
            ""wsgi.run_once"": 0,
            ""REQUEST_METHOD"": self.command,
            ""SCRIPT_NAME"": script_name,
            ""PATH_INFO"": path_info,
            ""QUERY_STRING"": query,
            ""CONTENT_TYPE"": self.headers.get(""Content-Type"", """"),
            ""CONTENT_LENGTH"": self.headers.get(""Content-Length"", """"),
            ""REMOTE_ADDR"": self.client_address[0],
            ""SERVER_NAME"": self.server.server_address[0],
            ""SERVER_PORT"": compat.to_native(self.server.server_address[1]),
            ""SERVER_PROTOCOL"": self.request_version,
        }
        for httpHeader, httpValue in self.headers.items():
            if not httpHeader.lower() in (""content-type"", ""content-length""):
                env[""HTTP_{}"".format(httpHeader.replace(""-"", ""_"").upper())] = httpValue

        
        self.wsgiSentHeaders = 0
        self.wsgiHeaders = []

        try:
            
            _logger.debug(""runWSGIApp application()..."")
            result = application(env, self.wsgiStartResponse)
            try:
                for data in result:
                    if data:
                        self.wsgiWriteData(data)
                    else:
                        _logger.debug(""runWSGIApp empty data"")
            finally:
                _logger.debug(""runWSGIApp finally."")
                if hasattr(result, ""close""):
                    result.close()
        except Exception:
            _logger.debug(""runWSGIApp caught exception..."")
            errorMsg = compat.StringIO()
            traceback.print_exc(file=errorMsg)
            logging.error(errorMsg.getvalue())
            if not self.wsgiSentHeaders:
                self.wsgiStartResponse(
                    ""500 Server Error"", [(""Content-type"", ""text/html"")]
                )
            self.wsgiWriteData(SERVER_ERROR)

        if not self.wsgiSentHeaders:
            
            
            
            self.wsgiWriteData(b"""")
        return

    def wsgiStartResponse(self, response_status, response_headers, exc_info=None):
        _logger.debug(
            ""wsgiStartResponse({}, {}, {})"".format(
                response_status, response_headers, exc_info
            )
        )
        if self.wsgiSentHeaders:
            raise Exception(""Headers already sent and start_response called again!"")
        
        self.wsgiHeaders = (response_status, response_headers)
        return self.wsgiWriteData

    def wsgiWriteData(self, data):
        if not self.wsgiSentHeaders:
            status, headers = self.wsgiHeaders
            
            statusCode = status[: status.find("" "")]
            statusMsg = status[status.find("" "") + 1 :]
            _logger.debug(
                ""wsgiWriteData: send headers '{!r}', {!r}"".format(status, headers)
            )
            self.send_response(int(statusCode), statusMsg)
            for header, value in headers:
                self.send_header(header, value)
            self.end_headers()
            self.wsgiSentHeaders = 1
        
        
        _logger.debug(
            ""wsgiWriteData: write {} bytes: '{!r}'..."".format(
                len(data), compat.to_native(data[:50])
            )
        )
        if compat.is_unicode(data):  
            _logger.info(""ext_wsgiutils_server: Got unicode data: {!r}"".format(data))
            
            data = compat.to_bytes(data)

        try:
            self.wfile.write(data)
        except socket.error as e:
            
            
            
            if e.args[0] in (10053, 10054):
                _logger.info(""*** Caught socket.error: "", e, file=sys.stderr)
            else:
                raise


class ExtServer(socketserver.ThreadingMixIn, BaseHTTPServer.HTTPServer):
    def handle_error(self, request, client_address):
        

        ei = sys.exc_info()
        e = ei[1]
        
        
        
        if e.args[0] in (10053, 10054):
            _logger.error(""*** Caught socket.error: {}"".format(e))
            return
        
        
        _logger.error(""-"" * 40, file=sys.stderr)
        _logger.error(
            ""<{}> Exception happened during processing of request from {}"".format(
                threading.currentThread().ident, client_address
            )
        )
        _logger.error(client_address, file=sys.stderr)
        traceback.print_exc()
        _logger.error(""-"" * 40, file=sys.stderr)
        _logger.error(request, file=sys.stderr)

    

    def stop_serve_forever(self):
        

        assert hasattr(
            self, ""stop_request""
        ), ""serve_forever_stoppable() must be called before""
        assert not self.stop_request, ""stop_serve_forever() must only be called once""

        
        self.stop_request = True
        time.sleep(0.1)
        if self.stopped:
            
            return

        
        def _shutdownHandler(self):
            

            
            self.send_response(200)
            self.end_headers()
            self.server.stop_request = True

        if not hasattr(ExtHandler, ""do_SHUTDOWN""):
            ExtHandler.do_SHUTDOWN = _shutdownHandler

        
        (host, port) = self.server_address
        
        conn = http_client.HTTPConnection(""{}:{}"".format(host, port))
        conn.request(""SHUTDOWN"", ""/"")
        
        conn.getresponse()
        
        assert self.stop_request

    def serve_forever_stoppable(self):
        

        self.stop_request = False
        self.stopped = False

        while not self.stop_request:
            self.handle_request()

        
        self.stopped = True

    def __init__(self, serverAddress, wsgiApplications, serveFiles=1):
        BaseHTTPServer.HTTPServer.__init__(self, serverAddress, ExtHandler)
        appList = []
        for urlPath, wsgiApp in wsgiApplications.items():
            appList.append((urlPath, wsgiApp))
        self.wsgiApplications = appList
        self.serveFiles = serveFiles
        self.serverShuttingDown = 0


def serve(conf, app):
    host = conf.get(""host"", ""localhost"")
    port = int(conf.get(""port"", 8080))
    server = ExtServer((host, port), {"""": app})
    server_version = ExtHandler.server_version
    if conf.get(""verbose"") >= 1:
        _logger.info(""Running {}"".format(server_version))
        if host in ("""", ""0.0.0.0""):
            (hostname, _aliaslist, ipaddrlist) = socket.gethostbyname_ex(
                socket.gethostname()
            )
            _logger.info(
                ""Serving at {}, port {} (host='{}' {})..."".format(
                    host, port, hostname, ipaddrlist
                )
            )
        else:
            _logger.info(""Serving at {}, port {}..."".format(host, port))
    server.serve_forever()





if __name__ == ""__main__"":
    raise RuntimeError(""Use run_server.py"")
","
from __future__ import print_function
import numpy as np
import pandas as pd
import sys
from itertools import product
import copy


class ipfn(object):

    def __init__(self, original, aggregates, dimensions, weight_col='total',
                 convergence_rate=0.01, max_iteration=500, verbose=0, rate_tolerance=1e-8):
        

        self.original = original
        self.aggregates = aggregates
        self.dimensions = dimensions
        self.weight_col = weight_col
        self.conv_rate = convergence_rate
        self.max_itr = max_iteration
        self.verbose = verbose
        self.rate_tolerance = rate_tolerance

    @staticmethod
    def index_axis_elem(dims, axes, elems):
        inc_axis = 0
        idx = ()
        for dim in range(dims):
            if (inc_axis < len(axes)):
                if (dim == axes[inc_axis]):
                    idx += (elems[inc_axis],)
                    inc_axis += 1
                else:
                    idx += (np.s_[:],)
        return idx

    def ipfn_np(self, m, aggregates, dimensions, weight_col='total'):
        

        steps = len(aggregates)
        dim = len(m.shape)
        product_elem = []
        tables = [m]
        
        
        for inc in range(steps - 1):
            tables.append(np.array(np.zeros(m.shape)))
        original = copy.copy(m)

        
        for inc in range(steps):
            if inc == (steps - 1):
                table_update = m
                table_current = tables[inc]
            else:
                table_update = tables[inc + 1]
                table_current = tables[inc]
            for dimension in dimensions[inc]:
                product_elem.append(range(m.shape[dimension]))
            for item in product(*product_elem):
                idx = self.index_axis_elem(dim, dimensions[inc], item)
                table_current_slice = table_current[idx]
                mijk = table_current_slice.sum()
                
                xijk = aggregates[inc]
                xijk = xijk[item]
                if mijk == 0:
                    
                    
                    
                    table_update[idx] = table_current_slice
                else:
                    
                    
                    table_update[idx] = table_current_slice * 1.0 * xijk / mijk
                
                
                
                
            product_elem = []

        
        max_conv = 0
        for inc in range(steps):
            
            for dimension in dimensions[inc]:
                product_elem.append(range(m.shape[dimension]))
            for item in product(*product_elem):
                idx = self.index_axis_elem(dim, dimensions[inc], item)
                ori_ijk = aggregates[inc][item]
                m_slice = m[idx]
                m_ijk = m_slice.sum()
                
                if abs(m_ijk / ori_ijk - 1) > max_conv:
                    max_conv = abs(m_ijk / ori_ijk - 1)

            product_elem = []

        return m, max_conv

    def ipfn_df(self, df, aggregates, dimensions, weight_col='total'):
        


        steps = len(aggregates)
        tables = [df]
        for inc in range(steps - 1):
            tables.append(df.copy())
        original = df.copy()

        
        inc = 0
        for features in dimensions:
            if inc == (steps - 1):
                table_update = df
                table_current = tables[inc]
            else:
                table_update = tables[inc + 1]
                table_current = tables[inc]

            tmp = table_current.groupby(features)[weight_col].sum()
            xijk = aggregates[inc]

            feat_l = []
            for feature in features:
                feat_l.append(np.unique(table_current[feature]))
            table_update.set_index(features, inplace=True)
            table_current.set_index(features, inplace=True)

            for feature in product(*feat_l):
                den = tmp.loc[feature]
                
                if den == 0:
                    table_update.loc[feature, weight_col] =\
                        table_current.loc[feature, weight_col] *\
                        xijk.loc[feature]
                else:
                    table_update.loc[feature, weight_col] = \
                        table_current.loc[feature, weight_col].astype(float) * \
                        xijk.loc[feature] / den

            table_update.reset_index(inplace=True)
            table_current.reset_index(inplace=True)
            inc += 1
            feat_l = []

        
        max_conv = 0
        inc = 0
        for features in dimensions:
            tmp = df.groupby(features)[weight_col].sum()
            ori_ijk = aggregates[inc]
            temp_conv = max(abs(tmp / ori_ijk - 1))
            if temp_conv > max_conv:
                max_conv = temp_conv
            inc += 1

        return df, max_conv

    def iteration(self):
        


        i = 0
        conv = np.inf
        old_conv = -np.inf
        conv_list = []
        m = self.original

        
        if isinstance(self.original, pd.DataFrame):
            ipfn_method = self.ipfn_df
        elif isinstance(self.original, np.ndarray):
            ipfn_method = self.ipfn_np
            self.original = self.original.astype('float64')
        else:
            print('Data input instance not recognized')
            sys.exit(0)
        while ((i <= self.max_itr and conv > self.conv_rate) and
               (i <= self.max_itr and abs(conv - old_conv) > self.rate_tolerance)):
            old_conv = conv
            m, conv = ipfn_method(m, self.aggregates, self.dimensions, self.weight_col)
            conv_list.append(conv)
            i += 1
        converged = 1
        if i <= self.max_itr:
            if not conv > self.conv_rate:
                print('ipfn converged: convergence_rate below threshold')
            elif not abs(conv - old_conv) > self.rate_tolerance:
                print('ipfn converged: convergence_rate not updating or below rate_tolerance')
        else:
            print('Maximum iterations reached')
            converged = 0

        
        if self.verbose == 0:
            return m
        elif self.verbose == 1:
            return m, converged
        elif self.verbose == 2:
            return m, converged, pd.DataFrame({'iteration': range(i), 'conv': conv_list}).set_index('iteration')
        else:
            print('wrong verbose input, return None')
            sys.exit(0)


if __name__ == '__main__':

    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

    
    m = np.array([1., 2., 1., 3., 5., 5., 6., 2., 2., 1., 7., 2.,
                  5., 4., 2., 5., 5., 5., 3., 8., 7., 2., 7., 6.], )
    dma_l = [501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501,
             502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502, 502]
    size_l = [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4,
              1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]

    age_l = ['20-25', '30-35', '40-45',
             '20-25', '30-35', '40-45',
             '20-25', '30-35', '40-45',
             '20-25', '30-35', '40-45',
             '20-25', '30-35', '40-45',
             '20-25', '30-35', '40-45',
             '20-25', '30-35', '40-45',
             '20-25', '30-35', '40-45']

    df = pd.DataFrame()
    df['dma'] = dma_l
    df['size'] = size_l
    df['age'] = age_l
    df['total'] = m

    xipp = df.groupby('dma')['total'].sum()
    xpjp = df.groupby('size')['total'].sum()
    xppk = df.groupby('age')['total'].sum()
    xijp = df.groupby(['dma', 'size'])['total'].sum()
    xpjk = df.groupby(['size', 'age'])['total'].sum()
    

    xipp.loc[501] = 52
    xipp.loc[502] = 48

    xpjp.loc[1] = 20
    xpjp.loc[2] = 30
    xpjp.loc[3] = 35
    xpjp.loc[4] = 15

    xppk.loc['20-25'] = 35
    xppk.loc['30-35'] = 40
    xppk.loc['40-45'] = 25

    xijp.loc[501] = [9, 17, 19, 7]
    xijp.loc[502] = [11, 13, 16, 8]

    xpjk.loc[1] = [7, 9, 4]
    xpjk.loc[2] = [8, 12, 10]
    xpjk.loc[3] = [15, 12, 8]
    xpjk.loc[4] = [5, 7, 3]

    ipfn_df = ipfn(df, [xipp, xpjp, xppk, xijp, xpjk],
                   [['dma'], ['size'], ['age'], ['dma', 'size'], ['size', 'age']], 'total')
    df = ipfn_df.iteration()

    print(df)
    print(df.groupby('size')['total'].sum(), xpjp)
","


import multiprocessing as mp
import numpy as np
from .vec_env import VecEnv, CloudpickleWrapper, clear_mpi_env_vars
import ctypes
from baselines import logger

from .util import dict_to_obs, obs_space_info, obs_to_dict

_NP_TO_CT = {np.float32: ctypes.c_float,
             np.int32: ctypes.c_int32,
             np.int8: ctypes.c_int8,
             np.uint8: ctypes.c_char,
             np.bool: ctypes.c_bool}


class ShmemVecEnv(VecEnv):
    


    def __init__(self, env_fns, spaces=None, context='spawn'):
        

        ctx = mp.get_context(context)
        if spaces:
            observation_space, action_space = spaces
        else:
            logger.log('Creating dummy env object to get spaces')
            with logger.scoped_configure(format_strs=[]):
                dummy = env_fns[0]()
                observation_space, action_space = dummy.observation_space, dummy.action_space
                dummy.close()
                del dummy
        VecEnv.__init__(self, len(env_fns), observation_space, action_space)
        self.obs_keys, self.obs_shapes, self.obs_dtypes = obs_space_info(observation_space)
        self.obs_bufs = [
            {k: ctx.Array(_NP_TO_CT[self.obs_dtypes[k].type], int(np.prod(self.obs_shapes[k]))) for k in self.obs_keys}
            for _ in env_fns]
        self.parent_pipes = []
        self.procs = []
        with clear_mpi_env_vars():
            for env_fn, obs_buf in zip(env_fns, self.obs_bufs):
                wrapped_fn = CloudpickleWrapper(env_fn)
                parent_pipe, child_pipe = ctx.Pipe()
                proc = ctx.Process(target=_subproc_worker,
                            args=(child_pipe, parent_pipe, wrapped_fn, obs_buf, self.obs_shapes, self.obs_dtypes, self.obs_keys))
                proc.daemon = True
                self.procs.append(proc)
                self.parent_pipes.append(parent_pipe)
                proc.start()
                child_pipe.close()
        self.waiting_step = False
        self.viewer = None

    def reset(self):
        if self.waiting_step:
            logger.warn('Called reset() while waiting for the step to complete')
            self.step_wait()
        for pipe in self.parent_pipes:
            pipe.send(('reset', None))
        return self._decode_obses([pipe.recv() for pipe in self.parent_pipes])

    def step_async(self, actions):
        assert len(actions) == len(self.parent_pipes)
        for pipe, act in zip(self.parent_pipes, actions):
            pipe.send(('step', act))

    def step_wait(self):
        outs = [pipe.recv() for pipe in self.parent_pipes]
        obs, rews, dones, infos = zip(*outs)
        return self._decode_obses(obs), np.array(rews), np.array(dones), infos

    def close_extras(self):
        if self.waiting_step:
            self.step_wait()
        for pipe in self.parent_pipes:
            pipe.send(('close', None))
        for pipe in self.parent_pipes:
            pipe.recv()
            pipe.close()
        for proc in self.procs:
            proc.join()

    def get_images(self, mode='human'):
        for pipe in self.parent_pipes:
            pipe.send(('render', None))
        return [pipe.recv() for pipe in self.parent_pipes]

    def _decode_obses(self, obs):
        result = {}
        for k in self.obs_keys:

            bufs = [b[k] for b in self.obs_bufs]
            o = [np.frombuffer(b.get_obj(), dtype=self.obs_dtypes[k]).reshape(self.obs_shapes[k]) for b in bufs]
            result[k] = np.array(o)
        return dict_to_obs(result)


def _subproc_worker(pipe, parent_pipe, env_fn_wrapper, obs_bufs, obs_shapes, obs_dtypes, keys):
    

    def _write_obs(maybe_dict_obs):
        flatdict = obs_to_dict(maybe_dict_obs)
        for k in keys:
            dst = obs_bufs[k].get_obj()
            dst_np = np.frombuffer(dst, dtype=obs_dtypes[k]).reshape(obs_shapes[k])  
            np.copyto(dst_np, flatdict[k])

    env = env_fn_wrapper.x()
    parent_pipe.close()
    try:
        while True:
            cmd, data = pipe.recv()
            if cmd == 'reset':
                pipe.send(_write_obs(env.reset()))
            elif cmd == 'step':
                obs, reward, done, info = env.step(data)
                if done:
                    obs = env.reset()
                pipe.send((_write_obs(obs), reward, done, info))
            elif cmd == 'render':
                pipe.send(env.render(mode='rgb_array'))
            elif cmd == 'close':
                pipe.send(None)
                break
            else:
                raise RuntimeError('Got unrecognized cmd %s' % cmd)
    except KeyboardInterrupt:
        print('ShmemVecEnv worker: got KeyboardInterrupt')
    finally:
        env.close()
","





























from __future__ import absolute_import, unicode_literals, print_function

import os
import re

from ._compat import string_types




__all__ = ['FileSet', 'includes', 'excludes']


def glob2re(part):
    

    return ""[^/]*"".join(
        re.escape(bit).replace(r'\[\^', '[^').replace(r'\[', '[').replace(r'\]', ']')
        for bit in part.split(""*"")
    )


def parse_glob(pattern):
    

    if not pattern:
        return

    bits = pattern.split(""/"")
    dirs, filename = bits[:-1], bits[-1]

    for dirname in dirs:
        if dirname == ""**"":
            yield  ""(|.+/)""
        else:
            yield glob2re(dirname) + ""/""

    yield glob2re(filename)


def compile_glob(spec):
    

    parsed = """".join(parse_glob(spec))
    regex = ""^{0}$"".format(parsed)
    return re.compile(regex)


class Pattern(object):
    


    def __init__(self, spec, inclusive):
        

        self.compiled = compile_glob(spec.rstrip('/'))
        self.inclusive = inclusive
        self.is_dir = spec.endswith('/')

    def __str__(self):
        

        return ('+' if self.inclusive else '-') + self.compiled.pattern

    def matches(self, path):
        

        return bool(self.compiled.match(path))


class FileSet(object):
    


    def __init__(self, root, patterns):
        if isinstance(patterns, string_types):
            patterns = [patterns]

        self.root = root
        self.patterns = [i if hasattr(i, 'inclusive') else includes(i) for i in patterns]

    def __repr__(self):
        return ""<FileSet at {0} {1}>"".format(repr(self.root), ' '.join(str(i) for i in self. patterns))

    def included(self, path, is_dir=False):
        

        inclusive = None
        for pattern in self.patterns:
            if pattern.is_dir == is_dir and pattern.matches(path):
                inclusive = pattern.inclusive

        
        return inclusive

    def __iter__(self):
        for path in self.walk():
            yield path

    def __or__(self, other):
        return set(self) | set(other)

    def __ror__(self, other):
        return self | other

    def __and__(self, other):
        return set(self) & set(other)

    def __rand__(self, other):
        return self & other

    def walk(self, **kwargs):
        

        lead = ''
        if 'with_root' in kwargs and kwargs.pop('with_root'):
            lead = self.root.rstrip(os.sep) + os.sep

        for base, dirs, files in os.walk(self.root, **kwargs):
            prefix = base[len(self.root):].lstrip(os.sep)
            bits = prefix.split(os.sep) if prefix else []

            for dirname in dirs[:]:
                path = '/'.join(bits + [dirname])
                inclusive = self.included(path, is_dir=True)
                if inclusive:
                    yield lead + path + '/'
                elif inclusive is False:
                    dirs.remove(dirname)

            for filename in files:
                path = '/'.join(bits + [filename])
                if self.included(path):
                    yield lead + path


def includes(pattern):
    

    return Pattern(pattern, inclusive=True)


def excludes(pattern):
    

    return Pattern(pattern, inclusive=False)
","














from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import importlib
from inspect import getargspec
from six.moves import xrange
import threading
import time
import warnings

from tensorforce import TensorForceError
from tensorforce.execution.base_runner import BaseRunner
from tensorforce.agents.learning_agent import LearningAgent
from tensorforce.agents import agents as AgentsDictionary


class ThreadedRunner(BaseRunner):
    


    def __init__(self, agent, environment, repeat_actions=1, save_path=None, save_episodes=None, save_frequency=None,
                 save_frequency_unit=None, agents=None, environments=None):
        

        if agents is not None:
            warnings.warn(""WARNING: `agents` parameter is deprecated, use `agent` instead."",
                          category=DeprecationWarning)
            agent = agents
        if environments is not None:
            warnings.warn(""WARNING: `environments` parameter is deprecated, use `environment` instead."",
                          category=DeprecationWarning)
            environment = environments
        super(ThreadedRunner, self).__init__(agent, environment, repeat_actions)

        if len(agent) != len(environment):
            raise TensorForceError(""Each agent must have its own environment. Got {a} agents and {e} environments."".
                                   format(a=len(self.agent), e=len(self.environment)))
        self.save_path = save_path
        self.save_episodes = save_episodes
        if self.save_episodes is not None:
            warnings.warn(""WARNING: `save_episodes` parameter is deprecated, use `save_frequency` AND ""
                          ""`save_frequency_unit` instead."",
                          category=DeprecationWarning)
            self.save_frequency = self.save_episodes
            self.save_frequency_unit = ""e""
        else:
            self.save_frequency = save_frequency
            self.save_frequency_unit = save_frequency_unit

        
        self.episode_list_lock = threading.Lock()
        
        self.should_stop = False
        
        self.time = None

    def close(self):
        self.agent[0].close()  
        for e in self.environment:
            e.close()

    def run(
        self,
        num_episodes=-1,
        max_episode_timesteps=-1,
        episode_finished=None,
        summary_report=None,
        summary_interval=0,
        num_timesteps=None,
        deterministic=False,
        episodes=None,
        max_timesteps=None,
        testing=False,
        sleep=None
    ):
        


        
        if episodes is not None:
            num_episodes = episodes
            warnings.warn(""WARNING: `episodes` parameter is deprecated, use `num_episodes` instead."",
                          category=DeprecationWarning)
        assert isinstance(num_episodes, int)
        
        if max_timesteps is not None:
            max_episode_timesteps = max_timesteps
            warnings.warn(""WARNING: `max_timesteps` parameter is deprecated, use `max_episode_timesteps` instead."",
                          category=DeprecationWarning)
        assert isinstance(max_episode_timesteps, int)

        if summary_report is not None:
            warnings.warn(""WARNING: `summary_report` parameter is deprecated, use `episode_finished` callback ""
                          ""instead to generate summaries every n episodes."",
                          category=DeprecationWarning)

        self.reset()

        
        self.global_episode = 0
        self.global_timestep = 0
        self.should_stop = False

        
        threads = [threading.Thread(target=self._run_single, args=(t, self.agent[t], self.environment[t],),
                                    kwargs={""deterministic"": deterministic,
                                            ""max_episode_timesteps"": max_episode_timesteps,
                                            ""episode_finished"": episode_finished,
                                            ""testing"": testing,
                                            ""sleep"": sleep})
                   for t in range(len(self.agent))]

        
        self.start_time = time.time()
        [t.start() for t in threads]

        
        try:
            next_summary = 0
            next_save = 0 if self.save_frequency_unit != ""s"" else time.time()
            while any([t.is_alive() for t in threads]) and self.global_episode < num_episodes or num_episodes == -1:
                self.time = time.time()

                
                if summary_report is not None and self.global_episode > next_summary:
                    summary_report(self)
                    next_summary += summary_interval

                if self.save_path and self.save_frequency is not None:
                    do_save = True
                    current = None
                    if self.save_frequency_unit == ""e"" and self.global_episode > next_save:
                        current = self.global_episode
                    elif self.save_frequency_unit == ""s"" and self.time > next_save:
                        current = self.time
                    elif self.save_frequency_unit == ""t"" and self.global_timestep > next_save:
                        current = self.global_timestep
                    else:
                        do_save = False

                    if do_save:
                        self.agent[0].save_model(self.save_path)
                        
                        while next_save < current:
                            next_save += self.save_frequency
                time.sleep(1)

        except KeyboardInterrupt:
            print('Keyboard interrupt, sending stop command to threads')

        self.should_stop = True

        
        [t.join() for t in threads]
        print('All threads stopped')

    def _run_single(self, thread_id, agent, environment, deterministic=False,
                    max_episode_timesteps=-1, episode_finished=None, testing=False, sleep=None):
        


        
        old_episode_finished = False
        if episode_finished is not None and len(getargspec(episode_finished).args) == 1:
            old_episode_finished = True

        episode = 0
        
        while not self.should_stop:
            state = environment.reset()
            agent.reset()
            self.global_timestep, self.global_episode = agent.timestep, agent.episode
            episode_reward = 0

            
            time_step = 0
            time_start = time.time()
            while True:
                action, internals, states = agent.act(states=state, deterministic=deterministic, buffered=False)
                reward = 0
                for repeat in xrange(self.repeat_actions):
                    state, terminal, step_reward = environment.execute(action=action)
                    reward += step_reward
                    if terminal:
                        break

                if not testing:
                    
                    
                    agent.atomic_observe(
                        states=state,
                        actions=action,
                        internals=internals,
                        reward=reward,
                        terminal=terminal
                    )

                if sleep is not None:
                    time.sleep(sleep)

                time_step += 1
                episode_reward += reward

                if terminal or time_step == max_episode_timesteps:
                    break

                
                if self.should_stop:
                    return

            self.global_timestep += time_step

            
            self.episode_list_lock.acquire()
            self.episode_rewards.append(episode_reward)
            self.episode_timesteps.append(time_step)
            self.episode_times.append(time.time() - time_start)
            self.episode_list_lock.release()

            if episode_finished is not None:
                
                if old_episode_finished:
                    summary_data = {
                        ""thread_id"": thread_id,
                        ""episode"": episode,
                        ""timestep"": time_step,
                        ""episode_reward"": episode_reward
                    }
                    if not episode_finished(summary_data):
                        return
                
                elif not episode_finished(self, thread_id):
                    return

            episode += 1

    
    @property
    def agents(self):
        return self.agent

    @property
    def environments(self):
        return self.environment

    @property
    def episode_lengths(self):
        return self.episode_timesteps

    @property
    def global_step(self):
        return self.global_timestep


def WorkerAgentGenerator(agent_class):
    


    
    if isinstance(agent_class, str):
        agent_class = AgentsDictionary.get(agent_class)
        
        if not agent_class and agent_class.find('.') != -1:
            module_name, function_name = agent_class.rsplit('.', 1)
            module = importlib.import_module(module_name)
            agent_class = getattr(module, function_name)

    class WorkerAgent(agent_class):
        


        def __init__(self, model=None, **kwargs):
            
            self.model = model
            
            if not issubclass(agent_class, LearningAgent):
                kwargs.pop(""network"")
            
            super(WorkerAgent, self).__init__(**kwargs)

        def initialize_model(self):
            
            return self.model

    return WorkerAgent


def clone_worker_agent(agent, factor, environment, network, agent_config):
    

    ret = [agent]
    for i in xrange(factor - 1):
        worker = WorkerAgentGenerator(type(agent))(
            states=environment.states,
            actions=environment.actions,
            network=network,
            model=agent.model,
            **agent_config
        )
        ret.append(worker)

    return ret
","


import sliplib
import pytun
import threading
import serial
import timeout_decorator

import serial.tools.list_ports


class Faraday(object):
    


    def __init__(self, serialPort=None):
        self._serialPort = serialPort

    def send(self, msg):
        

        
        slipDriver = sliplib.Driver()

        
        slipData = slipDriver.send(msg)

        
        res = self._serialPort.write(slipData)

        
        return res

    def receive(self, length):
        


        
        slipDriver = sliplib.Driver()

        
        ret = self._serialPort.read(length)

        
        temp = slipDriver.receive(ret)
        return iter(temp)


class TunnelServer(object):
    

    def __init__(self, addr,
                 netmask,
                 mtu,
                 name):
        self._tun = pytun.TunTapDevice(name=name)
        self._tun.addr = addr
        self._tun.netmask = netmask
        self._tun.mtu = mtu

        
        self._tun.persist(True)
        self._tun.up()

    def __del__(self):
        

        self._tun.down()
        print(""TUN brought down..."")


class Monitor(threading.Thread):
    

    def __init__(self,
                 serialPort,
                 isRunning,
                 name=""Faraday"",
                 addr='10.0.0.1',
                 netmask='255.255.255.0',
                 mtu=1500):
        super().__init__()
        self.isRunning = isRunning
        self._serialPort = serialPort

        
        self._TUN = TunnelServer(name=name,
                                 addr=addr,
                                 netmask=netmask,
                                 mtu=mtu)

        
        self._faraday = Faraday(serialPort=serialPort)

    @timeout_decorator.timeout(1, use_signals=False)
    def checkTUN(self):
        

        packet = self._TUN._tun.read(self._TUN._tun.mtu)
        return(packet)

    def monitorTUN(self):
        

        packet = self.checkTUN()

        if packet:
            try:
                
                ret = self._faraday.send(packet)
                return ret

            except AttributeError as error:
                
                print(""AttributeError"")

    def rxSerial(self, length):
        

        return(self._faraday.receive(length))

    def txSerial(self, data):
        

        return self._faraday.send(data)

    def checkSerial(self):
        

        for item in self.rxSerial(self._TUN._tun.mtu):
            
            try:
                self._TUN._tun.write(item)
            except pytun.Error as error:
                print(""pytun error writing: {0}"".format(item))
                print(error)

    def run(self):
        

        while self.isRunning.is_set():
            try:
                try:
                    
                    self.monitorTUN()

                except timeout_decorator.TimeoutError as error:
                    
                    pass
                self.checkSerial()
            except KeyboardInterrupt:
                break


class SerialTestClass(object):
    

    def __init__(self):
        

        self._port = ""loop://""
        self._timeout = 0
        self._baudrate = 115200
        self.serialPort = \
            serial.serial_for_url(url=self._port,
                                  timeout=self._timeout,
                                  baudrate=self._baudrate)

    def isPortAvailable(port='/dev/ttyUSB0'):
        

        isPortAvailable = serial.tools.list_ports.grep(port)

        try:
            next(isPortAvailable)
            available = True
        except StopIteration:
            available = False

        return available
","



















from __future__ import absolute_import, division

__docformat__ = ""restructuredtext en""

import threading
import socket
import logging

try:
    from socket import SOMAXCONN
except ImportError:
    SOMAXCONN = 5

from ..mainloop.interfaces import IOHandler, HandlerReady
from ..exceptions import PyXMPPIOError

logger = logging.getLogger(""pyxmpp2.server.listener"")

from ..transport import BLOCKING_ERRORS



class TCPListener(IOHandler):
    

    _socket = None
    def __init__(self, family, address, target):
        

        self._socket = None
        self._lock = threading.RLock()
        self._target = target
        sock = socket.socket(family, socket.SOCK_STREAM)
        try:
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.bind(address)
        except:
            sock.close()
            raise
        self._socket = sock

    def __del__(self):
        if self._socket:
            self._socket.close()
            self._socket = None

    def close(self):
        with self._lock:
            if self._socket:
                self._socket.close()
                self._socket = None

    def prepare(self):
        

        with self._lock:
            if self._socket:
                self._socket.listen(SOMAXCONN)
                self._socket.setblocking(False)
            return HandlerReady()

    def fileno(self):
        

        with self._lock:
            if self._socket:
                return self._socket.fileno()

    def is_readable(self):
        with self._lock:
            return self._socket is not None

    def wait_for_readability(self):
        with self._lock:
            return self._socket is not None

    def is_writable(self):
        return False

    def wait_for_writability(self):
        return False

    def handle_write(self):
        return

    def handle_read(self):
        

        with self._lock:
            logger.debug(""handle_read()"")
            if self._socket is None:
                return
            while True:
                try:
                    sock, address = self._socket.accept()
                except socket.error, err:
                    if err.args[0] in BLOCKING_ERRORS:
                        break
                    else:
                        raise
                logger.debug(""Accepted connection from: {0!r}"".format(address))
                self._target(sock, address)

    def handle_hup(self):
        self.close()

    def handle_err(self):
        self.close()
        raise PyXMPPIOError(""Unhandled error on socket"")

    def handle_nval(self):
        self.close()
        raise PyXMPPIOError(""Invalid file descriptor used in main event loop"")
","































from collections import deque
from abc import ABCMeta, abstractproperty
import sys
import locale
import gettext
from shoebot.core.drawqueue import DrawQueue

APP = 'shoebot'
DIR = sys.prefix + '/share/shoebot/locale'
locale.setlocale(locale.LC_ALL, '')
gettext.bindtextdomain(APP, DIR)

gettext.textdomain(APP)
_ = gettext.gettext

CENTER = 'center'
CORNER = 'corner'

TOP_LEFT = 1
BOTTOM_LEFT = 2


class Canvas(object):
    __metaclass__ = ABCMeta

    DEFAULT_SIZE = 400, 400
    DEFAULT_MODE = CENTER

    

    def __init__(self, sink):
        
        self.sink = sink

        self.finished = False
        self.color_range = 1
        self.color_mode = 1
        self.path_mode = CORNER
        self.size = None
        self.reset_canvas()

    def set_bot(self, bot):
        

        self.bot = bot
        self.sink.set_bot(bot)

    def get_input_device(self):
        

        return None

    def initial_drawqueue(self):
        

        return DrawQueue()

    def initial_transform(self):
        

        pass

    @abstractproperty
    def reset_drawqueue(self):
        pass

    @abstractproperty
    def reset_transform(self):
        pass

    def reset_canvas(self):
        self.reset_transform()
        self.reset_drawqueue()
        self.matrix_stack = deque()

    def settings(self, **kwargs):
        

        for k, v in kwargs.items():
            setattr(self, k, v)

    def size_or_default(self):
        

        if not self.size:
            self.size = self.DEFAULT_SIZE
        return self.size

    def set_size(self, size):
        

        if self.size is None:
            self.size = size
            return size
        else:
            return self.size

    def get_width(self):
        if self.size is not None:
            return self.size[0]
        else:
            return self.DEFAULT_SIZE[0]

    def get_height(self):
        if self.size is not None:
            return self.size[1]
        else:
            return self.DEFAULT_SIZE[1]

    def snapshot(self, target, defer=True, file_number=None):
        

        output_func = self.output_closure(target, file_number)
        if defer:
            self._drawqueue.append(output_func)
        else:
            self._drawqueue.append_immediate(output_func)

    def flush(self, frame):
        

        self.sink.render(self.size_or_default(), frame, self._drawqueue)
        self.reset_drawqueue()

    def deferred_render(self, render_func):
        

        self._drawqueue.append(render_func)

    width = property(get_width)
    height = property(get_height)
","



















__all__ = ['to_decimal', 'from_decimal']
from math import log, ceil
import sys
from re import findall
from aprslib import string_type, int_type

if sys.version_info < (3,):
    _range = xrange
else:
    _range = range


def to_decimal(text):
    


    if not isinstance(text, string_type):
        raise TypeError(""expected str or unicode, %s given"" % type(text))

    if findall(r""[\x00-\x20\x7c-\xff]"", text):
        raise ValueError(""invalid character in sequence"")

    text = text.lstrip('!')
    decimal = 0
    length = len(text) - 1
    for i, char in enumerate(text):
        decimal += (ord(char) - 33) * (91 ** (length - i))

    return decimal if text != '' else 0


def from_decimal(number, width=1):
    

    text = []

    if not isinstance(number, int_type):
        raise TypeError(""Expected number to be int, got %s"", type(number))
    elif not isinstance(width, int_type):
        raise TypeError(""Expected width to be int, got %s"", type(number))
    elif number < 0:
        raise ValueError(""Expected number to be positive integer"")
    elif number > 0:
        max_n = ceil(log(number) / log(91))

        for n in _range(int(max_n), -1, -1):
            quotient, number = divmod(number, 91**n)
            text.append(chr(33 + quotient))

    return """".join(text).lstrip('!').rjust(max(1, width), '!')
","from __future__ import absolute_import

import numpy as np
import logging

from .base import DifferentiableModel


class TensorFlowModel(DifferentiableModel):
    


    def __init__(
            self,
            images,
            logits,
            bounds,
            channel_axis=3,
            preprocessing=(0, 1)):

        super(TensorFlowModel, self).__init__(bounds=bounds,
                                              channel_axis=channel_axis,
                                              preprocessing=preprocessing)

        
        import tensorflow as tf

        session = tf.get_default_session()
        if session is None:
            logging.warning('No default session. Created a new tf.Session. '
                            'Please restore variables using this session.')
            session = tf.Session(graph=images.graph)
            self._created_session = True
        else:
            self._created_session = False
            assert session.graph == images.graph, \
                'The default session uses the wrong graph'

        with session.graph.as_default():
            self._session = session
            self._images = images
            self._batch_logits = logits
            self._logits = tf.squeeze(logits, axis=0)
            self._label = tf.placeholder(tf.int64, (), name='label')

            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=self._label[tf.newaxis],
                logits=self._logits[tf.newaxis])
            self._loss = tf.squeeze(loss, axis=0)
            gradients = tf.gradients(loss, images)
            assert len(gradients) == 1
            if gradients[0] is None:
                gradients[0] = tf.zeros_like(images)
            self._gradient = tf.squeeze(gradients[0], axis=0)

            self._bw_gradient_pre = tf.placeholder(tf.float32, self._logits.shape)  
            bw_loss = tf.reduce_sum(self._logits * self._bw_gradient_pre)
            bw_gradients = tf.gradients(bw_loss, images)
            assert len(bw_gradients) == 1
            if bw_gradients[0] is None:
                bw_gradients[0] = tf.zeros_like(images)
            self._bw_gradient = tf.squeeze(bw_gradients[0], axis=0)

    @classmethod
    def from_keras(cls, model, bounds, input_shape=None,
                   channel_axis=3, preprocessing=(0, 1)):
        

        import tensorflow as tf
        if input_shape is None:
            try:
                input_shape = model.input_shape[1:]
            except AttributeError:
                raise ValueError(
                    'Please specify input_shape manually or '
                    'provide a model with an input_shape attribute')
        with tf.keras.backend.get_session().as_default():
            inputs = tf.placeholder(tf.float32, (None,) + input_shape)
            logits = model(inputs)
            return cls(inputs, logits, bounds=bounds,
                       channel_axis=channel_axis, preprocessing=preprocessing)

    def __exit__(self, exc_type, exc_value, traceback):
        if self._created_session:
            self._session.close()
        return None

    @property
    def session(self):
        return self._session

    def num_classes(self):
        _, n = self._batch_logits.get_shape().as_list()
        return n

    def batch_predictions(self, images):
        images, _ = self._process_input(images)
        predictions = self._session.run(
            self._batch_logits,
            feed_dict={self._images: images})
        return predictions

    def predictions_and_gradient(self, image, label):
        image, dpdx = self._process_input(image)
        predictions, gradient = self._session.run(
            [self._logits, self._gradient],
            feed_dict={
                self._images: image[np.newaxis],
                self._label: label})
        gradient = self._process_gradient(dpdx, gradient)
        return predictions, gradient

    def gradient(self, image, label):
        image, dpdx = self._process_input(image)
        g = self._session.run(
            self._gradient,
            feed_dict={
                self._images: image[np.newaxis],
                self._label: label})
        g = self._process_gradient(dpdx, g)
        return g

    def _loss_fn(self, image, label):
        image, dpdx = self._process_input(image)
        loss = self._session.run(
            self._loss,
            feed_dict={
                self._images: image[np.newaxis],
                self._label: label})
        return loss

    def backward(self, gradient, image):
        assert gradient.ndim == 1
        input_shape = image.shape
        image, dpdx = self._process_input(image)
        g = self._session.run(
            self._bw_gradient,
            feed_dict={
                self._images: image[np.newaxis],
                self._bw_gradient_pre: gradient})
        g = self._process_gradient(dpdx, g)
        assert g.shape == input_shape
        return g
","


class MPEventLoopRunner(object):

    


    def __init__(self, communicationChannel):
        self.communicationChannel = communicationChannel
        self.nruns = 0

    def __repr__(self):
        return '{}(communicationChannel = {!r}'.format(
            self.__class__.__name__,
            self.communicationChannel
        )

    def begin(self):
        

        pass

    def run(self, eventLoop):
        


        self.nruns += 1
        return self.communicationChannel.put(eventLoop)

    def run_multiple(self, eventLoops):
        


        self.nruns += len(eventLoops)
        return self.communicationChannel.put_multiple(eventLoops)

    def poll(self):
        

        ret = self.communicationChannel.receive_finished()
        self.nruns -= len(ret)
        return ret

    def receive_one(self):
        

        if self.nruns == 0:
            return None
        ret = self.communicationChannel.receive_one()
        if ret is not None:
            self.nruns -= 1
        return ret

    def receive(self):
        

        ret = self.communicationChannel.receive_all()
        self.nruns -= len(ret)
        if self.nruns > 0:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(
                'too few results received: {} results received, {} more expected'.format(
                    len(ret), self.nruns))
        elif self.nruns < 0:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(
                'too many results received: {} results received, {} too many'.format(
                    len(ret), -self.nruns))
        return ret

    def end(self):
        


        results = self.communicationChannel.receive()

        if self.nruns != len(results):
            import logging
            logger = logging.getLogger(__name__)
            
            logger.warning(
                'too few results received: {} results received, {} expected'.format(
                    len(results),
                    self.nruns
                ))

        return results


","










import logging
from astrobase import log_sub, log_fmt, log_date_fmt

DEBUG = False
if DEBUG:
    level = logging.DEBUG
else:
    level = logging.INFO
LOGGER = logging.getLogger(__name__)
logging.basicConfig(
    level=level,
    style=log_sub,
    format=log_fmt,
    datefmt=log_date_fmt,
)

LOGDEBUG = LOGGER.debug
LOGINFO = LOGGER.info
LOGWARNING = LOGGER.warning
LOGERROR = LOGGER.error
LOGEXCEPTION = LOGGER.exception






try:
    import cPickle as pickle
except Exception as e:
    import pickle

import os
import os.path
import glob
import multiprocessing as mp

from tornado.escape import squeeze



from functools import reduce
from operator import getitem
def _dict_get(datadict, keylist):
    return reduce(getitem, keylist, datadict)







NCPUS = mp.cpu_count()







from astrobase.lcproc import get_lcformat
from astrobase.lcmath import (
    normalize_magseries,
    time_bin_magseries_with_errs,
)







def timebinlc(lcfile,
              binsizesec,
              outdir=None,
              lcformat='hat-sql',
              lcformatdir=None,
              timecols=None,
              magcols=None,
              errcols=None,
              minbinelems=7):

    


    try:
        formatinfo = get_lcformat(lcformat,
                                  use_lcformat_dir=lcformatdir)
        if formatinfo:
            (dfileglob, readerfunc,
             dtimecols, dmagcols, derrcols,
             magsarefluxes, normfunc) = formatinfo
        else:
            LOGERROR(""can't figure out the light curve format"")
            return None
    except Exception as e:
        LOGEXCEPTION(""can't figure out the light curve format"")
        return None

    
    
    if timecols is None:
        timecols = dtimecols
    if magcols is None:
        magcols = dmagcols
    if errcols is None:
        errcols = derrcols

    
    lcdict = readerfunc(lcfile)

    
    
    
    if ( (isinstance(lcdict, (list, tuple))) and
         (isinstance(lcdict[0], dict)) ):
        lcdict = lcdict[0]

    
    if 'binned' in lcdict:
        LOGERROR('this light curve appears to be binned already, skipping...')
        return None

    lcdict['binned'] = {}

    for tcol, mcol, ecol in zip(timecols, magcols, errcols):

        
        if '.' in tcol:
            tcolget = tcol.split('.')
        else:
            tcolget = [tcol]
        times = _dict_get(lcdict, tcolget)

        if '.' in mcol:
            mcolget = mcol.split('.')
        else:
            mcolget = [mcol]
        mags = _dict_get(lcdict, mcolget)

        if '.' in ecol:
            ecolget = ecol.split('.')
        else:
            ecolget = [ecol]
        errs = _dict_get(lcdict, ecolget)

        
        if normfunc is None:
            ntimes, nmags = normalize_magseries(
                times, mags,
                magsarefluxes=magsarefluxes
            )

            times, mags, errs = ntimes, nmags, errs

        
        binned = time_bin_magseries_with_errs(times,
                                              mags,
                                              errs,
                                              binsize=binsizesec,
                                              minbinelems=minbinelems)

        
        lcdict['binned'][mcol] = {'times':binned['binnedtimes'],
                                  'mags':binned['binnedmags'],
                                  'errs':binned['binnederrs'],
                                  'nbins':binned['nbins'],
                                  'timebins':binned['jdbins'],
                                  'binsizesec':binsizesec}


    
    

    if outdir is None:
        outdir = os.path.dirname(lcfile)

    outfile = os.path.join(outdir, '%s-binned%.1fsec-%s.pkl' %
                           (squeeze(lcdict['objectid']).replace(' ','-'),
                            binsizesec, lcformat))

    with open(outfile, 'wb') as outfd:
        pickle.dump(lcdict, outfd, protocol=pickle.HIGHEST_PROTOCOL)

    return outfile



def timebinlc_worker(task):
    


    lcfile, binsizesec, kwargs = task

    try:
        binnedlc = timebinlc(lcfile, binsizesec, **kwargs)
        LOGINFO('%s binned using %s sec -> %s OK' %
                (lcfile, binsizesec, binnedlc))
        return binnedlc
    except Exception as e:
        LOGEXCEPTION('failed to bin %s using binsizesec = %s' % (lcfile,
                                                                 binsizesec))
        return None



def parallel_timebin(lclist,
                     binsizesec,
                     maxobjects=None,
                     outdir=None,
                     lcformat='hat-sql',
                     lcformatdir=None,
                     timecols=None,
                     magcols=None,
                     errcols=None,
                     minbinelems=7,
                     nworkers=NCPUS,
                     maxworkertasks=1000):
    


    if outdir and not os.path.exists(outdir):
        os.mkdir(outdir)

    if maxobjects is not None:
        lclist = lclist[:maxobjects]

    tasks = [(x, binsizesec, {'outdir':outdir,
                              'lcformat':lcformat,
                              'lcformatdir':lcformatdir,
                              'timecols':timecols,
                              'magcols':magcols,
                              'errcols':errcols,
                              'minbinelems':minbinelems}) for x in lclist]

    pool = mp.Pool(nworkers, maxtasksperchild=maxworkertasks)
    results = pool.map(timebinlc_worker, tasks)
    pool.close()
    pool.join()

    resdict = {os.path.basename(x):y for (x,y) in zip(lclist, results)}

    return resdict



def parallel_timebin_lcdir(lcdir,
                           binsizesec,
                           maxobjects=None,
                           outdir=None,
                           lcformat='hat-sql',
                           lcformatdir=None,
                           timecols=None,
                           magcols=None,
                           errcols=None,
                           minbinelems=7,
                           nworkers=NCPUS,
                           maxworkertasks=1000):
    

    try:
        formatinfo = get_lcformat(lcformat,
                                  use_lcformat_dir=lcformatdir)
        if formatinfo:
            (fileglob, readerfunc,
             dtimecols, dmagcols, derrcols,
             magsarefluxes, normfunc) = formatinfo
        else:
            LOGERROR(""can't figure out the light curve format"")
            return None
    except Exception as e:
        LOGEXCEPTION(""can't figure out the light curve format"")
        return None

    lclist = sorted(glob.glob(os.path.join(lcdir, fileglob)))

    return parallel_timebin(lclist,
                            binsizesec,
                            maxobjects=maxobjects,
                            outdir=outdir,
                            lcformat=lcformat,
                            timecols=timecols,
                            magcols=magcols,
                            errcols=errcols,
                            minbinelems=minbinelems,
                            nworkers=nworkers,
                            maxworkertasks=maxworkertasks)
","

import os, sys, json
from ..plugins.common import plugins_get_mgr
from dgitcore import exceptions





__all__ = ['transform']




def instantiate(repo, name=None, filename=None):
    


    default_transformers = repo.options.get('transformer', {})

    
    
    
    transformers = {}
    if name is not None:
        
        if name in default_transformers:
            transformers = {
                name : default_transformers[name]
            }
        else:
            transformers = {
                name : {
                    'files': [],
                }
            }
    else:
        transformers = default_transformers

    
    
    
    
    input_matching_files = None
    if filename is not None:
        input_matching_files = repo.find_matching_files([filename])

    for t in transformers:
        for k in transformers[t]:
            if ""files"" not in k:
                continue
            if k == ""files"" and input_matching_files is not None:
                
                transformers[t][k] = input_matching_files
            else:
                
                if transformers[t][k] is None or len(transformers[t][k]) == 0:
                    transformers[t][k] = []
                else:
                    matching_files = repo.find_matching_files(transformers[t][k])
                    transformers[t][k] = matching_files

    return transformers

def transform(repo,
              name=None,
              filename=None,
              force=False,
              args=[]):
    

    mgr = plugins_get_mgr()

    
    specs = instantiate(repo, name, filename)

    
    allresults = []
    for s in specs:
        keys = mgr.search(what='transformer',name=s)['transformer']
        for k in keys:
            t = mgr.get_by_key('transformer', k)
            result = t.evaluate(repo,
                                specs[s],
                                force,
                                args)
            allresults.extend(result)

    return allresults
","




















from __future__ import absolute_import, division

__docformat__ = ""restructuredtext en""

from .streambase import StreamBase
from .jid import JID
from .settings import XMPPSettings
from .constants import STANZA_CLIENT_NS

class ClientStream(StreamBase):
    

    
    def __init__(self, jid, stanza_route, handlers, settings = None):
        

        if handlers is None:
            handlers = []
        if settings is None:
            settings = XMPPSettings()
        if ""resource"" not in settings:
            settings[""resource""] = jid.resource
        StreamBase.__init__(self, STANZA_CLIENT_NS, stanza_route,
                                                        handlers, settings)
        self.me = JID(jid.local, jid.domain)

    def initiate(self, transport, to = None):
        

        if to is None:
            to = JID(self.me.domain)
        return StreamBase.initiate(self, transport, to)

    def receive(self, transport, myname = None):
        

        if myname is None:
            myname = JID(self.me.domain)
        return StreamBase.receive(self, transport, myname)

    def fix_out_stanza(self, stanza):
        

        StreamBase.fix_out_stanza(self, stanza)
        if self.initiator:
            if stanza.from_jid:
                stanza.from_jid = None
        else:
            if not stanza.from_jid:
                stanza.from_jid = self.me

    def fix_in_stanza(self, stanza):
        

        StreamBase.fix_in_stanza(self, stanza)
        if not self.initiator:
            if stanza.from_jid != self.peer:
                stanza.set_from(self.peer)


","from . import WhenIWork
from .models import Location


class Locations(WhenIWork):
    def get_location(self, location_id):
        

        url = ""/2/locations/%s"" % location_id

        return self.location_from_json(self._get_resource(url)[""location""])

    def get_locations(self):
        

        url = ""/2/locations""

        data = self._get_resource(url)
        locations = []
        for entry in data['locations']:
            locations.append(self.location_from_json(entry))

        return locations

    @staticmethod
    def location_from_json(data):
        location = Location()
        location.location_id = data[""id""]
        location.name = data[""name""]
        location.address = data[""address""]
        return location
","from __future__ import print_function, division, absolute_import

import numpy as np
import six.moves as sm

from .. import imgaug as ia


class HeatmapsOnImage(object):
    


    def __init__(self, arr, shape, min_value=0.0, max_value=1.0):
        

        ia.do_assert(ia.is_np_array(arr), ""Expected numpy array as heatmap input array, got type %s"" % (type(arr),))
        
        ia.do_assert(arr.shape[0] > 0 and arr.shape[1] > 0,
                     ""Expected numpy array as heatmap with height and width greater than 0, got shape %s."" % (
                         arr.shape,))
        ia.do_assert(arr.dtype.type in [np.float32],
                     ""Heatmap input array expected to be of dtype float32, got dtype %s."" % (arr.dtype,))
        ia.do_assert(arr.ndim in [2, 3], ""Heatmap input array must be 2d or 3d, got shape %s."" % (arr.shape,))
        ia.do_assert(len(shape) in [2, 3],
                     ""Argument 'shape' in HeatmapsOnImage expected to be 2d or 3d, got shape %s."" % (shape,))
        ia.do_assert(min_value < max_value)
        if np.min(arr.flat[0:50]) < min_value - np.finfo(arr.dtype).eps \
                or np.max(arr.flat[0:50]) > max_value + np.finfo(arr.dtype).eps:
            import warnings
            warnings.warn(
                (""Value range of heatmap was chosen to be (%.8f, %.8f), but ""
                 ""found actual min/max of (%.8f, %.8f). Array will be ""
                 ""clipped to chosen value range."") % (
                    min_value, max_value, np.min(arr), np.max(arr)))
            arr = np.clip(arr, min_value, max_value)

        if arr.ndim == 2:
            arr = arr[..., np.newaxis]
            self.arr_was_2d = True
        else:
            self.arr_was_2d = False

        eps = np.finfo(np.float32).eps
        min_is_zero = 0.0 - eps < min_value < 0.0 + eps
        max_is_one = 1.0 - eps < max_value < 1.0 + eps
        if min_is_zero and max_is_one:
            self.arr_0to1 = arr
        else:
            self.arr_0to1 = (arr - min_value) / (max_value - min_value)

        
        
        self.shape = shape

        self.min_value = min_value
        self.max_value = max_value

    def get_arr(self):
        

        if self.arr_was_2d and self.arr_0to1.shape[2] == 1:
            arr = self.arr_0to1[:, :, 0]
        else:
            arr = self.arr_0to1

        eps = np.finfo(np.float32).eps
        min_is_zero = 0.0 - eps < self.min_value < 0.0 + eps
        max_is_one = 1.0 - eps < self.max_value < 1.0 + eps
        if min_is_zero and max_is_one:
            return np.copy(arr)
        else:
            diff = self.max_value - self.min_value
            return self.min_value + diff * arr

    
    
    

    def draw(self, size=None, cmap=""jet""):
        

        heatmaps_uint8 = self.to_uint8()
        heatmaps_drawn = []

        for c in sm.xrange(heatmaps_uint8.shape[2]):
            
            heatmap_c = heatmaps_uint8[..., c:c+1]

            if size is not None:
                heatmap_c_rs = ia.imresize_single_image(heatmap_c, size, interpolation=""nearest"")
            else:
                heatmap_c_rs = heatmap_c
            heatmap_c_rs = np.squeeze(heatmap_c_rs).astype(np.float32) / 255.0

            if cmap is not None:
                
                import matplotlib.pyplot as plt

                cmap_func = plt.get_cmap(cmap)
                heatmap_cmapped = cmap_func(heatmap_c_rs)
                heatmap_cmapped = np.delete(heatmap_cmapped, 3, 2)
            else:
                heatmap_cmapped = np.tile(heatmap_c_rs[..., np.newaxis], (1, 1, 3))

            heatmap_cmapped = np.clip(heatmap_cmapped * 255, 0, 255).astype(np.uint8)

            heatmaps_drawn.append(heatmap_cmapped)
        return heatmaps_drawn

    def draw_on_image(self, image, alpha=0.75, cmap=""jet"", resize=""heatmaps""):
        

        
        ia.do_assert(image.ndim == 3)
        ia.do_assert(image.shape[2] == 3)
        ia.do_assert(image.dtype.type == np.uint8)

        ia.do_assert(0 - 1e-8 <= alpha <= 1.0 + 1e-8)
        ia.do_assert(resize in [""heatmaps"", ""image""])

        if resize == ""image"":
            image = ia.imresize_single_image(image, self.arr_0to1.shape[0:2], interpolation=""cubic"")

        heatmaps_drawn = self.draw(
            size=image.shape[0:2] if resize == ""heatmaps"" else None,
            cmap=cmap
        )

        mix = [
            np.clip((1-alpha) * image + alpha * heatmap_i, 0, 255).astype(np.uint8)
            for heatmap_i
            in heatmaps_drawn
        ]

        return mix

    def invert(self):
        

        arr_inv = HeatmapsOnImage.from_0to1(1 - self.arr_0to1, shape=self.shape, min_value=self.min_value,
                                            max_value=self.max_value)
        arr_inv.arr_was_2d = self.arr_was_2d
        return arr_inv

    def pad(self, top=0, right=0, bottom=0, left=0, mode=""constant"", cval=0.0):
        

        arr_0to1_padded = ia.pad(self.arr_0to1, top=top, right=right, bottom=bottom, left=left, mode=mode, cval=cval)
        return HeatmapsOnImage.from_0to1(arr_0to1_padded, shape=self.shape, min_value=self.min_value,
                                         max_value=self.max_value)

    def pad_to_aspect_ratio(self, aspect_ratio, mode=""constant"", cval=0.0, return_pad_amounts=False):
        

        arr_0to1_padded, pad_amounts = ia.pad_to_aspect_ratio(self.arr_0to1, aspect_ratio=aspect_ratio, mode=mode,
                                                              cval=cval, return_pad_amounts=True)
        heatmaps = HeatmapsOnImage.from_0to1(arr_0to1_padded, shape=self.shape, min_value=self.min_value,
                                             max_value=self.max_value)
        if return_pad_amounts:
            return heatmaps, pad_amounts
        else:
            return heatmaps

    def avg_pool(self, block_size):
        

        arr_0to1_reduced = ia.avg_pool(self.arr_0to1, block_size, cval=0.0)
        return HeatmapsOnImage.from_0to1(arr_0to1_reduced, shape=self.shape, min_value=self.min_value,
                                         max_value=self.max_value)

    def max_pool(self, block_size):
        

        arr_0to1_reduced = ia.max_pool(self.arr_0to1, block_size)
        return HeatmapsOnImage.from_0to1(arr_0to1_reduced, shape=self.shape, min_value=self.min_value,
                                         max_value=self.max_value)

    @ia.deprecated(alt_func=""HeatmapsOnImage.resize()"",
                   comment=""resize() has the exactly same interface."")
    def scale(self, *args, **kwargs):
        return self.resize(*args, **kwargs)

    def resize(self, sizes, interpolation=""cubic""):
        

        arr_0to1_resized = ia.imresize_single_image(self.arr_0to1, sizes, interpolation=interpolation)

        
        
        
        arr_0to1_resized = np.clip(arr_0to1_resized, 0.0, 1.0)

        return HeatmapsOnImage.from_0to1(arr_0to1_resized, shape=self.shape, min_value=self.min_value,
                                         max_value=self.max_value)

    def to_uint8(self):
        

        
        
        arr_0to255 = np.clip(np.round(self.arr_0to1 * 255), 0, 255)
        arr_uint8 = arr_0to255.astype(np.uint8)
        return arr_uint8

    @staticmethod
    def from_uint8(arr_uint8, shape, min_value=0.0, max_value=1.0):
        

        arr_0to1 = arr_uint8.astype(np.float32) / 255.0
        return HeatmapsOnImage.from_0to1(arr_0to1, shape, min_value=min_value, max_value=max_value)

    @staticmethod
    def from_0to1(arr_0to1, shape, min_value=0.0, max_value=1.0):
        

        heatmaps = HeatmapsOnImage(arr_0to1, shape, min_value=0.0, max_value=1.0)
        heatmaps.min_value = min_value
        heatmaps.max_value = max_value
        return heatmaps

    @classmethod
    def change_normalization(cls, arr, source, target):
        

        ia.do_assert(ia.is_np_array(arr))

        if isinstance(source, HeatmapsOnImage):
            source = (source.min_value, source.max_value)
        else:
            ia.do_assert(isinstance(source, tuple))
            ia.do_assert(len(source) == 2)
            ia.do_assert(source[0] < source[1])

        if isinstance(target, HeatmapsOnImage):
            target = (target.min_value, target.max_value)
        else:
            ia.do_assert(isinstance(target, tuple))
            ia.do_assert(len(target) == 2)
            ia.do_assert(target[0] < target[1])

        
        
        
        eps = np.finfo(arr.dtype).eps
        mins_same = source[0] - 10*eps < target[0] < source[0] + 10*eps
        maxs_same = source[1] - 10*eps < target[1] < source[1] + 10*eps
        if mins_same and maxs_same:
            return np.copy(arr)

        min_source, max_source = source
        min_target, max_target = target

        diff_source = max_source - min_source
        diff_target = max_target - min_target

        arr_0to1 = (arr - min_source) / diff_source
        arr_target = min_target + arr_0to1 * diff_target

        return arr_target

    def copy(self):
        

        return self.deepcopy()

    def deepcopy(self):
        

        return HeatmapsOnImage(self.get_arr(), shape=self.shape, min_value=self.min_value, max_value=self.max_value)
","



















from __future__ import absolute_import, division

__docformat__=""restructuredtext en""

raise ImportError(""{0} is not yet rewritten for PyXMPP2"".format(__name__))

import warnings

import libxml2

from ..xmlextra import common_doc,common_root
from ..jid import JID
from .. import cache

from ..utils import to_utf8
from ..objects import StanzaPayloadWrapperObject
from ..exceptions import ProtocolError

DISCO_NS=""http://jabber.org/protocol/disco""
DISCO_ITEMS_NS=DISCO_NS+""
DISCO_INFO_NS=DISCO_NS+""

class DiscoItem(StanzaPayloadWrapperObject):
    

    def __init__(self,disco,xmlnode_or_jid,node=None,name=None,action=None):
        

        self.disco=disco
        if isinstance(xmlnode_or_jid,JID):
            if disco:
                self.xmlnode=disco.xmlnode.newChild(None,""item"",None)
            else:
                self.xmlnode=common_root.newChild(None,""item"",None)
                ns=self.xmlnode.newNs(DISCO_ITEMS_NS,None)
                self.xmlnode.setNs(ns)
            self.set_jid(xmlnode_or_jid)
            self.set_name(name)
            self.set_node(node)
            self.set_action(action)
        else:
            if disco is None:
                self.xmlnode=xmlnode_or_jid.copyNode(1)
            else:
                self.xmlnode=xmlnode_or_jid
            if name:
                self.set_name(name)
            if node:
                self.set_node(node)
            if action:
                self.set_action(action)
        self.xpath_ctxt=common_doc.xpathNewContext()
        self.xpath_ctxt.setContextNode(self.xmlnode)
        self.xpath_ctxt.xpathRegisterNs(""d"",DISCO_ITEMS_NS)

    def __del__(self):
        if self.disco is None:
            if self.xmlnode:
                self.xmlnode.unlinkNode()
                self.xmlnode.freeNode()
                self.xmlnode=None
        if self.xpath_ctxt:
            self.xpath_ctxt.xpathFreeContext()

    def __str__(self):
        return self.xmlnode.serialize()

    def remove(self):
        

        if self.disco is None:
            return
        self.xmlnode.unlinkNode()
        oldns=self.xmlnode.ns()
        ns=self.xmlnode.newNs(oldns.getContent(),None)
        self.xmlnode.replaceNs(oldns,ns)
        common_root.addChild(self.xmlnode())
        self.disco=None

    def get_name(self):
        

        name = self.xmlnode.prop(""name"")
        if name is None:
            return None
        return name.decode(""utf-8"")

    def set_name(self, name):
        

        if name is None:
            if self.xmlnode.hasProp(""name""):
                self.xmlnode.unsetProp(""name"")
            return
        name = unicode(name)
        self.xmlnode.setProp(""name"", name.encode(""utf-8""))

    name = property(get_name, set_name)

    def get_node(self):
        

        node = self.xmlnode.prop(""node"")
        if node is None:
            return None
        return node.decode(""utf-8"")

    def set_node(self,node):
        

        if node is None:
            if self.xmlnode.hasProp(""node""):
                self.xmlnode.unsetProp(""node"")
            return
        node = unicode(node)
        self.xmlnode.setProp(""node"", node.encode(""utf-8""))

    node = property(get_node, set_node)

    def get_action(self):
        

        action=self.xmlnode.prop(""action"")
        if action is None:
            return None
        return action.decode(""utf-8"")

    def set_action(self,action):
        

        if action is None:
            if self.xmlnode.hasProp(""action""):
                self.xmlnode.unsetProp(""action"")
            return
        if action not in (""remove"",""update""):
            raise ValueError(""Action must be 'update' or 'remove'"")
        action = unicode(action)
        self.xmlnode.setProp(""action"", action.encode(""utf-8""))

    action = property(get_action, set_action)

    def get_jid(self):
        

        jid = self.xmlnode.prop(""jid"")
        return JID( jid.decode(""utf-8"") )

    def set_jid(self,jid):
        

        self.xmlnode.setProp(""jid"", jid.as_unicode().encode(""utf-8""))

    jid = property(get_jid, set_jid)

class DiscoIdentity(StanzaPayloadWrapperObject):
    

    def __init__(self, disco, xmlnode_or_name, item_category=None, item_type=None, replace=False):
        

        self.disco=disco
        if disco and replace:
            old=disco.xpath_ctxt.xpathEval(""d:identity"")
            if old:
                for n in old:
                    n.unlinkNode()
                    n.freeNode()
        if isinstance(xmlnode_or_name,libxml2.xmlNode):
            if disco is None:
                self.xmlnode=xmlnode_or_name.copyNode(1)
            else:
                self.xmlnode=xmlnode_or_name
        elif not item_category:
            raise ValueError(""DiscoInfo requires category"")
        elif not item_type:
            raise ValueError(""DiscoInfo requires type"")
        else:
            if disco:
                self.xmlnode=disco.xmlnode.newChild(None,""identity"",None)
            else:
                self.xmlnode=common_root.newChild(None,""identity"",None)
                ns=self.xmlnode.newNs(DISCO_INFO_NS,None)
                self.xmlnode.setNs(ns)
            self.set_name(xmlnode_or_name)
            self.set_category(item_category)
            self.set_type(item_type)
        self.xpath_ctxt=common_doc.xpathNewContext()
        self.xpath_ctxt.setContextNode(self.xmlnode)
        self.xpath_ctxt.xpathRegisterNs(""d"",DISCO_INFO_NS)

    def __del__(self):
        if self.disco is None:
            if self.xmlnode:
                self.xmlnode.unlinkNode()
                self.xmlnode.freeNode()
                self.xmlnode=None
        if self.xpath_ctxt:
            self.xpath_ctxt.xpathFreeContext()

    def __str__(self):
        return self.xmlnode.serialize()

    def remove(self):
        

        if self.disco is None:
            return
        self.xmlnode.unlinkNode()
        oldns=self.xmlnode.ns()
        ns=self.xmlnode.newNs(oldns.getContent(),None)
        self.xmlnode.replaceNs(oldns,ns)
        common_root.addChild(self.xmlnode())
        self.disco=None

    def get_name(self):
        

        var = self.xmlnode.prop(""name"")
        if not var:
            var = """"
        return var.decode(""utf-8"")

    def set_name(self,name):
        

        if not name:
            raise ValueError(""name is required in DiscoIdentity"")
        name = unicode(name)
        self.xmlnode.setProp(""name"", name.encode(""utf-8""))

    name = property(get_name, set_name)

    def get_category(self):
        

        var = self.xmlnode.prop(""category"")
        if not var:
            var = ""?""
        return var.decode(""utf-8"")

    def set_category(self, category):
        

        if not category:
            raise ValueError(""Category is required in DiscoIdentity"")
        category = unicode(category)
        self.xmlnode.setProp(""category"", category.encode(""utf-8""))

    category = property(get_category, set_category)

    def get_type(self):
        

        item_type = self.xmlnode.prop(""type"")
        if not item_type:
            item_type = ""?""
        return item_type.decode(""utf-8"")

    def set_type(self, item_type):
        

        if not item_type:
            raise ValueError(""Type is required in DiscoIdentity"")
        item_type = unicode(item_type)
        self.xmlnode.setProp(""type"", item_type.encode(""utf-8""))

    type = property(get_type, set_type)

class DiscoItems(StanzaPayloadWrapperObject):
    

    def __init__(self,xmlnode_or_node=None):
        

        self.xmlnode=None
        self.xpath_ctxt=None
        if isinstance(xmlnode_or_node,libxml2.xmlNode):
            ns=xmlnode_or_node.ns()
            if ns.getContent() != DISCO_ITEMS_NS:
                raise ValueError(""Bad disco-items namespace"")
            self.xmlnode=xmlnode_or_node.docCopyNode(common_doc,1)
            common_root.addChild(self.xmlnode)
            self.ns=self.xmlnode.ns()
        else:
            self.xmlnode=common_root.newChild(None,""query"",None)
            self.ns=self.xmlnode.newNs(DISCO_ITEMS_NS,None)
            self.xmlnode.setNs(self.ns)
            self.set_node(xmlnode_or_node)
        self.xpath_ctxt=common_doc.xpathNewContext()
        self.xpath_ctxt.setContextNode(self.xmlnode)
        self.xpath_ctxt.xpathRegisterNs(""d"",DISCO_ITEMS_NS)

    def __del__(self):
        if self.xmlnode:
            self.xmlnode.unlinkNode()
            self.xmlnode.freeNode()
            self.xmlnode=None
        if self.xpath_ctxt:
            self.xpath_ctxt.xpathFreeContext()
            self.xpath_ctxt=None

    def get_node(self):
        

        node = self.xmlnode.prop(""node"")
        if not node:
            return None
        return node.decode(""utf-8"")

    def set_node(self, node):
        

        if node is None:
            if self.xmlnode.hasProp(""node""):
                self.xmlnode.unsetProp(""node"")
            return
        node = unicode(node)
        self.xmlnode.setProp(""node"", node.encode(""utf-8""))

    node = property(get_node, set_node)

    def get_items(self):
        

        ret=[]
        l=self.xpath_ctxt.xpathEval(""d:item"")
        if l is not None:
            for i in l:
                ret.append(DiscoItem(self, i))
        return ret

    def set_items(self, item_list):
        

        for item in self.items:
            item.remove()
        for item in item_list:
            try:
                self.add_item(item.jid,item.node,item.name,item.action)
            except AttributeError:
                self.add_item(*item)

    items = property(get_items, set_items, doc = ""List of `DiscoItems`"")

    def invalidate_items(self):
        

        warnings.warn(""DiscoItems.invalidate_items() is deprecated and not needed any more."", DeprecationWarning, stacklevel=1)

    def add_item(self,jid,node=None,name=None,action=None):
        

        return DiscoItem(self,jid,node,name,action)

    def has_item(self,jid,node=None):
        

        l=self.xpath_ctxt.xpathEval(""d:item"")
        if l is None:
            return False
        for it in l:
            di=DiscoItem(self,it)
            if di.jid==jid and di.node==node:
                return True
        return False

class DiscoInfo(StanzaPayloadWrapperObject):
    

    def __init__(self,xmlnode_or_node=None, parent=None, doc=None):
        

        self.xmlnode=None
        self.xpath_ctxt=None
        if not doc:
            doc=common_doc
        if not parent:
            parent=common_root
        if isinstance(xmlnode_or_node,libxml2.xmlNode):
            ns=xmlnode_or_node.ns()
            if ns.getContent() != DISCO_INFO_NS:
                raise ValueError(""Bad disco-info namespace"")
            self.xmlnode=xmlnode_or_node.docCopyNode(doc,1)
            parent.addChild(self.xmlnode)
        else:
            self.xmlnode=parent.newChild(None,""query"",None)
            self.ns=self.xmlnode.newNs(DISCO_INFO_NS,None)
            self.xmlnode.setNs(self.ns)
            self.set_node(xmlnode_or_node)

        self.xpath_ctxt=doc.xpathNewContext()
        self.xpath_ctxt.setContextNode(self.xmlnode)
        self.xpath_ctxt.xpathRegisterNs(""d"",DISCO_INFO_NS)

    def __del__(self):
        if self.xmlnode:
            self.xmlnode.unlinkNode()
            self.xmlnode.freeNode()
            self.xmlnode=None
        if self.xpath_ctxt:
            self.xpath_ctxt.xpathFreeContext()
            self.xpath_ctxt=None

    def get_node(self):
        


        node=self.xmlnode.prop(""node"")
        if not node:
            return None
        return node.decode(""utf-8"")

    def set_node(self,node):
        

        if node is None:
            if self.xmlnode.hasProp(""node""):
                self.xmlnode.unsetProp(""node"")
            return
        node = unicode(node)
        self.xmlnode.setProp(""node"", node.encode(""utf-8""))

    node = property(get_node, set_node)

    def get_features(self):
        

        l = self.xpath_ctxt.xpathEval(""d:feature"")
        ret = []
        for f in l:
            if f.hasProp(""var""):
                ret.append( f.prop(""var"").decode(""utf-8"") )
        return ret

    def set_features(self, features):
        

        for var in self.features:
            self.remove_feature(var)

        for var in features:
            self.add_feature(var)

    features = property(get_features, set_features)

    def has_feature(self,var):
        

        if not var:
            raise ValueError(""var is None"")
        if 
 not in var:
            expr=u""d:feature[@var='%s']"" % (var,)
        else:
            raise ValueError(""Invalid feature name"")

        l=self.xpath_ctxt.xpathEval(to_utf8(expr))
        if l:
            return True
        else:
            return False

    def invalidate_features(self):
        

        warnings.warn(""DiscoInfo.invalidate_features() is deprecated and not needed any more."", DeprecationWarning, stacklevel=1)

    def add_feature(self,var):
        

        if self.has_feature(var):
            return
        n=self.xmlnode.newChild(None, ""feature"", None)
        n.setProp(""var"", to_utf8(var))

    def remove_feature(self,var):
        

        if not var:
            raise ValueError(""var is None"")
        if 
 not in var:
            expr=""d:feature[@var='%s']"" % (var,)
        else:
            raise ValueError(""Invalid feature name"")

        l=self.xpath_ctxt.xpathEval(expr)
        if not l:
            return

        for f in l:
            f.unlinkNode()
            f.freeNode()

    def get_identities(self):
        

        ret=[]
        l=self.xpath_ctxt.xpathEval(""d:identity"")
        if l is not None:
            for i in l:
                ret.append(DiscoIdentity(self,i))
        return ret

    def set_identities(self,identities):
        

        for identity in self.identities:
            identity.remove()
        for identity in identities:
            try:
                self.add_identity(identity.name,identity.category,identity.type)
            except AttributeError:
                self.add_identity(*identity)

    identities = property(get_identities, set_identities)

    def identity_is(self,item_category,item_type=None):
        

        if not item_category:
            raise ValueError(""bad category"")
        if not item_type:
            type_expr=u""""
        elif 
 not in type:
            type_expr=u"" and @type='%s'"" % (item_type,)
        else:
            raise ValueError(""Invalid type name"")
        if 
 not in item_category:
            expr=u""d:identity[@category='%s'%s]"" % (item_category,type_expr)
        else:
            raise ValueError(""Invalid category name"")

        l=self.xpath_ctxt.xpathEval(to_utf8(expr))
        if l:
            return True
        else:
            return False

    def invalidate_identities(self):
        

        warnings.warn(""DiscoInfo.invalidate_identities() is deprecated and not needed any more."", DeprecationWarning, stacklevel=1)

    def add_identity(self,item_name,item_category=None,item_type=None):
        

        return DiscoIdentity(self,item_name,item_category,item_type)

class DiscoCacheFetcherBase(cache.CacheFetcher):
    

    stream=None
    disco_class=None
    def fetch(self):
        

        from ..iq import Iq
        jid,node = self.address
        iq = Iq(to_jid = jid, stanza_type = ""get"")
        disco = self.disco_class(node)
        iq.add_content(disco.xmlnode)
        self.stream.set_response_handlers(iq,self.__response, self.__error,
                self.__timeout)
        self.stream.send(iq)

    def __response(self,stanza):
        

        try:
            d=self.disco_class(stanza.get_query())
            self.got_it(d)
        except ValueError,e:
            self.error(e)

    def __error(self,stanza):
        

        try:
            self.error(stanza.get_error())
        except ProtocolError:
            from ..error import StanzaErrorNode
            self.error(StanzaErrorNode(""undefined-condition""))

    def __timeout(self,stanza):
        

        pass

def register_disco_cache_fetchers(cache_suite,stream):
    

    tmp=stream
    class DiscoInfoCacheFetcher(DiscoCacheFetcherBase):
        

        stream=tmp
        disco_class=DiscoInfo
    class DiscoItemsCacheFetcher(DiscoCacheFetcherBase):
        

        stream=tmp
        disco_class=DiscoItems
    cache_suite.register_fetcher(DiscoInfo,DiscoInfoCacheFetcher)
    cache_suite.register_fetcher(DiscoItems,DiscoItemsCacheFetcher)


","










import numpy            as np
import nibabel          as nib
import scipy.ndimage    as scn
from   collections      import OrderedDict

from   .check           import check_img_compatibility, repr_imgs, check_img
from   .read            import read_img, get_img_data, get_img_info
from   .mask            import binarise, load_mask
from   ..utils.strings  import search_list


def drain_rois(img):
    

    img_data = get_img_data(img)

    out = np.zeros(img_data.shape, dtype=img_data.dtype)

    krn_dim = [3] * img_data.ndim
    kernel  = np.ones(krn_dim, dtype=int)

    vals = np.unique(img_data)
    vals = vals[vals != 0]

    for i in vals:
        roi  = img_data == i
        hits = scn.binary_hit_or_miss(roi, kernel)
        roi[hits] = 0
        out[roi > 0] = i

    return out


def pick_rois(rois_img, roi_values, bg_val=0):
    

    img = read_img(rois_img)
    img_data = img.get_data()

    if bg_val == 0:
        out = np.zeros(img_data.shape, dtype=img_data.dtype)
    else:
        out = np.ones(img_data.shape, dtype=img_data.dtype) * bg_val

    for r in roi_values:
        out[img_data == r] = r

    return nib.Nifti2Image(out, affine=img.affine, header=img.header)


def largest_connected_component(volume):
    

    
    volume = np.asarray(volume)
    labels, num_labels = scn.label(volume)
    if not num_labels:
        raise ValueError('No non-zero values: no connected components found.')

    if num_labels == 1:
        return volume.astype(np.bool)

    label_count = np.bincount(labels.ravel().astype(np.int))
    
    label_count[0] = 0
    return labels == label_count.argmax()


def large_clusters_mask(volume, min_cluster_size):
    

    labels, num_labels = scn.label(volume)

    labels_to_keep = set([i for i in range(num_labels)
                         if np.sum(labels == i) >= min_cluster_size])

    clusters_mask = np.zeros_like(volume, dtype=int)
    for l in range(num_labels):
        if l in labels_to_keep:
            clusters_mask[labels == l] = 1

    return clusters_mask


def create_rois_mask(roislist, filelist):
    

    roifiles = []

    for roi in roislist:
        try:
            roi_file = search_list(roi, filelist)[0]
        except Exception as exc:
            raise Exception('Error creating list of roi files. \n {}'.format(str(exc)))
        else:
            roifiles.append(roi_file)

    return binarise(roifiles)


def get_unique_nonzeros(arr):
    

    rois = np.unique(arr)
    rois = rois[np.nonzero(rois)]
    rois.sort()

    return rois


def get_roilist_from_atlas(atlas_img):
    

    return get_unique_nonzeros(check_img(atlas_img).get_data())


def get_rois_centers_of_mass(vol):
    

    from scipy.ndimage.measurements import center_of_mass

    roisvals = np.unique(vol)
    roisvals = roisvals[roisvals != 0]

    rois_centers = OrderedDict()
    for r in roisvals:
        rois_centers[r] = center_of_mass(vol, vol, r)

    return rois_centers


def partition_timeseries(image, roi_img, mask_img=None, zeroe=True, roi_values=None, outdict=False):
    

    img  = read_img(image)
    rois = read_img(roi_img)

    
    check_img_compatibility(img, rois, only_check_3d=True)

    
    roi_data = rois.get_data()
    if roi_values is not None:
        for rv in roi_values:
            if not np.any(roi_data == rv):
                raise ValueError('Could not find value {} in rois_img {}.'.format(rv, repr_imgs(roi_img)))
    else:
        roi_values = get_unique_nonzeros(roi_data)

    
    if mask_img is None:
        mask_data = None
    else:
        mask = load_mask(mask_img)
        check_img_compatibility(img, mask, only_check_3d=True)
        mask_data = mask.get_data()

    
    if outdict:
        extract_data = _extract_timeseries_dict
    else:
        extract_data = _extract_timeseries_list

    
    try:
        return extract_data(img.get_data(), rois.get_data(), mask_data,
                            roi_values=roi_values, zeroe=zeroe)
    except:
        raise


def partition_volume(*args, **kwargs):
    

    return partition_timeseries(*args, **kwargs)


def _check_for_partition(datavol, roivol, maskvol=None):
    if datavol.ndim != 4 and datavol.ndim != 3:
        raise AttributeError('Expected a volume with 3 or 4 dimensions. '
                             '`datavol` has {} dimensions.'.format(datavol.ndim))

    if datavol.shape[:3] != roivol.shape:
        raise AttributeError('Expected a ROI volume with the same 3D shape as the timeseries volume. '
                             'In this case, datavol has shape {} and roivol {}.'.format(datavol.shape, roivol.shape))

    if maskvol is not None:
        if datavol.shape[:3] != maskvol.shape:
            raise AttributeError('Expected a mask volume with the same 3D shape as the timeseries volume. '
                                 'In this case, datavol has shape {} and maskvol {}.'.format(datavol.shape,
                                                                                             maskvol.shape))


def _partition_data(datavol, roivol, roivalue, maskvol=None, zeroe=True):
    

    if maskvol is not None:
        
        indices = (roivol == roivalue) * (maskvol > 0)
    else:
        
        indices = roivol == roivalue

    if datavol.ndim == 4:
        ts = datavol[indices, :]
    else:
        ts = datavol[indices]

    
    if zeroe:
        if datavol.ndim == 4:
            ts = ts[ts.sum(axis=1) != 0, :]

    return ts


def _extract_timeseries_dict(tsvol, roivol, maskvol=None, roi_values=None, zeroe=True):
    

    _check_for_partition(tsvol, roivol, maskvol)

    
    if roi_values is None:
        roi_values = get_unique_nonzeros(roivol)

    ts_dict = OrderedDict()
    for r in roi_values:
        ts = _partition_data(tsvol, roivol, r, maskvol, zeroe)

        if len(ts) == 0:
            ts = np.zeros(tsvol.shape[-1])

        ts_dict[r] = ts

    return ts_dict


def _extract_timeseries_list(tsvol, roivol, maskvol=None, roi_values=None, zeroe=True):
    

    _check_for_partition(tsvol, roivol, maskvol)

    if roi_values is None:
        roi_values = get_unique_nonzeros(roivol)

    ts_list = []
    for r in roi_values:
        ts = _partition_data(tsvol, roivol, r, maskvol, zeroe)

        if len(ts) == 0:
            ts = np.zeros(tsvol.shape[-1])

        ts_list.append(ts)

    return ts_list


def get_3D_from_4D(image, vol_idx=0):
    

    img      = check_img(image)
    hdr, aff = get_img_info(img)

    if len(img.shape) != 4:
        raise AttributeError('Volume in {} does not have 4 dimensions.'.format(repr_imgs(img)))

    if not 0 <= vol_idx < img.shape[3]:
        raise IndexError('IndexError: 4th dimension in volume {} has {} volumes, '
                         'not {}.'.format(repr_imgs(img), img.shape[3], vol_idx))

    img_data = img.get_data()
    new_vol  = img_data[:, :, :, vol_idx].copy()

    hdr.set_data_shape(hdr.get_data_shape()[:3])

    return new_vol, hdr, aff
","

















import phpserialize
from collections import OrderedDict
import pprint
import json

class ConvertPHP():
    

    def __init__(self):
        

        self.data_structure = ''
        self.built_in = set(['python', 'json'])

    def __str__(self):
        

        return self.data_structure

    def is_built_in(self, language):
        

        return language in self.built_in

    
    lang_specific_values = {'php': { 
                                    'True'  : 'true',
                                    'False' : 'false',
                                    'None'  : 'null'},
                            'javascript' : {
                                    'True' : 'true',
                                    'False' : 'false',
                                    'None' : 'null'},
                            'ocaml' : {
                                    'True' : 'true',
                                    'False': 'false'}}

    
    outer_templates = {'php' : 'array (\n%s\n);',
                       'javascript' : 'var jsObject = {\n%s\n}',
                       'ocaml' : 'let map = [|\n%s\n|] ;;'}

    def get_built_in(self, language, level, data):
        

        
        pp = pprint.PrettyPrinter(indent=level)

        lookup = {'python' : pp.pformat(data),
                  'json' : str(json.dumps(data, sort_keys=True, indent=level, separators=(',', ': ')))}

        self.data_structure = lookup[language]

    def get_inner_template(self, language, template_type, indentation, key, val):
        

        
        inner_templates = {'php' : {
                                'iterable' : '%s%s => array \n%s( \n%s%s),\n' % (indentation, key, indentation, val, indentation),
                                'singular' : '%s%s => %s, \n' % (indentation, key, val) },
                           'javascript' : {
                                'iterable' : '%s%s : {\n%s\n%s},\n' % (indentation, key, val, indentation),
                                'singular' : '%s%s: %s,\n' % (indentation, key, val)},
                           'ocaml' : { 
                                'iterable' : '%s[| (%s, (\n%s\n%s))|] ;;\n' % (indentation, key, val, indentation),
                                'singular' : '%s(%s, %s);\n' % (indentation, key, val)}}

        return inner_templates[language][template_type]

    def translate_val(self, language, value):
        

        return self.lang_specific_values[language][value]


    def is_iterable(self, data):
        

        try:
            iterate = iter(data)
            return True
        except:
            return False 


    def translate_array(self, string, language, level=3, retdata=False):
        

        language = language.lower()
        assert self.is_built_in(language) or language in self.outer_templates, \
            ""Sorry, "" + language + "" is not a supported language.""

        
        data = phpserialize.loads(bytes(string, 'utf-8'), array_hook=list, decode_strings=True)

        
        
        if self.is_built_in(language):
            self.get_built_in(language, level, data) 
            print(self)
            return self.data_structure if retdata else None

        
        def loop_print(iterable, level=3):
            

            retval = ''
            indentation = ' ' * level

            
            if not self.is_iterable(iterable) or isinstance(iterable, str):
                non_iterable = str(iterable)
                return str(non_iterable)
             
            
            for item in iterable:
                
                if isinstance(item, tuple) and len(item) == 2:
                    
                    key = item[0]
                    val = loop_print(item[1], level=level+3)
            
                    
                    val = self.translate_val(language, val) if language in self.lang_specific_values \
                          and val in self.lang_specific_values[language] else val
     
                    
                    
                    key = str(key) if isinstance(key, int) else '\'' + str(key) + '\''

                    
                    needs_unpacking = hasattr(item[0],'__iter__') == False \
                                      and hasattr(item[1],'__iter__') == True 

                    
                    if needs_unpacking:
                        retval += self.get_inner_template(language, 'iterable', indentation, key, val)
                    
                    else:
                        
                        
                        val = str(val) if val.isdigit() or val in self.lang_specific_values[language].values() else '\'' + str(val) + '\''

                        retval += self.get_inner_template(language, 'singular', indentation, key, val) 

            return retval
    
        
        self.data_structure = self.outer_templates[language] % (loop_print(data))
        print(self)
        return self.data_structure if retdata else None

","
from __future__ import division, absolute_import
















from ..task import Task
from ..exceptions import WorkflowException
from .base import TaskSpec
from ..operators import valueof


class Join(TaskSpec):

    


    def __init__(self,
                 wf_spec,
                 name,
                 split_task=None,
                 threshold=None,
                 cancel=False,
                 **kwargs):
        

        super(Join, self).__init__(wf_spec, name, **kwargs)
        self.split_task = split_task
        self.threshold = threshold
        self.cancel_remaining = cancel

    def _branch_is_complete(self, my_task):
        
        
        skip = None
        for task in Task.Iterator(my_task, my_task.NOT_FINISHED_MASK):
            
            if skip is not None and task._is_descendant_of(skip):
                continue
            if task.task_spec == self:
                skip = task
                continue
            return False
        return True

    def _branch_may_merge_at(self, task):
        for child in task:
            
            if child.triggered:
                continue
            
            if child.task_spec == self:
                return True
            
            
            
            if not child._is_definite() \
                    and len(child.task_spec.outputs) > len(child.children):
                return True
        return False

    def _check_threshold_unstructured(self, my_task, force=False):
        
        threshold = valueof(my_task, self.threshold)
        if threshold is None:
            threshold = len(self.inputs)

        
        tasks = []
        for input in self.inputs:
            for task in my_task.workflow.task_tree:
                if task.thread_id != my_task.thread_id:
                    continue
                if task.task_spec != input:
                    continue
                tasks.append(task)

        
        waiting_tasks = []
        completed = 0
        for task in tasks:
            if task.parent is None or task._has_state(Task.COMPLETED):
                completed += 1
            else:
                waiting_tasks.append(task)

        
        return force or completed >= threshold, waiting_tasks

    def _check_threshold_structured(self, my_task, force=False):
        
        
        split_task = my_task._find_ancestor_from_name(self.split_task)
        if split_task is None:
            msg = 'Join with %s, which was not reached' % self.split_task
            raise WorkflowException(self, msg)
        tasks = split_task.task_spec._get_activated_tasks(split_task, my_task)

        
        threshold = valueof(my_task, self.threshold)
        if threshold is None:
            threshold = len(tasks)

        
        waiting_tasks = []
        completed = 0
        for task in tasks:
            
            task.task_spec._predict(task)

            if not self._branch_may_merge_at(task):
                completed += 1
            elif self._branch_is_complete(task):
                completed += 1
            else:
                waiting_tasks.append(task)

        
        return force or completed >= threshold, waiting_tasks

    def _start(self, my_task, force=False):
        

        
        if my_task._has_state(Task.COMPLETED):
            return True, None
        if my_task._has_state(Task.READY):
            return True, None

        
        if self.split_task is None:
            return self._check_threshold_unstructured(my_task, force)
        return self._check_threshold_structured(my_task, force)

    def _update_hook(self, my_task):
        
        may_fire, waiting_tasks = self._start(my_task)
        if not may_fire:
            my_task._set_state(Task.WAITING)
            return

        
        
        if self.cancel_remaining:
            for task in waiting_tasks:
                task.cancel()

        
        
        
        
        my_task._ready()

        
        self._do_join(my_task)

    def _do_join(self, my_task):
        
        
        
        
        
        
        
        
        
        
        
        
        if self.split_task:
            split_task = my_task.workflow.get_task_spec_from_name(
                self.split_task)
            split_task = my_task._find_ancestor(split_task)
        else:
            split_task = my_task.workflow.task_tree

        
        
        
        
        
        last_changed = None
        thread_tasks = []
        for task in split_task._find_any(self):
            
            if task.thread_id != my_task.thread_id:
                continue
            
            if self.split_task and task._is_descendant_of(my_task):
                continue

            
            thread_tasks.append(task)

            
            
            changed = task.parent.last_state_change
            if last_changed is None \
                    or changed > last_changed.parent.last_state_change:
                last_changed = task

        
        
        
        
        
        for task in thread_tasks:
            if task == last_changed:
                self.entered_event.emit(my_task.workflow, my_task)
                task._ready()
            else:
                task.state = Task.COMPLETED
                task._drop_children()

    def _on_trigger(self, my_task):
        

        for task in my_task.workflow.task_tree._find_any(self):
            if task.thread_id != my_task.thread_id:
                continue
            self._do_join(task)

    def serialize(self, serializer):
        return serializer.serialize_join(self)

    @classmethod
    def deserialize(self, serializer, wf_spec, s_state):
        return serializer.deserialize_join(wf_spec, s_state)
","



















from __future__ import absolute_import, division

__docformat__ = ""restructuredtext en""

import inspect
import sys
import logging
import glib
import functools

from .interfaces import HandlerReady, PrepareAgain
from .base import MainLoopBase

logger = logging.getLogger(""pyxmpp2.mainloop.glib"")

def hold_exception(method):
    

    @functools.wraps(method)
    def wrapper(self, *args, **kwargs):
        

        
        try:
            return method(self, *args, **kwargs)
        except Exception:
            if self.exc_info:
                raise
            if not self._stack:
                logger.debug('@hold_exception wrapped method {0!r} called'
                            ' from outside of the main loop'.format(method))
                raise
            self.exc_info = sys.exc_info()
            logger.debug(u""exception in glib main loop callback:"",
                                                exc_info = self.exc_info)
            
            main_loop = self._stack[-1]
            if main_loop is not None:
                main_loop.quit()
            return False
    return wrapper

class GLibMainLoop(MainLoopBase):
    

    
    def __init__(self, settings = None, handlers = None):
        self._unprepared_handlers = {}
        self._io_sources = {}
        self._timer_sources = {}
        self._prepare_sources = {}
        self._stack = []
        self.exc_info = None
        self._anything_done = False
        self._unprepared_pending = set()
        MainLoopBase.__init__(self, settings, handlers)

    def __del__(self):
        for tag in self._prepare_sources.values():
            glib.source_remove(tag)
        for tag in self._io_sources.values():
            glib.source_remove(tag)
        for tag in self._timer_sources.values():
            glib.source_remove(tag)

    def _add_io_handler(self, handler):
        

        self._unprepared_handlers[handler] = None
        self._configure_io_handler(handler)

    def _configure_io_handler(self, handler):
        

        if self.check_events():
            return
        if handler in self._unprepared_handlers:
            old_fileno = self._unprepared_handlers[handler]
            prepared = self._prepare_io_handler(handler)
        else:
            old_fileno = None
            prepared = True
        fileno = handler.fileno()
        if old_fileno is not None and fileno != old_fileno:
            tag = self._io_sources.pop(handler, None)
            if tag is not None:
                glib.source_remove(tag)
        if not prepared:
            self._unprepared_handlers[handler] = fileno
        if fileno is None:
            logger.debug("" {0!r}.fileno() is None, not polling""
                                                    .format(handler))
            return
        events = 0
        if handler.is_readable():
            logger.debug("" {0!r} readable"".format(handler))
            events |= glib.IO_IN | glib.IO_ERR
        if handler.is_writable():
            logger.debug("" {0!r} writable"".format(handler))
            events |= glib.IO_OUT | glib.IO_HUP | glib.IO_ERR
        if events:
            logger.debug("" registering {0!r} handler fileno {1} for""
                            "" events {2}"".format(handler, fileno, events))
            glib.io_add_watch(fileno, events, self._io_callback, handler)

    @hold_exception
    def _io_callback(self, fileno, condition, handler):
        

        
        self._anything_done = True
        logger.debug(""_io_callback called for {0!r}, cond: {1}"".format(handler,
                                                                    condition))
        try:
            if condition & glib.IO_HUP:
                handler.handle_hup()
            if condition & glib.IO_IN:
                handler.handle_read()
            elif condition & glib.IO_ERR:
                handler.handle_err()
            if condition & glib.IO_OUT:
                handler.handle_write()
            if self.check_events():
                return False
        finally:
            self._io_sources.pop(handler, None)
            self._configure_io_handler(handler)
            self._prepare_pending()
        return False

    def _prepare_io_handler(self, handler):
        

        logger.debug("" preparing handler: {0!r}"".format(handler))
        self._unprepared_pending.discard(handler)
        ret = handler.prepare()
        logger.debug(""   prepare result: {0!r}"".format(ret))
        if isinstance(ret, HandlerReady):
            del self._unprepared_handlers[handler]
            prepared = True
        elif isinstance(ret, PrepareAgain):
            if ret.timeout == 0:
                tag = glib.idle_add(self._prepare_io_handler_cb, handler)
                self._prepare_sources[handler] = tag
            elif ret.timeout is not None:
                timeout = ret.timeout
                timeout = int(timeout * 1000)
                if not timeout:
                    timeout = 1
                tag = glib.timeout_add(timeout, self._prepare_io_handler_cb,
                                                                    handler)
                self._prepare_sources[handler] = tag
            else:
                self._unprepared_pending.add(handler)
            prepared = False
        else:
            raise TypeError(""Unexpected result type from prepare()"")
        return prepared

    def _prepare_pending(self):
        

        if not self._unprepared_pending:
            return
        for handler in list(self._unprepared_pending):
            self._configure_io_handler(handler)
        self.check_events()

    @hold_exception
    def _prepare_io_handler_cb(self, handler):
        

        self._anything_done = True
        logger.debug(""_prepar_io_handler_cb called for {0!r}"".format(handler))
        self._configure_io_handler(handler)
        self._prepare_sources.pop(handler, None)
        return False

    def _remove_io_handler(self, handler):
        

        if handler in self._unprepared_handlers:
            del self._unprepared_handlers[handler]
        tag = self._prepare_sources.pop(handler, None)
        if tag is not None:
            glib.source_remove(tag)
        tag = self._io_sources.pop(handler, None)
        if tag is not None:
            glib.source_remove(tag)

    def _add_timeout_handler(self, handler):
        

        
        for dummy, method in inspect.getmembers(handler, callable):
            if not hasattr(method, ""_pyxmpp_timeout""):
                continue
            tag = glib.timeout_add(int(method._pyxmpp_timeout * 1000),
                                                self._timeout_cb, method)
            self._timer_sources[method] = tag

    def _remove_timeout_handler(self, handler):
        

        for dummy, method in inspect.getmembers(handler, callable):
            if not hasattr(method, ""_pyxmpp_timeout""):
                continue
            tag = self._timer_sources.pop(method, None)
            if tag is not None:
                glib.source_remove(tag)

    @hold_exception
    def _timeout_cb(self, method):
        

        self._anything_done = True
        logger.debug(""_timeout_cb() called for: {0!r}"".format(method))
        result = method()
        
        rec = method._pyxmpp_recurring
        if rec:
            self._prepare_pending()
            return True

        if rec is None and result is not None:
            logger.debug("" auto-recurring, restarting in {0} s""
                                                            .format(result))
            tag = glib.timeout_add(int(result * 1000), self._timeout_cb, method)
            self._timer_sources[method] = tag
        else:
            self._timer_sources.pop(method, None)
        self._prepare_pending()
        return False

    def loop(self, timeout = None):
        main_loop = glib.MainLoop()
        self._stack.append(main_loop)
        try:
            self._prepare_pending()
            if timeout is None:
                logger.debug(""Calling main_loop.run()"")
                main_loop.run()
                logger.debug(""..main_loop.run() exited"")
            else:
                tag = glib.timeout_add(int(timeout * 1000),
                                            self._loop_timeout_cb, main_loop)
                try:
                    logger.debug(""Calling main_loop.run()"")
                    main_loop.run()
                    logger.debug(""..main_loop.run() exited"")
                finally:
                    glib.source_remove(tag)
        finally:
            self._stack.pop()
        if self.exc_info:
            (exc_type, exc_value, ext_stack), self.exc_info = (self.exc_info,
                                                                        None)
            raise exc_type, exc_value, ext_stack

    def loop_iteration(self, timeout = 1):
        self._stack.append(None)
        try:
            if self.check_events():
                return
            self._prepare_pending()
            def dummy_cb():
                ""Dummy callback function to force event if none are pending.""
                self._anything_done = True
                logger.debug(""Dummy timeout func called"")
                return True
            self._anything_done = False
            tag = None
            logger.debug(""Calling main_context_default().iteration()"")
            while not self._anything_done:
                if not glib.main_context_default().pending() and not tag:
                    tag = glib.timeout_add(int(timeout * 1000), dummy_cb)
                glib.main_context_default().iteration(True)
            if tag:
                glib.source_remove(tag)
            logger.debug(""..main_context_default().iteration() exited"")
        finally:
            self._stack.pop()
        if self.exc_info:
            (exc_type, exc_value, ext_stack), self.exc_info = (self.exc_info,
                                                                        None)
            raise exc_type, exc_value, ext_stack

    def _loop_timeout_cb(self, main_loop):
        

        self._anything_done = True
        logger.debug(""_loop_timeout_cb() called"")
        main_loop.quit()

    def check_events(self):
        result = MainLoopBase.check_events(self)
        if result:
            main_loop = self._stack[-1]
            if main_loop:
                main_loop.quit()
        return result

","





















import argparse
import atexit
import base64
import functools
import json
import os
import random
import signal
import string
import subprocess
import sys
import stat
import threading
import time
import yaml
import socket
import traceback

from heron.common.src.python.utils import log
from heron.common.src.python.utils import proc

from heron.proto.packing_plan_pb2 import PackingPlan
from heron.statemgrs.src.python import statemanagerfactory
from heron.statemgrs.src.python import configloader
from heron.statemgrs.src.python.config import Config as StateMgrConfig

Log = log.Log



def print_usage():
  print(
      ""Usage: ./heron-executor --shard=<shardid> --topology-name=<topname>""
      "" --topology-id=<topid> --topology-defn-file=<topdefnfile>""
      "" --state-manager-connection=<state_manager_connection>""
      "" --state-manager-root=<state_manager_root>""
      "" --state-manager-config-file=<state_manager_config_file>""
      "" --tmaster-binary=<tmaster_binary>""
      "" --stmgr-binary=<stmgr_binary> --metrics-manager-classpath=<metricsmgr_classpath>""
      "" --instance-jvm-opts=<instance_jvm_opts_in_base64> --classpath=<classpath>""
      "" --master-port=<master_port> --tmaster-controller-port=<tmaster_controller_port>""
      "" --tmaster-stats-port=<tmaster_stats_port>""
      "" --heron-internals-config-file=<heron_internals_config_file>""
      "" --override-config-file=<override_config_file> --component-ram-map=<component_ram_map>""
      "" --component-jvm-opts=<component_jvm_opts_in_base64> --pkg-type=<pkg_type>""
      "" --topology-binary-file=<topology_bin_file> --heron-java-home=<heron_java_home>""
      "" --shell-port=<shell-port> --heron-shell-binary=<heron_shell_binary>""
      "" --metrics-manager-port=<metricsmgr_port>""
      "" --cluster=<cluster> --role=<role> --environment=<environ>""
      "" --instance-classpath=<instance_classpath>""
      "" --metrics-sinks-config-file=<metrics_sinks_config_file>""
      "" --scheduler-classpath=<scheduler_classpath> --scheduler-port=<scheduler_port>""
      "" --python-instance-binary=<python_instance_binary>""
      "" --metricscache-manager-classpath=<metricscachemgr_classpath>""
      "" --metricscache-manager-master-port=<metricscachemgr_masterport>""
      "" --metricscache-manager-stats-port=<metricscachemgr_statsport>""
      "" --is-stateful=<is_stateful> --checkpoint-manager-classpath=<ckptmgr_classpath>""
      "" --checkpoint-manager-port=<ckptmgr_port> --checkpoint-manager-ram=<checkpoint_manager_ram>""
      "" --stateful-config-file=<stateful_config_file>""
      "" --health-manager-mode=<healthmgr_mode> --health-manager-classpath=<healthmgr_classpath>""
      "" --cpp-instance-binary=<cpp_instance_binary>""
      "" --jvm-remote-debugger-ports=<comma_seperated_port_list>"")

def id_map(prefix, container_plans, add_zero_id=False):
  ids = {}
  if add_zero_id:
    ids[0] = ""%s-0"" % prefix

  for container_plan in container_plans:
    ids[container_plan.id] = ""%s-%d"" % (prefix, container_plan.id)
  return ids

def stmgr_map(container_plans):
  return id_map(""stmgr"", container_plans)

def metricsmgr_map(container_plans):
  return id_map(""metricsmgr"", container_plans, True)

def ckptmgr_map(container_plans):
  return id_map(""ckptmgr"", container_plans, True)

def heron_shell_map(container_plans):
  return id_map(""heron-shell"", container_plans, True)

def get_heron_executor_process_name(shard_id):
  return 'heron-executor-%d' % shard_id

def get_process_pid_filename(process_name):
  return '%s.pid' % process_name

def get_tmp_filename():
  return '%s.heron.tmp' % (''.join(random.choice(string.ascii_uppercase) for i in range(12)))

def atomic_write_file(path, content):
  

  
  tmp_file = get_tmp_filename()
  with open(tmp_file, 'w') as f:
    f.write(content)
    
    f.flush()
    os.fsync(f.fileno())

  
  os.rename(tmp_file, path)

def log_pid_for_process(process_name, pid):
  filename = get_process_pid_filename(process_name)
  Log.info('Logging pid %d to file %s' %(pid, filename))
  atomic_write_file(filename, str(pid))

def is_docker_environment():
  return os.path.isfile('/.dockerenv')

def stdout_log_fn(cmd):
  

  
  return lambda line: Log.info(""%s stdout: %s"", cmd, line.rstrip('\n'))

class Command(object):
  

  def __init__(self, cmd, env):
    if isinstance(cmd, list):
      self.cmd = cmd
    else:
      self.cmd = [cmd]
    self.env = env

  def extend(self, args):
    self.cmd.extend(args)

  def append(self, arg):
    self.cmd.append(arg)

  def copy(self):
    return Command(list(self.cmd), self.env.copy() if self.env is not None else {})

  def __repr__(self):
    return str(self.cmd)

  def __str__(self):
    return ' '.join(self.cmd)

  def __eq__(self, other):
    return self.cmd == other.cmd

class ProcessInfo(object):
  def __init__(self, process, name, command, attempts=1):
    

    self.process = process
    self.pid = process.pid
    self.name = name
    self.command = command
    self.command_str = command.__str__() 
    self.attempts = attempts

  def increment_attempts(self):
    self.attempts += 1
    return self


class HeronExecutor(object):
  

  def init_from_parsed_args(self, parsed_args):
    

    self.shard = parsed_args.shard
    self.topology_name = parsed_args.topology_name
    self.topology_id = parsed_args.topology_id
    self.topology_defn_file = parsed_args.topology_defn_file
    self.state_manager_connection = parsed_args.state_manager_connection
    self.state_manager_root = parsed_args.state_manager_root
    self.state_manager_config_file = parsed_args.state_manager_config_file
    self.tmaster_binary = parsed_args.tmaster_binary
    self.stmgr_binary = parsed_args.stmgr_binary
    self.metrics_manager_classpath = parsed_args.metrics_manager_classpath
    self.metricscache_manager_classpath = parsed_args.metricscache_manager_classpath
    
    
    
    
    
    self.instance_jvm_opts =\
        base64.b64decode(parsed_args.instance_jvm_opts.lstrip(
).replace('(61)', '=').replace('&equals;', '='))
    self.classpath = parsed_args.classpath
    
    
    
    if is_docker_environment():
      self.master_host = os.environ.get('HOST') if 'HOST' in os.environ else socket.gethostname()
    else:
      self.master_host = socket.gethostname()
    self.master_port = parsed_args.master_port
    self.tmaster_controller_port = parsed_args.tmaster_controller_port
    self.tmaster_stats_port = parsed_args.tmaster_stats_port
    self.heron_internals_config_file = parsed_args.heron_internals_config_file
    self.override_config_file = parsed_args.override_config_file
    self.component_ram_map =\
        map(lambda x: {x.split(':')[0]:
                           int(x.split(':')[1])}, parsed_args.component_ram_map.split(','))
    self.component_ram_map =\
        functools.reduce(lambda x, y: dict(x.items() + y.items()), self.component_ram_map)

    
    
    
    self.component_jvm_opts = {}
    
    
    
    
    
    
    component_jvm_opts_in_json =\
        base64.b64decode(parsed_args.component_jvm_opts.
                         lstrip(
).replace('(61)', '=').replace('&equals;', '='))
    if component_jvm_opts_in_json != """":
      for (k, v) in json.loads(component_jvm_opts_in_json).items():
        
        self.component_jvm_opts[base64.b64decode(k)] = base64.b64decode(v)

    self.pkg_type = parsed_args.pkg_type
    self.topology_binary_file = parsed_args.topology_binary_file
    self.heron_java_home = parsed_args.heron_java_home
    self.shell_port = parsed_args.shell_port
    self.heron_shell_binary = parsed_args.heron_shell_binary
    self.metrics_manager_port = parsed_args.metrics_manager_port
    self.metricscache_manager_master_port = parsed_args.metricscache_manager_master_port
    self.metricscache_manager_stats_port = parsed_args.metricscache_manager_stats_port
    self.cluster = parsed_args.cluster
    self.role = parsed_args.role
    self.environment = parsed_args.environment
    self.instance_classpath = parsed_args.instance_classpath
    self.metrics_sinks_config_file = parsed_args.metrics_sinks_config_file
    self.scheduler_classpath = parsed_args.scheduler_classpath
    self.scheduler_port = parsed_args.scheduler_port
    self.python_instance_binary = parsed_args.python_instance_binary
    self.cpp_instance_binary = parsed_args.cpp_instance_binary

    self.is_stateful_topology = (parsed_args.is_stateful.lower() == 'true')
    self.checkpoint_manager_classpath = parsed_args.checkpoint_manager_classpath
    self.checkpoint_manager_port = parsed_args.checkpoint_manager_port
    self.checkpoint_manager_ram = parsed_args.checkpoint_manager_ram
    self.stateful_config_file = parsed_args.stateful_config_file
    self.metricscache_manager_mode = parsed_args.metricscache_manager_mode \
        if parsed_args.metricscache_manager_mode else ""disabled""
    self.health_manager_mode = parsed_args.health_manager_mode
    self.health_manager_classpath = '%s:%s'\
        % (self.scheduler_classpath, parsed_args.health_manager_classpath)
    self.jvm_remote_debugger_ports = \
      parsed_args.jvm_remote_debugger_ports.split("","") \
        if parsed_args.jvm_remote_debugger_ports else None

  def __init__(self, args, shell_env):
    parsed_args = self.parse_args(args)
    self.init_from_parsed_args(parsed_args)

    self.shell_env = shell_env
    self.max_runs = 100
    self.interval_between_runs = 10

    
    self.log_dir = self._load_logging_dir(self.heron_internals_config_file)

    
    self.packing_plan = None
    self.stmgr_ids = {}
    self.metricsmgr_ids = {}
    self.heron_shell_ids = {}
    self.ckptmgr_ids = {}

    
    
    self.process_lock = threading.RLock()
    self.processes_to_monitor = {}

    self.state_managers = []
    self.jvm_version = None

  @staticmethod
  def parse_args(args):
    

    Log.info(""Input args: %r"" % args)

    parser = argparse.ArgumentParser()

    parser.add_argument(""--shard"", type=int, required=True)
    parser.add_argument(""--topology-name"", required=True)
    parser.add_argument(""--topology-id"", required=True)
    parser.add_argument(""--topology-defn-file"", required=True)
    parser.add_argument(""--state-manager-connection"", required=True)
    parser.add_argument(""--state-manager-root"", required=True)
    parser.add_argument(""--state-manager-config-file"", required=True)
    parser.add_argument(""--tmaster-binary"", required=True)
    parser.add_argument(""--stmgr-binary"", required=True)
    parser.add_argument(""--metrics-manager-classpath"", required=True)
    parser.add_argument(""--instance-jvm-opts"", required=True)
    parser.add_argument(""--classpath"", required=True)
    parser.add_argument(""--master-port"", required=True)
    parser.add_argument(""--tmaster-controller-port"", required=True)
    parser.add_argument(""--tmaster-stats-port"", required=True)
    parser.add_argument(""--heron-internals-config-file"", required=True)
    parser.add_argument(""--override-config-file"", required=True)
    parser.add_argument(""--component-ram-map"", required=True)
    parser.add_argument(""--component-jvm-opts"", required=True)
    parser.add_argument(""--pkg-type"", required=True)
    parser.add_argument(""--topology-binary-file"", required=True)
    parser.add_argument(""--heron-java-home"", required=True)
    parser.add_argument(""--shell-port"", required=True)
    parser.add_argument(""--heron-shell-binary"", required=True)
    parser.add_argument(""--metrics-manager-port"", required=True)
    parser.add_argument(""--cluster"", required=True)
    parser.add_argument(""--role"", required=True)
    parser.add_argument(""--environment"", required=True)
    parser.add_argument(""--instance-classpath"", required=True)
    parser.add_argument(""--metrics-sinks-config-file"", required=True)
    parser.add_argument(""--scheduler-classpath"", required=True)
    parser.add_argument(""--scheduler-port"", required=True)
    parser.add_argument(""--python-instance-binary"", required=True)
    parser.add_argument(""--cpp-instance-binary"", required=True)
    parser.add_argument(""--metricscache-manager-classpath"", required=True)
    parser.add_argument(""--metricscache-manager-master-port"", required=True)
    parser.add_argument(""--metricscache-manager-stats-port"", required=True)
    parser.add_argument(""--metricscache-manager-mode"", required=False)
    parser.add_argument(""--is-stateful"", required=True)
    parser.add_argument(""--checkpoint-manager-classpath"", required=True)
    parser.add_argument(""--checkpoint-manager-port"", required=True)
    parser.add_argument(""--checkpoint-manager-ram"", type=long, required=True)
    parser.add_argument(""--stateful-config-file"", required=True)
    parser.add_argument(""--health-manager-mode"", required=True)
    parser.add_argument(""--health-manager-classpath"", required=True)
    parser.add_argument(""--jvm-remote-debugger-ports"", required=False,
                        help=""ports to be used by a remote debugger for JVM instances"")

    parsed_args, unknown_args = parser.parse_known_args(args[1:])

    if unknown_args:
      Log.error('Unknown argument: %s' % unknown_args[0])
      parser.print_help()
      sys.exit(1)

    return parsed_args

  def run_command_or_exit(self, command):
    if self._run_blocking_process(command, True) != 0:
      Log.error(""Failed to run command: %s. Exiting"" % command)
      sys.exit(1)

  def initialize(self):
    

    create_folders = Command('mkdir -p %s' % self.log_dir, self.shell_env)
    self.run_command_or_exit(create_folders)

    chmod_logs_dir = Command('chmod a+rx . && chmod a+x %s' % self.log_dir, self.shell_env)
    self.run_command_or_exit(chmod_logs_dir)

    chmod_x_binaries = [self.tmaster_binary, self.stmgr_binary, self.heron_shell_binary]

    for binary in chmod_x_binaries:
      stat_result = os.stat(binary)[stat.ST_MODE]
      if not stat_result & stat.S_IXOTH:
        chmod_binary = Command('chmod +x %s' % binary, self.shell_env)
        self.run_command_or_exit(chmod_binary)

    
    log_pid_for_process(get_heron_executor_process_name(self.shard), os.getpid())

  def update_packing_plan(self, new_packing_plan):
    self.packing_plan = new_packing_plan
    self.stmgr_ids = stmgr_map(self.packing_plan.container_plans)
    self.ckptmgr_ids = ckptmgr_map(self.packing_plan.container_plans)
    self.metricsmgr_ids = metricsmgr_map(self.packing_plan.container_plans)
    self.heron_shell_ids = heron_shell_map(self.packing_plan.container_plans)

  
  def _load_logging_dir(self, heron_internals_config_file):
    with open(heron_internals_config_file, 'r') as stream:
      heron_internals_config = yaml.load(stream)
    return heron_internals_config['heron.logging.directory']

  def _get_metricsmgr_cmd(self, metricsManagerId, sink_config_file, port):
    

    metricsmgr_main_class = 'org.apache.heron.metricsmgr.MetricsManager'

    metricsmgr_cmd = [os.path.join(self.heron_java_home, 'bin/java'),
                      
                      
                      '-Xmx1024M',
                      '-XX:+PrintCommandLineFlags',
                      '-verbosegc',
                      '-XX:+PrintGCDetails',
                      '-XX:+PrintGCTimeStamps',
                      '-XX:+PrintGCDateStamps',
                      '-XX:+PrintGCCause',
                      '-XX:+UseGCLogFileRotation',
                      '-XX:NumberOfGCLogFiles=5',
                      '-XX:GCLogFileSize=100M',
                      '-XX:+PrintPromotionFailure',
                      '-XX:+PrintTenuringDistribution',
                      '-XX:+PrintHeapAtGC',
                      '-XX:+HeapDumpOnOutOfMemoryError',
                      '-XX:+UseConcMarkSweepGC',
                      '-XX:+PrintCommandLineFlags',
                      '-Xloggc:log-files/gc.metricsmgr.log',
                      '-Djava.net.preferIPv4Stack=true',
                      '-cp',
                      self.metrics_manager_classpath,
                      metricsmgr_main_class,
                      '--id=' + metricsManagerId,
                      '--port=' + str(port),
                      '--topology=' + self.topology_name,
                      '--cluster=' + self.cluster,
                      '--role=' + self.role,
                      '--environment=' + self.environment,
                      '--topology-id=' + self.topology_id,
                      '--system-config-file=' + self.heron_internals_config_file,
                      '--override-config-file=' + self.override_config_file,
                      '--sink-config-file=' + sink_config_file]

    return Command(metricsmgr_cmd, self.shell_env)

  def _get_metrics_cache_cmd(self):
    

    metricscachemgr_main_class = 'org.apache.heron.metricscachemgr.MetricsCacheManager'

    metricscachemgr_cmd = [os.path.join(self.heron_java_home, 'bin/java'),
                           
                           
                           '-Xmx1024M',
                           '-XX:+PrintCommandLineFlags',
                           '-verbosegc',
                           '-XX:+PrintGCDetails',
                           '-XX:+PrintGCTimeStamps',
                           '-XX:+PrintGCDateStamps',
                           '-XX:+PrintGCCause',
                           '-XX:+UseGCLogFileRotation',
                           '-XX:NumberOfGCLogFiles=5',
                           '-XX:GCLogFileSize=100M',
                           '-XX:+PrintPromotionFailure',
                           '-XX:+PrintTenuringDistribution',
                           '-XX:+PrintHeapAtGC',
                           '-XX:+HeapDumpOnOutOfMemoryError',
                           '-XX:+UseConcMarkSweepGC',
                           '-XX:+PrintCommandLineFlags',
                           '-Xloggc:log-files/gc.metricscache.log',
                           '-Djava.net.preferIPv4Stack=true',
                           '-cp',
                           self.metricscache_manager_classpath,
                           metricscachemgr_main_class,
                           ""--metricscache_id"", 'metricscache-0',
                           ""--master_port"", self.metricscache_manager_master_port,
                           ""--stats_port"", self.metricscache_manager_stats_port,
                           ""--topology_name"", self.topology_name,
                           ""--topology_id"", self.topology_id,
                           ""--system_config_file"", self.heron_internals_config_file,
                           ""--override_config_file"", self.override_config_file,
                           ""--sink_config_file"", self.metrics_sinks_config_file,
                           ""--cluster"", self.cluster,
                           ""--role"", self.role,
                           ""--environment"", self.environment]

    return Command(metricscachemgr_cmd, self.shell_env)

  def _get_healthmgr_cmd(self):
    

    healthmgr_main_class = 'org.apache.heron.healthmgr.HealthManager'

    healthmgr_cmd = [os.path.join(self.heron_java_home, 'bin/java'),
                     
                     
                     '-Xmx1024M',
                     '-XX:+PrintCommandLineFlags',
                     '-verbosegc',
                     '-XX:+PrintGCDetails',
                     '-XX:+PrintGCTimeStamps',
                     '-XX:+PrintGCDateStamps',
                     '-XX:+PrintGCCause',
                     '-XX:+UseGCLogFileRotation',
                     '-XX:NumberOfGCLogFiles=5',
                     '-XX:GCLogFileSize=100M',
                     '-XX:+PrintPromotionFailure',
                     '-XX:+PrintTenuringDistribution',
                     '-XX:+PrintHeapAtGC',
                     '-XX:+HeapDumpOnOutOfMemoryError',
                     '-XX:+UseConcMarkSweepGC',
                     '-XX:+PrintCommandLineFlags',
                     '-Xloggc:log-files/gc.healthmgr.log',
                     '-Djava.net.preferIPv4Stack=true',
                     '-cp', self.health_manager_classpath,
                     healthmgr_main_class,
                     ""--cluster"", self.cluster,
                     ""--role"", self.role,
                     ""--environment"", self.environment,
                     ""--topology_name"", self.topology_name,
                     ""--metricsmgr_port"", self.metrics_manager_port]

    return Command(healthmgr_cmd, self.shell_env)

  def _get_tmaster_processes(self):
    

    retval = {}
    tmaster_cmd_lst = [
        self.tmaster_binary,
        '--topology_name=%s' % self.topology_name,
        '--topology_id=%s' % self.topology_id,
        '--zkhostportlist=%s' % self.state_manager_connection,
        '--zkroot=%s' % self.state_manager_root,
        '--myhost=%s' % self.master_host,
        '--master_port=%s' % str(self.master_port),
        '--controller_port=%s' % str(self.tmaster_controller_port),
        '--stats_port=%s' % str(self.tmaster_stats_port),
        '--config_file=%s' % self.heron_internals_config_file,
        '--override_config_file=%s' % self.override_config_file,
        '--metrics_sinks_yaml=%s' % self.metrics_sinks_config_file,
        '--metricsmgr_port=%s' % str(self.metrics_manager_port),
        '--ckptmgr_port=%s' % str(self.checkpoint_manager_port)]

    tmaster_env = self.shell_env.copy() if self.shell_env is not None else {}
    tmaster_cmd = Command(tmaster_cmd_lst, tmaster_env)
    if os.environ.get('ENABLE_HEAPCHECK') is not None:
      tmaster_cmd.env.update({
          'LD_PRELOAD': ""/usr/lib/libtcmalloc.so"",
          'HEAPCHECK': ""normal""
      })

    retval[""heron-tmaster""] = tmaster_cmd

    if self.metricscache_manager_mode.lower() != ""disabled"":
      retval[""heron-metricscache""] = self._get_metrics_cache_cmd()

    if self.health_manager_mode.lower() != ""disabled"":
      retval[""heron-healthmgr""] = self._get_healthmgr_cmd()

    retval[self.metricsmgr_ids[0]] = self._get_metricsmgr_cmd(
        self.metricsmgr_ids[0],
        self.metrics_sinks_config_file,
        self.metrics_manager_port)

    if self.is_stateful_topology:
      retval.update(self._get_ckptmgr_process())

    return retval

  
  def _get_java_instance_cmd(self, instance_info):
    retval = {}
    
    instance_class_name = 'org.apache.heron.instance.HeronInstance'

    if self.jvm_remote_debugger_ports and \
            (len(instance_info) > len(self.jvm_remote_debugger_ports)):
      Log.warn(""Not enough remote debugger ports for all instances!"")

    
    for (instance_id, component_name, global_task_id, component_index) in instance_info:
      
      remote_debugger_port = None
      if self.jvm_remote_debugger_ports:
        remote_debugger_port = self.jvm_remote_debugger_ports.pop()

      instance_cmd = self._get_jvm_instance_cmd().copy()            
      instance_cmd.extend(                                          
          self._get_jvm_instance_options(
              instance_id, component_name, remote_debugger_port))
      instance_cmd.append(instance_class_name)                      
      instance_cmd.extend(                                          
          self._get_jvm_instance_arguments(
              instance_id, component_name, global_task_id, component_index, remote_debugger_port))

      retval[instance_id] = instance_cmd

    return retval

  def _get_jvm_instance_cmd(self):
    return Command(os.path.join(self.heron_java_home, 'bin/java'), self.shell_env)

  def _get_jvm_instance_options(self, instance_id, component_name, remote_debugger_port):
    code_cache_size_mb = 64
    java_metasize_mb = 128

    total_jvm_size = int(self.component_ram_map[component_name] / (1024 * 1024))
    heap_size_mb = total_jvm_size - code_cache_size_mb - java_metasize_mb
    Log.info(""component name: %s, RAM request: %d, total JVM size: %dM, ""
             ""cache size: %dM, metaspace size: %dM""
             % (component_name, self.component_ram_map[component_name],
                total_jvm_size, code_cache_size_mb, java_metasize_mb))
    xmn_size = int(heap_size_mb / 2)

    java_version = self._get_jvm_version()
    java_metasize_param = 'MetaspaceSize'
    if java_version.startswith(""1.7"") or \
            java_version.startswith(""1.6"") or \
            java_version.startswith(""1.5""):
      java_metasize_param = 'PermSize'

    instance_options = [
        '-Xmx%dM' % heap_size_mb,
        '-Xms%dM' % heap_size_mb,
        '-Xmn%dM' % xmn_size,
        '-XX:Max%s=%dM' % (java_metasize_param, java_metasize_mb),
        '-XX:%s=%dM' % (java_metasize_param, java_metasize_mb),
        '-XX:ReservedCodeCacheSize=%dM' % code_cache_size_mb,
        '-XX:+CMSScavengeBeforeRemark',
        '-XX:TargetSurvivorRatio=90',
        '-XX:+PrintCommandLineFlags',
        '-verbosegc',
        '-XX:+PrintGCDetails',
        '-XX:+PrintGCTimeStamps',
        '-XX:+PrintGCDateStamps',
        '-XX:+PrintGCCause',
        '-XX:+UseGCLogFileRotation',
        '-XX:NumberOfGCLogFiles=5',
        '-XX:GCLogFileSize=100M',
        '-XX:+PrintPromotionFailure',
        '-XX:+PrintTenuringDistribution',
        '-XX:+PrintHeapAtGC',
        '-XX:+HeapDumpOnOutOfMemoryError',
        '-XX:+UseConcMarkSweepGC',
        '-XX:ParallelGCThreads=4',
        '-Xloggc:log-files/gc.%s.log' % instance_id,
        '-Djava.net.preferIPv4Stack=true',
        '-cp',
        '%s:%s'% (self.instance_classpath, self.classpath)]

    
    if remote_debugger_port:
      instance_options.append('-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=%s'
                              % remote_debugger_port)

    
    instance_options.extend(self.instance_jvm_opts.split())
    if component_name in self.component_jvm_opts:
      instance_options.extend(self.component_jvm_opts[component_name].split())

    return instance_options

  def _get_jvm_instance_arguments(self, instance_id, component_name, global_task_id,
                                  component_index, remote_debugger_port):
    instance_args = [
        '-topology_name', self.topology_name,
        '-topology_id', self.topology_id,
        '-instance_id', instance_id,
        '-component_name', component_name,
        '-task_id', str(global_task_id),
        '-component_index', str(component_index),
        '-stmgr_id', self.stmgr_ids[self.shard],
        '-stmgr_port', self.tmaster_controller_port,
        '-metricsmgr_port', self.metrics_manager_port,
        '-system_config_file', self.heron_internals_config_file,
        '-override_config_file', self.override_config_file]

    
    if remote_debugger_port:
      instance_args += ['-remote_debugger_port', remote_debugger_port]

    return instance_args

  def _get_jvm_version(self):
    if not self.jvm_version:
      cmd = [os.path.join(self.heron_java_home, 'bin/java'),
             '-cp', self.instance_classpath, 'org.apache.heron.instance.util.JvmVersion']
      process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
      (process_stdout, process_stderr) = process.communicate()
      if process.returncode != 0:
        Log.error(""Failed to determine JVM version. Exiting. Output of %s: %s"",
                  ' '.join(cmd), process_stderr)
        sys.exit(1)

      self.jvm_version = process_stdout
      Log.info(""Detected JVM version %s"" % self.jvm_version)
    return self.jvm_version

  
  def _get_python_instance_cmd(self, instance_info):
    
    
    retval = {}
    for (instance_id, component_name, global_task_id, component_index) in instance_info:
      Log.info(""Python instance %s component: %s"" %(instance_id, component_name))
      instance_cmd = [self.python_instance_binary,
                      '--topology_name=%s' % self.topology_name,
                      '--topology_id=%s' % self.topology_id,
                      '--instance_id=%s' % instance_id,
                      '--component_name=%s' % component_name,
                      '--task_id=%s' % str(global_task_id),
                      '--component_index=%s' % str(component_index),
                      '--stmgr_id=%s' % self.stmgr_ids[self.shard],
                      '--stmgr_port=%s' % self.tmaster_controller_port,
                      '--metricsmgr_port=%s' % self.metrics_manager_port,
                      '--sys_config=%s' % self.heron_internals_config_file,
                      '--override_config=%s' % self.override_config_file,
                      '--topology_pex=%s' % self.topology_binary_file,
                      '--max_ram=%s' % str(self.component_ram_map[component_name])]

      retval[instance_id] = Command(instance_cmd, self.shell_env)

    return retval

  
  def _get_cpp_instance_cmd(self, instance_info):
    
    
    retval = {}
    for (instance_id, component_name, global_task_id, component_index) in instance_info:
      Log.info(""CPP instance %s component: %s"" %(instance_id, component_name))
      instance_cmd = [
          self.cpp_instance_binary,
          '--topology_name=%s' % self.topology_name,
          '--topology_id=%s' % self.topology_id,
          '--instance_id=%s' % instance_id,
          '--component_name=%s' % component_name,
          '--task_id=%s' % str(global_task_id),
          '--component_index=%s' % str(component_index),
          '--stmgr_id=%s' % self.stmgr_ids[self.shard],
          '--stmgr_port=%s' % str(self.tmaster_controller_port),
          '--metricsmgr_port=%s' % str(self.metrics_manager_port),
          '--config_file=%s' % self.heron_internals_config_file,
          '--override_config_file=%s' % self.override_config_file,
          '--topology_binary=%s' % os.path.abspath(self.topology_binary_file)
      ]

      retval[instance_id] = Command(instance_cmd, self.shell_env)

    return retval

  
  
  def _get_streaming_processes(self):
    

    retval = {}
    instance_plans = self._get_instance_plans(self.packing_plan, self.shard)
    instance_info = []
    for instance_plan in instance_plans:
      global_task_id = instance_plan.task_id
      component_index = instance_plan.component_index
      component_name = instance_plan.component_name
      instance_id = ""container_%s_%s_%d"" % (str(self.shard), component_name, global_task_id)
      instance_info.append((instance_id, component_name, global_task_id, component_index))

    stmgr_cmd_lst = [
        self.stmgr_binary,
        '--topology_name=%s' % self.topology_name,
        '--topology_id=%s' % self.topology_id,
        '--topologydefn_file=%s' % self.topology_defn_file,
        '--zkhostportlist=%s' % self.state_manager_connection,
        '--zkroot=%s' % self.state_manager_root,
        '--stmgr_id=%s' % self.stmgr_ids[self.shard],
        '--instance_ids=%s' % ','.join(map(lambda x: x[0], instance_info)),
        '--myhost=%s' % self.master_host,
        '--data_port=%s' % str(self.master_port),
        '--local_data_port=%s' % str(self.tmaster_controller_port),
        '--metricsmgr_port=%s' % str(self.metrics_manager_port),
        '--shell_port=%s' % str(self.shell_port),
        '--config_file=%s' % self.heron_internals_config_file,
        '--override_config_file=%s' % self.override_config_file,
        '--ckptmgr_port=%s' % str(self.checkpoint_manager_port),
        '--ckptmgr_id=%s' % self.ckptmgr_ids[self.shard],
        '--metricscachemgr_mode=%s' % self.metricscache_manager_mode.lower()]

    stmgr_env = self.shell_env.copy() if self.shell_env is not None else {}
    stmgr_cmd = Command(stmgr_cmd_lst, stmgr_env)
    if os.environ.get('ENABLE_HEAPCHECK') is not None:
      stmgr_cmd.env.update({
          'LD_PRELOAD': ""/usr/lib/libtcmalloc.so"",
          'HEAPCHECK': ""normal""
      })

    retval[self.stmgr_ids[self.shard]] = stmgr_cmd

    

    retval[self.metricsmgr_ids[self.shard]] = self._get_metricsmgr_cmd(
        self.metricsmgr_ids[self.shard],
        self.metrics_sinks_config_file,
        self.metrics_manager_port
    )

    if self.is_stateful_topology:
      retval.update(self._get_ckptmgr_process())

    if self.pkg_type == 'jar' or self.pkg_type == 'tar':
      retval.update(self._get_java_instance_cmd(instance_info))
    elif self.pkg_type == 'pex':
      retval.update(self._get_python_instance_cmd(instance_info))
    elif self.pkg_type == 'so':
      retval.update(self._get_cpp_instance_cmd(instance_info))
    elif self.pkg_type == 'dylib':
      retval.update(self._get_cpp_instance_cmd(instance_info))
    else:
      raise ValueError(""Unrecognized package type: %s"" % self.pkg_type)

    return retval

  def _get_ckptmgr_process(self):
    


    ckptmgr_main_class = 'org.apache.heron.ckptmgr.CheckpointManager'

    ckptmgr_ram_mb = self.checkpoint_manager_ram / (1024 * 1024)
    ckptmgr_cmd = [os.path.join(self.heron_java_home, ""bin/java""),
                   '-Xms%dM' % ckptmgr_ram_mb,
                   '-Xmx%dM' % ckptmgr_ram_mb,
                   '-XX:+PrintCommandLineFlags',
                   '-verbosegc',
                   '-XX:+PrintGCDetails',
                   '-XX:+PrintGCTimeStamps',
                   '-XX:+PrintGCDateStamps',
                   '-XX:+PrintGCCause',
                   '-XX:+UseGCLogFileRotation',
                   '-XX:NumberOfGCLogFiles=5',
                   '-XX:GCLogFileSize=100M',
                   '-XX:+PrintPromotionFailure',
                   '-XX:+PrintTenuringDistribution',
                   '-XX:+PrintHeapAtGC',
                   '-XX:+HeapDumpOnOutOfMemoryError',
                   '-XX:+UseConcMarkSweepGC',
                   '-XX:+UseConcMarkSweepGC',
                   '-Xloggc:log-files/gc.ckptmgr.log',
                   '-Djava.net.preferIPv4Stack=true',
                   '-cp',
                   self.checkpoint_manager_classpath,
                   ckptmgr_main_class,
                   '-t' + self.topology_name,
                   '-i' + self.topology_id,
                   '-c' + self.ckptmgr_ids[self.shard],
                   '-p' + self.checkpoint_manager_port,
                   '-f' + self.stateful_config_file,
                   '-o' + self.override_config_file,
                   '-g' + self.heron_internals_config_file]
    retval = {}
    retval[self.ckptmgr_ids[self.shard]] = Command(ckptmgr_cmd, self.shell_env)

    return retval

  def _get_instance_plans(self, packing_plan, container_id):
    

    this_container_plan = None
    for container_plan in packing_plan.container_plans:
      if container_plan.id == container_id:
        this_container_plan = container_plan

    
    
    
    if this_container_plan is None:
      return None
    return this_container_plan.instance_plans

  
  def _get_heron_support_processes(self):
    

    retval = {}

    retval[self.heron_shell_ids[self.shard]] = Command([
        '%s' % self.heron_shell_binary,
        '--port=%s' % self.shell_port,
        '--log_file_prefix=%s/heron-shell-%s.log' % (self.log_dir, self.shard),
        '--secret=%s' % self.topology_id], self.shell_env)

    return retval

  def _untar_if_needed(self):
    if self.pkg_type == ""tar"":
      os.system(""tar -xvf %s"" % self.topology_binary_file)
    elif self.pkg_type == ""pex"":
      os.system(""unzip -qq -n %s"" % self.topology_binary_file)

  
  def _wait_process_std_out_err(self, name, process):
    

    proc.stream_process_stdout(process, stdout_log_fn(name))
    process.wait()

  def _run_process(self, name, cmd):
    Log.info(""Running %s process as %s"" % (name, cmd))
    try:
      
      
      process = subprocess.Popen(cmd.cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                                 env=cmd.env, bufsize=1)
      proc.async_stream_process_stdout(process, stdout_log_fn(name))
    except Exception:
      Log.info(""Exception running command %s"", cmd)
      traceback.print_exc()

    return process

  def _run_blocking_process(self, cmd, is_shell=False):
    Log.info(""Running blocking process as %s"" % cmd)
    try:
      
      
      process = subprocess.Popen(cmd.cmd, shell=is_shell, stdout=subprocess.PIPE,
                                 stderr=subprocess.STDOUT, env=cmd.env)

      
      self._wait_process_std_out_err(cmd.cmd, process)
    except Exception:
      Log.info(""Exception running command %s"", cmd)
      traceback.print_exc()

    
    return process.returncode

  def _kill_processes(self, commands):
    
    with self.process_lock:
      for command_name, command in commands.items():
        for process_info in self.processes_to_monitor.values():
          if process_info.name == command_name:
            del self.processes_to_monitor[process_info.pid]
            Log.info(""Killing %s process with pid %d: %s"" %
                     (process_info.name, process_info.pid, command))
            try:
              process_info.process.terminate()  
            except OSError as e:
              if e.errno == 3: 
                Log.warn(""Expected process %s with pid %d was not running, ignoring."" %
                         (process_info.name, process_info.pid))
              else:
                raise e

  def _start_processes(self, commands):
    

    Log.info(""Start processes"")
    processes_to_monitor = {}
    
    for (name, command) in commands.items():
      p = self._run_process(name, command)
      processes_to_monitor[p.pid] = ProcessInfo(p, name, command)

      
      log_pid_for_process(name, p.pid)

    with self.process_lock:
      self.processes_to_monitor.update(processes_to_monitor)

  def start_process_monitor(self):
    

    
    Log.info(""Start process monitor"")
    while True:
      if len(self.processes_to_monitor) > 0:
        (pid, status) = os.wait()

        with self.process_lock:
          if pid in self.processes_to_monitor.keys():
            old_process_info = self.processes_to_monitor[pid]
            name = old_process_info.name
            command = old_process_info.command
            Log.info(""%s (pid=%s) exited with status %d. command=%s"" % (name, pid, status, command))
            
            self._wait_process_std_out_err(name, old_process_info.process)

            
            if os.path.isfile(""core.%d"" % pid):
              os.system(""chmod a+r core.%d"" % pid)
            if old_process_info.attempts >= self.max_runs:
              Log.info(""%s exited too many times"" % name)
              sys.exit(1)
            time.sleep(self.interval_between_runs)
            p = self._run_process(name, command)
            del self.processes_to_monitor[pid]
            self.processes_to_monitor[p.pid] =\
              ProcessInfo(p, name, command, old_process_info.attempts + 1)

            
            log_pid_for_process(name, p.pid)

  def get_commands_to_run(self):
    

    
    if len(self.packing_plan.container_plans) == 0:
      return {}
    if self._get_instance_plans(self.packing_plan, self.shard) is None and self.shard != 0:
      retval = {}
      retval['heron-shell'] = Command([
          '%s' % self.heron_shell_binary,
          '--port=%s' % self.shell_port,
          '--log_file_prefix=%s/heron-shell-%s.log' % (self.log_dir, self.shard),
          '--secret=%s' % self.topology_id], self.shell_env)
      return retval

    if self.shard == 0:
      commands = self._get_tmaster_processes()
    else:
      self._untar_if_needed()
      commands = self._get_streaming_processes()

    
    commands.update(self._get_heron_support_processes())
    return commands

  def get_command_changes(self, current_commands, updated_commands):
    

    commands_to_kill = {}
    commands_to_keep = {}
    commands_to_start = {}

    
    
    for current_name, current_command in current_commands.items():
      
      
      if current_name in updated_commands.keys() and \
        current_command == updated_commands[current_name] and \
        not current_name.startswith('stmgr-'):
        commands_to_keep[current_name] = current_command
      else:
        commands_to_kill[current_name] = current_command

    
    for updated_name, updated_command in updated_commands.items():
      if updated_name not in commands_to_keep.keys():
        commands_to_start[updated_name] = updated_command

    return commands_to_kill, commands_to_keep, commands_to_start

  def launch(self):
    

    with self.process_lock:
      current_commands = dict(map((lambda process: (process.name, process.command)),
                                  self.processes_to_monitor.values()))
      updated_commands = self.get_commands_to_run()

      
      commands_to_kill, commands_to_keep, commands_to_start = \
          self.get_command_changes(current_commands, updated_commands)

      Log.info(""current commands: %s"" % sorted(current_commands.keys()))
      Log.info(""new commands    : %s"" % sorted(updated_commands.keys()))
      Log.info(""commands_to_kill: %s"" % sorted(commands_to_kill.keys()))
      Log.info(""commands_to_keep: %s"" % sorted(commands_to_keep.keys()))
      Log.info(""commands_to_start: %s"" % sorted(commands_to_start.keys()))

      self._kill_processes(commands_to_kill)
      self._start_processes(commands_to_start)
      Log.info(""Launch complete - processes killed=%s kept=%s started=%s monitored=%s"" %
               (len(commands_to_kill), len(commands_to_keep),
                len(commands_to_start), len(self.processes_to_monitor)))

  
  def start_state_manager_watches(self):
    

    Log.info(""Start state manager watches"")
    statemgr_config = StateMgrConfig()
    statemgr_config.set_state_locations(configloader.load_state_manager_locations(
        self.cluster, state_manager_config_file=self.state_manager_config_file,
        overrides={""heron.statemgr.connection.string"": self.state_manager_connection}))
    try:
      self.state_managers = statemanagerfactory.get_all_state_managers(statemgr_config)
      for state_manager in self.state_managers:
        state_manager.start()
    except Exception as ex:
      Log.error(""Found exception while initializing state managers: %s. Bailing out..."" % ex)
      traceback.print_exc()
      sys.exit(1)

    
    def on_packing_plan_watch(state_manager, new_packing_plan):
      Log.debug(""State watch triggered for PackingPlan update on shard %s. Existing: %s, New: %s"" %
                (self.shard, str(self.packing_plan), str(new_packing_plan)))

      if self.packing_plan != new_packing_plan:
        Log.info(""PackingPlan change detected on shard %s, relaunching effected processes.""
                 % self.shard)
        self.update_packing_plan(new_packing_plan)

        Log.info(""Updating executor processes"")
        self.launch()
      else:
        Log.info(
            ""State watch triggered for PackingPlan update but plan not changed so not relaunching."")

    for state_manager in self.state_managers:
      
      
      onPackingPlanWatch = functools.partial(on_packing_plan_watch, state_manager)
      state_manager.get_packing_plan(self.topology_name, onPackingPlanWatch)
      Log.info(""Registered state watch for packing plan changes with state manager %s."" %
               str(state_manager))

  def stop_state_manager_watches(self):
    Log.info(""Stopping state managers"")
    for state_manager in self.state_managers:
      state_manager.stop()

def setup(executor):
  

  
  def signal_handler(signal_to_handle, frame):
    
    
    Log.info('signal_handler invoked with signal %s', signal_to_handle)
    executor.stop_state_manager_watches()
    sys.exit(signal_to_handle)

  def cleanup():
    

    Log.info('Executor terminated; exiting all process in executor.')

    
    for pid in executor.processes_to_monitor.keys():
      os.kill(pid, signal.SIGTERM)
    time.sleep(5)

    
    os.killpg(0, signal.SIGTERM)

  
  
  shardid = executor.shard
  log.configure(logfile='heron-executor-%s.stdout' % shardid)

  pid = os.getpid()
  sid = os.getsid(pid)

  
  if pid <> sid:
    Log.info('Set up process group; executor becomes leader')
    os.setpgrp() 

  Log.info('Register the SIGTERM signal handler')
  signal.signal(signal.SIGTERM, signal_handler)

  Log.info('Register the atexit clean up')
  atexit.register(cleanup)

def start(executor):
  

  setup(executor)

  
  
  executor.start_state_manager_watches()

  
  
  executor.start_process_monitor()

def main():
  

  
  
  
  
  shell_env = os.environ.copy()
  shell_env[""PEX_ROOT""] = os.path.join(os.path.abspath('.'), "".pex"")

  
  executor = HeronExecutor(sys.argv, shell_env)
  executor.initialize()

  start(executor)

if __name__ == ""__main__"":
  main()
","










import logging
from astrobase import log_sub, log_fmt, log_date_fmt

DEBUG = False
if DEBUG:
    level = logging.DEBUG
else:
    level = logging.INFO
LOGGER = logging.getLogger(__name__)
logging.basicConfig(
    level=level,
    style=log_sub,
    format=log_fmt,
    datefmt=log_date_fmt,
)

LOGDEBUG = LOGGER.debug
LOGINFO = LOGGER.info
LOGWARNING = LOGGER.warning
LOGERROR = LOGGER.error
LOGEXCEPTION = LOGGER.exception






try:
    import cPickle as pickle
except Exception as e:
    import pickle

import os
import sys
import os.path
import glob
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor

from tornado.escape import squeeze



from functools import reduce
from operator import getitem
def _dict_get(datadict, keylist):
    return reduce(getitem, keylist, datadict)

import numpy as np

try:
    from tqdm import tqdm
    TQDM = True
except Exception as e:
    TQDM = False
    pass






NCPUS = mp.cpu_count()







from astrobase.lcmath import normalize_magseries
from astrobase.varclass import varfeatures

from astrobase.lcproc import get_lcformat







def get_varfeatures(lcfile,
                    outdir,
                    timecols=None,
                    magcols=None,
                    errcols=None,
                    mindet=1000,
                    lcformat='hat-sql',
                    lcformatdir=None):
    


    try:
        formatinfo = get_lcformat(lcformat,
                                  use_lcformat_dir=lcformatdir)
        if formatinfo:
            (dfileglob, readerfunc,
             dtimecols, dmagcols, derrcols,
             magsarefluxes, normfunc) = formatinfo
        else:
            LOGERROR(""can't figure out the light curve format"")
            return None
    except Exception as e:
        LOGEXCEPTION(""can't figure out the light curve format"")
        return None

    
    
    if timecols is None:
        timecols = dtimecols
    if magcols is None:
        magcols = dmagcols
    if errcols is None:
        errcols = derrcols

    try:

        
        lcdict = readerfunc(lcfile)

        
        
        
        if ( (isinstance(lcdict, (list, tuple))) and
             (isinstance(lcdict[0], dict)) ):
            lcdict = lcdict[0]

        resultdict = {'objectid':lcdict['objectid'],
                      'info':lcdict['objectinfo'],
                      'lcfbasename':os.path.basename(lcfile)}


        
        if normfunc is not None:
            lcdict = normfunc(lcdict)

        for tcol, mcol, ecol in zip(timecols, magcols, errcols):

            
            if '.' in tcol:
                tcolget = tcol.split('.')
            else:
                tcolget = [tcol]
            times = _dict_get(lcdict, tcolget)

            if '.' in mcol:
                mcolget = mcol.split('.')
            else:
                mcolget = [mcol]
            mags = _dict_get(lcdict, mcolget)

            if '.' in ecol:
                ecolget = ecol.split('.')
            else:
                ecolget = [ecol]
            errs = _dict_get(lcdict, ecolget)

            
            if normfunc is None:
                ntimes, nmags = normalize_magseries(
                    times, mags,
                    magsarefluxes=magsarefluxes
                )

                times, mags, errs = ntimes, nmags, errs


            
            finind = np.isfinite(times) & np.isfinite(mags) & np.isfinite(errs)

            
            if mags[finind].size < mindet:

                LOGINFO('not enough LC points: %s in normalized %s LC: %s' %
                        (mags[finind].size, mcol, os.path.basename(lcfile)))
                resultdict[mcol] = None

            else:

                
                lcfeatures = varfeatures.all_nonperiodic_features(
                    times, mags, errs
                )
                resultdict[mcol] = lcfeatures

        
        
        

        try:
            magmads = np.zeros(len(magcols))
            for mind, mcol in enumerate(magcols):
                if '.' in mcol:
                    mcolget = mcol.split('.')
                else:
                    mcolget = [mcol]

                magmads[mind] = resultdict[mcol]['mad']

            
            bestmagcolind = np.where(magmads == np.min(magmads))[0]
            resultdict['bestmagcol'] = magcols[bestmagcolind]

        except Exception as e:
            resultdict['bestmagcol'] = None

        outfile = os.path.join(outdir,
                               'varfeatures-%s.pkl' %
                               squeeze(resultdict['objectid']).replace(' ','-'))

        with open(outfile, 'wb') as outfd:
            pickle.dump(resultdict, outfd, protocol=4)

        return outfile

    except Exception as e:

        LOGEXCEPTION('failed to get LC features for %s because: %s' %
                     (os.path.basename(lcfile), e))
        return None



def _varfeatures_worker(task):
    


    try:
        (lcfile, outdir, timecols, magcols, errcols,
         mindet, lcformat, lcformatdir) = task
        return get_varfeatures(lcfile, outdir,
                               timecols=timecols,
                               magcols=magcols,
                               errcols=errcols,
                               mindet=mindet,
                               lcformat=lcformat,
                               lcformatdir=lcformatdir)

    except Exception as e:
        return None


def serial_varfeatures(lclist,
                       outdir,
                       maxobjects=None,
                       timecols=None,
                       magcols=None,
                       errcols=None,
                       mindet=1000,
                       lcformat='hat-sql',
                       lcformatdir=None):
    


    if maxobjects:
        lclist = lclist[:maxobjects]

    tasks = [(x, outdir, timecols, magcols, errcols,
              mindet, lcformat, lcformatdir)
             for x in lclist]

    for task in tqdm(tasks):
        result = _varfeatures_worker(task)

    return result



def parallel_varfeatures(lclist,
                         outdir,
                         maxobjects=None,
                         timecols=None,
                         magcols=None,
                         errcols=None,
                         mindet=1000,
                         lcformat='hat-sql',
                         lcformatdir=None,
                         nworkers=NCPUS):
    

    
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    if maxobjects:
        lclist = lclist[:maxobjects]

    tasks = [(x, outdir, timecols, magcols, errcols, mindet,
              lcformat, lcformatdir) for x in lclist]

    with ProcessPoolExecutor(max_workers=nworkers) as executor:
        resultfutures = executor.map(varfeatures_worker, tasks)

    results = [x for x in resultfutures]
    resdict = {os.path.basename(x):y for (x,y) in zip(lclist, results)}

    return resdict



def parallel_varfeatures_lcdir(lcdir,
                               outdir,
                               fileglob=None,
                               maxobjects=None,
                               timecols=None,
                               magcols=None,
                               errcols=None,
                               recursive=True,
                               mindet=1000,
                               lcformat='hat-sql',
                               lcformatdir=None,
                               nworkers=NCPUS):
    


    try:
        formatinfo = get_lcformat(lcformat,
                                  use_lcformat_dir=lcformatdir)
        if formatinfo:
            (dfileglob, readerfunc,
             dtimecols, dmagcols, derrcols,
             magsarefluxes, normfunc) = formatinfo
        else:
            LOGERROR(""can't figure out the light curve format"")
            return None
    except Exception as e:
        LOGEXCEPTION(""can't figure out the light curve format"")
        return None

    if not fileglob:
        fileglob = dfileglob

    
    LOGINFO('searching for %s light curves in %s ...' % (lcformat, lcdir))

    if recursive is False:
        matching = glob.glob(os.path.join(lcdir, fileglob))

    else:
        
        if sys.version_info[:2] > (3,4):

            matching = glob.glob(os.path.join(lcdir,
                                              '**',
                                              fileglob),
                                 recursive=True)

        
        else:

            
            walker = os.walk(lcdir)
            matching = []

            for root, dirs, _files in walker:
                for sdir in dirs:
                    searchpath = os.path.join(root,
                                              sdir,
                                              fileglob)
                    foundfiles = glob.glob(searchpath)

                    if foundfiles:
                        matching.extend(foundfiles)


    
    if matching and len(matching) > 0:

        LOGINFO('found %s light curves, getting varfeatures...' %
                len(matching))

        return parallel_varfeatures(matching,
                                    outdir,
                                    maxobjects=maxobjects,
                                    timecols=timecols,
                                    magcols=magcols,
                                    errcols=errcols,
                                    mindet=mindet,
                                    lcformat=lcformat,
                                    lcformatdir=lcformatdir,
                                    nworkers=nworkers)

    else:

        LOGERROR('no light curve files in %s format found in %s' % (lcformat,
                                                                    lcdir))
        return None
",,"



import datetime

from arguspy.wmi_sh import Wmi


class FileNumber(Wmi):

    


    def __init__(self, *args, **kwargs):
        super(FileNumber, self).__init__(*args, **kwargs)
        self.logger.debug(""Init FileNumber"")

    def define_sub_options(self):
        super(FileNumber, self).define_sub_options()
        self.fn_parser = self.subparsers.add_parser('filenumber',
                                                    help='Count file number.',
                                                    description='Options\
                                                    for filenumber.')
        self.fn_parser.add_argument('-q', '--query',
                                    required=False,
                                    help='wql for wmi.',
                                    dest='query')
        self.fn_parser.add_argument('-d', '--drive',
                                    required=True,
                                    help='the windows driver, like C:',
                                    dest='drive')
        self.fn_parser.add_argument('-p', '--path',
                                    default=""\\\\"",
                                    required=False,
                                    help='the folder, default is %(default)s',
                                    dest='path')
        self.fn_parser.add_argument(
            '-f',
            '--filename',
            default=""%%"",
            required=False,
            help='the filename, default is %(default)s',
            dest='filename')
        self.fn_parser.add_argument(
            '-e', '--extension', default=""%%"", required=False,
            help='the file extension, default is %(default)s',
            dest='extension')
        self.fn_parser.add_argument('-R', '--recursion',
                                    action='store_true',
                                    help='Recursive count file under path.',
                                    dest='recursion')
        self.fn_parser.add_argument(
            '-w',
            '--warning',
            default=0,
            type=int,
            required=False,
            help='Warning number of file, default is %(default)s',
            dest='warning')
        self.fn_parser.add_argument(
            '-c',
            '--critical',
            default=0,
            type=int,
            required=False,
            help='Critical number of file, default is %(default)s',
            dest='critical')

    def __get_file(self, path):
        self.wql_file = ""SELECT Name FROM CIM_DataFile WHERE Drive='{0}' \
            AND Path='{1}' AND FileName LIKE '{2}' AND Extension LIKE '{3}'"".format(
            self.args.drive, path, self.args.filename, self.args.extension)
        self.file_data = self.query(self.wql_file)
        [self.file_list.append(file_data) for file_data in self.file_data]
        self.logger.debug(""file_data: {}"".format(self.file_data))
        return len(self.file_data), self.file_list

    def __get_folder(self, path):
        self.wql_folder = ""SELECT FileName FROM CIM_Directory WHERE Drive='{0}' AND Path='{1}'"".format(
            self.args.drive, path)
        self.number, self.file_list = self.__get_file(path)
        self.count += self.number
        self.folder_data = self.query(self.wql_folder)
        self.logger.debug(""folder_data: {}"".format(self.folder_data))
        if self.folder_data:
            for folder in self.folder_data:
                self.new_path = (
                    folder['Name'].split("":"")[1] +
                    ""\\"").replace(
                    ""\\"",
                    ""\\\\"")
                self.__get_folder(self.new_path)
        return self.count, self.file_list

    def filenumber_handle(self):
        

        self.file_list = []
        self.count = 0
        status = self.ok

        if self.args.recursion:
            self.__result, self.__file_list = self.__get_folder(self.args.path)
        else:
            self.__result, self.__file_list = self.__get_file(self.args.path)

        
        if self.__result > self.args.critical:
            status = self.critical
        elif self.__result > self.args.warning:
            status = self.warning
        else:
            status = self.ok

        
        self.shortoutput = ""Found {0} files in {1}."".format(self.__result,
                                                            self.args.path)
        self.logger.debug(""file_list: {}"".format(self.__file_list))
        [self.longoutput.append(file_data.get('Name'))
         for file_data in self.__file_list]
        self.perfdata.append(""{path}={result};{warn};{crit};0;"".format(
            crit=self.args.critical,
            warn=self.args.warning,
            result=self.__result,
            path=self.args.path))

        
        status(self.output(long_output_limit=None))
        self.logger.debug(""Return status and exit to Nagios."")


class FileAge(Wmi):

    


    def __init__(self, *args, **kwargs):
        super(FileAge, self).__init__(*args, **kwargs)
        self.logger.debug(""Init FileAge"")

    def define_sub_options(self):
        super(FileAge, self).define_sub_options()
        self.fa_parser = self.subparsers.add_parser('fileage',
                                                    help='Get file age.',
                                                    description='Options\
                                                    for fileage.')
        self.fa_parser.add_argument('-q', '--query',
                                    required=False,
                                    help='wql for wmi.',
                                    dest='query')
        self.fa_parser.add_argument('-d', '--drive',
                                    required=True,
                                    help='the windows driver, like C:',
                                    dest='drive')
        self.fa_parser.add_argument('-p', '--path',
                                    default=""\\\\"",
                                    required=False,
                                    help='the folder, default is %(default)s',
                                    dest='path')
        self.fa_parser.add_argument(
            '-f',
            '--filename',
            default=""%%"",
            required=False,
            help='the filename, default is %(default)s',
            dest='filename')
        self.fa_parser.add_argument(
            '-e', '--extension', default=""%%"", required=False,
            help='the file extension, default is %(default)s',
            dest='extension')
        self.fa_parser.add_argument('-R', '--recursion',
                                    action='store_true',
                                    help='Recursive count file under path.',
                                    dest='recursion')
        self.fa_parser.add_argument(
            '-w',
            '--warning',
            default=30,
            type=int,
            required=False,
            help='Warning minute of file, default is %(default)s',
            dest='warning')
        self.fa_parser.add_argument(
            '-c',
            '--critical',
            default=60,
            type=int,
            required=False,
            help='Critical minute of file, default is %(default)s',
            dest='critical')

    def __get_file(self, path):
        self.wql_file = ""SELECT LastModified FROM CIM_DataFile WHERE Drive='{0}' \
            AND Path='{1}' AND FileName LIKE '{2}' AND Extension LIKE '{3}'"".format(
            self.args.drive, path, self.args.filename, self.args.extension)
        self.file_data = self.query(self.wql_file)
        [self.file_list.append(file_data) for file_data in self.file_data]
        self.logger.debug(""file_data: {}"".format(self.file_data))
        return self.file_list

    def __get_folder(self, path):
        self.wql_folder = ""SELECT FileName FROM CIM_Directory WHERE Drive='{0}' AND Path='{1}'"".format(
            self.args.drive, path)
        self.file_list = self.__get_file(path)
        self.folder_data = self.query(self.wql_folder)
        self.logger.debug(""folder_data: {}"".format(self.folder_data))
        if self.folder_data:
            for folder in self.folder_data:
                self.new_path = (
                    folder['Name'].split("":"")[1] +
                    ""\\"").replace(
                    ""\\"",
                    ""\\\\"")
                self.__get_folder(self.new_path)
        return self.file_list

    def __get_current_datetime(self):
        

        self.wql_time = ""SELECT LocalDateTime FROM Win32_OperatingSystem""
        self.current_time = self.query(self.wql_time)
        
        self.current_time_string = str(
            self.current_time[0].get('LocalDateTime').split('.')[0])
        
        self.current_time_format = datetime.datetime.strptime(
            self.current_time_string, '%Y%m%d%H%M%S')
        
        
        return self.current_time_format

    def fileage_handle(self):
        

        self.file_list = []
        self.ok_file = []
        self.warn_file = []
        self.crit_file = []
        status = self.ok

        if self.args.recursion:
            self.__file_list = self.__get_folder(self.args.path)
        else:
            self.__file_list = self.__get_file(self.args.path)
        self.logger.debug(""file_list: {}"".format(self.__file_list))
        
        
        

        for file_dict in self.__file_list:
            self.filename = file_dict.get('Name')
            if self.filename and self.filename != 'Name':
                self.logger.debug(
                    ""===== start to compare {} ====="".format(
                        self.filename))

                self.file_datetime_string = file_dict.get(
                    'LastModified').split('.')[0]
                self.file_datetime = datetime.datetime.strptime(
                    self.file_datetime_string, '%Y%m%d%H%M%S')
                self.logger.debug(
                    ""file_datetime: {}"".format(
                        self.file_datetime))

                self.current_datetime = self.__get_current_datetime()
                self.logger.debug(
                    ""current_datetime: {}"".format(
                        self.current_datetime))

                self.__delta_datetime = self.current_datetime - self.file_datetime
                self.logger.debug(
                    ""delta_datetime: {}"".format(
                        self.__delta_datetime))
                self.logger.debug(
                    ""warn_datetime: {}"".format(
                        datetime.timedelta(
                            minutes=self.args.warning)))
                self.logger.debug(
                    ""crit_datetime: {}"".format(
                        datetime.timedelta(
                            minutes=self.args.critical)))
                if self.__delta_datetime > datetime.timedelta(
                        minutes=self.args.critical):
                    self.crit_file.append(self.filename)
                elif self.__delta_datetime > datetime.timedelta(minutes=self.args.warning):
                    self.warn_file.append(self.filename)
                else:
                    self.ok_file.append(self.filename)

        
        if self.crit_file:
            status = self.critical
        elif self.warn_file:
            status = self.warning
        else:
            status = self.ok

        
        self.shortoutput = ""Found {0} files out of date."".format(
            len(self.crit_file))
        if self.crit_file:
            self.longoutput.append(""===== Critical File out of date ===="")
        [self.longoutput.append(filename)
         for filename in self.crit_file if self.crit_file]
        if self.warn_file:
            self.longoutput.append(""===== Warning File out of date ===="")
        [self.longoutput.append(filename)
         for filename in self.warn_file if self.warn_file]
        if self.ok_file:
            self.longoutput.append(""===== OK File out of date ===="")
        [self.longoutput.append(filename)
         for filename in self.ok_file if self.ok_file]
        self.perfdata.append(""{path}={result};{warn};{crit};0;"".format(
            crit=self.args.critical,
            warn=self.args.warning,
            result=len(self.crit_file),
            path=self.args.drive + self.args.path))

        
        status(self.output(long_output_limit=None))
        self.logger.debug(""Return status and exit to Nagios."")


class SqlserverLocks(Wmi):

    


    def __init__(self, *args, **kwargs):
        super(SqlserverLocks, self).__init__(*args, **kwargs)
        self.logger.debug(""Init SqlserverLocks"")

    def define_sub_options(self):
        super(SqlserverLocks, self).define_sub_options()
        self.sl_parser = self.subparsers.add_parser(
            'sqlserverlocks', help='Options for SqlserverLocks',
            description='All options for SqlserverLocks')
        self.sl_parser.add_argument('-q', '--query',
                                    required=False,
                                    help='wql for wmi.',
                                    dest='query')
        self.sl_parser.add_argument(
            '-m', '--mode', required=True,
            help='From [""LockTimeoutsPersec"", ""LockWaitsPersec"", ""NumberofDeadlocksPersec""]',
            dest='mode')
        self.sl_parser.add_argument('-w', '--warning',
                                    default=0,
                                    type=int,
                                    required=False,
                                    help='Default is %(default)s',
                                    dest='warning')
        self.sl_parser.add_argument('-c', '--critical',
                                    default=0,
                                    type=int,
                                    required=False,
                                    help='Default is %(default)s',
                                    dest='critical')

    def sqlserverlocks_handle(self):
        self.ok_list = []
        self.warn_list = []
        self.crit_list = []
        status = self.ok

        if self.args.mode == ""LockTimeoutsPersec"":
            self.wql = ""select LockTimeoutsPersec from Win32_PerfFormattedData_MSSQLSERVER_SQLServerLocks""
        elif self.args.mode == ""LockWaitsPersec"":
            self.wql = ""select LockWaitsPersec from Win32_PerfFormattedData_MSSQLSERVER_SQLServerLocks""
        elif self.args.mode == ""NumberofDeadlocksPersec"":
            self.wql = ""select NumberofDeadlocksPersec from Win32_PerfFormattedData_MSSQLSERVER_SQLServerLocks""
        else:
            self.unknown(""Unknown SqlServerLocks options"")

        self.__results = self.query(self.wql)
        self.logger.debug(""results: {}"".format(self.__results))
        
        
        for lock_dict in self.__results:
            self.name = lock_dict.get('Name')
            self.logger.debug(""name: {}"".format(self.name))
            self.value = int(lock_dict.get(self.args.mode))
            self.logger.debug(""value: {}"".format(self.value))
            if self.value > self.args.critical:
                self.crit_list.append(self.name + "" : "" + self.value)
            elif self.value > self.args.warning:
                self.warn_list.append(self.name + "" : "" + self.value)
            else:
                self.ok_list.append(self.name + "" : "" + str(self.value))

        if self.crit_list:
            status = self.critical
        elif self.warn_list:
            status = self.warning
        else:
            status = self.ok

        self.shortoutput = ""Found {0} {1} critical."".format(
            len(self.crit_list), self.args.mode)
        if self.crit_list:
            self.longoutput.append(""===== Critical ===="")
        [self.longoutput.append(filename)
         for filename in self.crit_list if self.crit_list]
        if self.warn_list:
            self.longoutput.append(""===== Warning ===="")
        [self.longoutput.append(filename)
         for filename in self.warn_list if self.warn_list]
        if self.ok_list:
            self.longoutput.append(""===== OK ===="")
        [self.longoutput.append(filename)
         for filename in self.ok_list if self.ok_list]
        self.perfdata.append(""{mode}={result};{warn};{crit};0;"".format(
            crit=self.args.critical,
            warn=self.args.warning,
            result=len(self.crit_list),
            mode=self.args.mode))

        
        status(self.output(long_output_limit=None))
        self.logger.debug(""Return status and exit to Nagios."")


class Register(FileNumber, FileAge, SqlserverLocks):

    


    def __init__(self, *args, **kwargs):
        super(Register, self).__init__(*args, **kwargs)


def main():
    

    plugin = Register()
    if plugin.args.option == 'filenumber':
        plugin.filenumber_handle()
    elif plugin.args.option == 'fileage':
        plugin.fileage_handle()
    elif plugin.args.option == 'sqlserverlocks':
        plugin.sqlserverlocks_handle()
    else:
        plugin.unknown(""Unknown actions."")

if __name__ == ""__main__"":
    main()
","
import collections

def freeze(value):
    

    if isinstance(value, list):
        return FrozenList(*value)
    if isinstance(value, dict):
        return FrozenDict(**value)
    return value

def thaw(value):
    if isinstance(value, FrozenList):
        return value.thaw()
    if isinstance(value, FrozenDict):
        return value.thaw()
    return value

class FrozenDict(collections.Mapping):
    

    def __init__(self, **kwargs):
        self.__data = {key : freeze(value) for key, value in kwargs.items()}

    def thaw(self):
        return {key : thaw(value) for key, value in self.__data.items()}

    def __eq__(self, other):
        if not isinstance(other, FrozenDict):
            return self.thaw() == other
        return super(FrozenDict, self).__eq__(self, other)

    def __getitem__(self, item):
        return self.__data[item]

    def __iter__(self):
        return iter(self.__data)

    def __len__(self):
        return len(self.__data)

    def __repr__(self):
        return repr(self.__data)

class FrozenList(collections.Sequence):
    

    def __init__(self, *args):
        self.__data = [freeze(value) for value in args]

    def thaw(self):
        return [thaw(value) for value in self.__data]

    def __eq__(self, other):
        if not isinstance(other, FrozenList):
            return self.thaw() == other
        return super(FrozenList, self).__eq__(self, other)

    def __getitem__(self, item):
        return self.__data[item]

    def __iter__(self):
        return iter(self.__data)

    def __len__(self):
        return len(self.__data)

    def __repr__(self):
        return repr(self.__data)","


import os

import imageio

import numpy as np

import tensorlayer as tl
from tensorlayer.lazy_imports import LazyImport
cv2 = LazyImport(""cv2"")





__all__ = [
    'read_image',
    'read_images',
    'save_image',
    'save_images',
    'draw_boxes_and_labels_to_image',
    'draw_mpii_people_to_image',
    'frame',
    'CNN2d',
    'images2d',
    'tsne_embedding',
    'draw_weights',
    'W',
]


def read_image(image, path=''):
    

    return imageio.imread(os.path.join(path, image))


def read_images(img_list, path='', n_threads=10, printable=True):
    

    imgs = []
    for idx in range(0, len(img_list), n_threads):
        b_imgs_list = img_list[idx:idx + n_threads]
        b_imgs = tl.prepro.threading_data(b_imgs_list, fn=read_image, path=path)
        
        imgs.extend(b_imgs)
        if printable:
            tl.logging.info('read %d from %s' % (len(imgs), path))
    return imgs


def save_image(image, image_path='_temp.png'):
    

    try:  
        imageio.imwrite(image_path, image)
    except Exception:  
        imageio.imwrite(image_path, image[:, :, 0])


def save_images(images, size, image_path='_temp.png'):
    

    if len(images.shape) == 3:  
        images = images[:, :, :, np.newaxis]

    def merge(images, size):
        h, w = images.shape[1], images.shape[2]
        img = np.zeros((h * size[0], w * size[1], 3), dtype=images.dtype)
        for idx, image in enumerate(images):
            i = idx % size[1]
            j = idx // size[1]
            img[j * h:j * h + h, i * w:i * w + w, :] = image
        return img

    def imsave(images, size, path):
        if np.max(images) <= 1 and (-1 <= np.min(images) < 0):
            images = ((images + 1) * 127.5).astype(np.uint8)
        elif np.max(images) <= 1 and np.min(images) >= 0:
            images = (images * 255).astype(np.uint8)

        return imageio.imwrite(path, merge(images, size))

    if len(images) > size[0] * size[1]:
        raise AssertionError(""number of images should be equal or less than size[0] * size[1] {}"".format(len(images)))

    return imsave(images, size, image_path)


def draw_boxes_and_labels_to_image(
        image, classes, coords, scores, classes_list, is_center=True, is_rescale=True, save_name=None
):
    

    if len(coords) != len(classes):
        raise AssertionError(""number of coordinates and classes are equal"")

    if len(scores) > 0 and len(scores) != len(classes):
        raise AssertionError(""number of scores and classes are equal"")

    
    image = image.copy()

    imh, imw = image.shape[0:2]
    thick = int((imh + imw) // 430)

    for i, _v in enumerate(coords):
        if is_center:
            x, y, x2, y2 = tl.prepro.obj_box_coord_centroid_to_upleft_butright(coords[i])
        else:
            x, y, x2, y2 = coords[i]

        if is_rescale:  
            x, y, x2, y2 = tl.prepro.obj_box_coord_scale_to_pixelunit([x, y, x2, y2], (imh, imw))

        cv2.rectangle(
            image,
            (int(x), int(y)),
            (int(x2), int(y2)),  
            [0, 255, 0],
            thick
        )

        cv2.putText(
            image,
            classes_list[classes[i]] + (("" %.2f"" % (scores[i])) if (len(scores) != 0) else "" ""),
            (int(x), int(y)),  
            0,
            1.5e-3 * imh,  
            [0, 0, 256],  
            int(thick / 2) + 1
        )  

    if save_name is not None:
        
        save_image(image, save_name)
    
    
    return image


def draw_mpii_pose_to_image(image, poses, save_name='image.png'):
    

    
    
    image = image.copy()

    imh, imw = image.shape[0:2]
    thick = int((imh + imw) // 430)
    
    radius = int(thick * 1.5)

    if image.max() < 1:
        image = image * 255

    for people in poses:
        
        joint_pos = people['joint_pos']
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

        lines = [
            [(0, 1), [100, 255, 100]],
            [(1, 2), [50, 255, 50]],
            [(2, 6), [0, 255, 0]],  
            [(3, 4), [100, 100, 255]],
            [(4, 5), [50, 50, 255]],
            [(6, 3), [0, 0, 255]],  
            [(6, 7), [255, 255, 100]],
            [(7, 8), [255, 150, 50]],  
            [(8, 9), [255, 200, 100]],  
            [(10, 11), [255, 100, 255]],
            [(11, 12), [255, 50, 255]],
            [(12, 8), [255, 0, 255]],  
            [(8, 13), [0, 255, 255]],
            [(13, 14), [100, 255, 255]],
            [(14, 15), [200, 255, 255]]  
        ]
        for line in lines:
            start, end = line[0]
            if (start in joint_pos) and (end in joint_pos):
                cv2.line(
                    image,
                    (int(joint_pos[start][0]), int(joint_pos[start][1])),
                    (int(joint_pos[end][0]), int(joint_pos[end][1])),  
                    line[1],
                    thick
                )
                
                
        
        for pos in joint_pos.items():
            _, pos_loc = pos  
            pos_loc = (int(pos_loc[0]), int(pos_loc[1]))
            cv2.circle(image, center=pos_loc, radius=radius, color=(200, 200, 200), thickness=-1)
            
            

        
        head_rect = people['head_rect']
        if head_rect:  
            cv2.rectangle(
                image,
                (int(head_rect[0]), int(head_rect[1])),
                (int(head_rect[2]), int(head_rect[3])),  
                [0, 180, 0],
                thick
            )

    if save_name is not None:
        
        save_image(image, save_name)
    return image


draw_mpii_people_to_image = draw_mpii_pose_to_image


def frame(I=None, second=5, saveable=True, name='frame', cmap=None, fig_idx=12836):
    

    import matplotlib.pyplot as plt
    if saveable is False:
        plt.ion()
    plt.figure(fig_idx)  

    if len(I.shape) and I.shape[-1] == 1:  
        I = I[:, :, 0]

    plt.imshow(I, cmap)
    plt.title(name)
    
    

    if saveable:
        plt.savefig(name + '.pdf', format='pdf')
    else:
        plt.draw()
        plt.pause(second)


def CNN2d(CNN=None, second=10, saveable=True, name='cnn', fig_idx=3119362):
    

    import matplotlib.pyplot as plt
    
    
    n_mask = CNN.shape[3]
    n_row = CNN.shape[0]
    n_col = CNN.shape[1]
    n_color = CNN.shape[2]
    row = int(np.sqrt(n_mask))
    col = int(np.ceil(n_mask / row))
    plt.ion()  
    fig = plt.figure(fig_idx)
    count = 1
    for _ir in range(1, row + 1):
        for _ic in range(1, col + 1):
            if count > n_mask:
                break
            fig.add_subplot(col, row, count)
            
            
            
            
            
            if n_color == 1:
                plt.imshow(np.reshape(CNN[:, :, :, count - 1], (n_row, n_col)), cmap='gray', interpolation=""nearest"")
            elif n_color == 3:
                plt.imshow(
                    np.reshape(CNN[:, :, :, count - 1], (n_row, n_col, n_color)), cmap='gray', interpolation=""nearest""
                )
            else:
                raise Exception(""Unknown n_color"")
            plt.gca().xaxis.set_major_locator(plt.NullLocator())  
            plt.gca().yaxis.set_major_locator(plt.NullLocator())
            count = count + 1
    if saveable:
        plt.savefig(name + '.pdf', format='pdf')
    else:
        plt.draw()
        plt.pause(second)


def images2d(images=None, second=10, saveable=True, name='images', dtype=None, fig_idx=3119362):
    

    import matplotlib.pyplot as plt
    
    
    if dtype:
        images = np.asarray(images, dtype=dtype)
    n_mask = images.shape[0]
    n_row = images.shape[1]
    n_col = images.shape[2]
    n_color = images.shape[3]
    row = int(np.sqrt(n_mask))
    col = int(np.ceil(n_mask / row))
    plt.ion()  
    fig = plt.figure(fig_idx)
    count = 1
    for _ir in range(1, row + 1):
        for _ic in range(1, col + 1):
            if count > n_mask:
                break
            fig.add_subplot(col, row, count)
            
            
            
            
            if n_color == 1:
                plt.imshow(np.reshape(images[count - 1, :, :], (n_row, n_col)), cmap='gray', interpolation=""nearest"")
                
            elif n_color == 3:
                plt.imshow(images[count - 1, :, :], cmap='gray', interpolation=""nearest"")
                
            else:
                raise Exception(""Unknown n_color"")
            plt.gca().xaxis.set_major_locator(plt.NullLocator())  
            plt.gca().yaxis.set_major_locator(plt.NullLocator())
            count = count + 1
    if saveable:
        plt.savefig(name + '.pdf', format='pdf')
    else:
        plt.draw()
        plt.pause(second)


def tsne_embedding(embeddings, reverse_dictionary, plot_only=500, second=5, saveable=False, name='tsne', fig_idx=9862):
    

    import matplotlib.pyplot as plt

    def plot_with_labels(low_dim_embs, labels, figsize=(18, 18), second=5, saveable=True, name='tsne', fig_idx=9862):

        if low_dim_embs.shape[0] < len(labels):
            raise AssertionError(""More labels than embeddings"")

        if saveable is False:
            plt.ion()
            plt.figure(fig_idx)

        plt.figure(figsize=figsize)  

        for i, label in enumerate(labels):
            x, y = low_dim_embs[i, :]
            plt.scatter(x, y)
            plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')

        if saveable:
            plt.savefig(name + '.pdf', format='pdf')
        else:
            plt.draw()
            plt.pause(second)

    try:
        from sklearn.manifold import TSNE
        from six.moves import xrange

        tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)
        
        low_dim_embs = tsne.fit_transform(embeddings[:plot_only, :])
        labels = [reverse_dictionary[i] for i in xrange(plot_only)]
        plot_with_labels(low_dim_embs, labels, second=second, saveable=saveable, name=name, fig_idx=fig_idx)

    except ImportError:
        _err = ""Please install sklearn and matplotlib to visualize embeddings.""
        tl.logging.error(_err)
        raise ImportError(_err)


def draw_weights(W=None, second=10, saveable=True, shape=None, name='mnist', fig_idx=2396512):
    

    if shape is None:
        shape = [28, 28]

    import matplotlib.pyplot as plt
    if saveable is False:
        plt.ion()
    fig = plt.figure(fig_idx)  
    n_units = W.shape[1]

    num_r = int(np.sqrt(n_units))  
    num_c = int(np.ceil(n_units / num_r))
    count = int(1)
    for _row in range(1, num_r + 1):
        for _col in range(1, num_c + 1):
            if count > n_units:
                break
            fig.add_subplot(num_r, num_c, count)
            
            
            
            feature = W[:, count - 1] / np.sqrt((W[:, count - 1]**2).sum())
            
            
            
            
            
            
            
            plt.imshow(
                np.reshape(feature, (shape[0], shape[1])), cmap='gray', interpolation=""nearest""
            )  
            
            
            
            plt.gca().xaxis.set_major_locator(plt.NullLocator())  
            plt.gca().yaxis.set_major_locator(plt.NullLocator())
            count = count + 1
    if saveable:
        plt.savefig(name + '.pdf', format='pdf')
    else:
        plt.draw()
        plt.pause(second)


W = draw_weights
","

















from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

from math import sqrt
import numpy as np
import tensorflow as tf

from tensorforce import TensorForceError, util
import tensorforce.core.networks


class Layer(object):
    


    def __init__(self, named_tensors=None, scope='layer', summary_labels=None):
        

        self.scope = scope
        self.summary_labels = set(summary_labels or ())

        self.named_tensors = named_tensors
        self.variables = dict()
        self.all_variables = dict()

        def custom_getter(getter, name, registered=False, **kwargs):
            variable = getter(name=name, registered=True, **kwargs)
            if registered:
                pass
            elif name in self.all_variables:
                assert variable is self.all_variables[name]
                if kwargs.get('trainable', True):
                    assert variable is self.variables[name]
                    if 'variables' in self.summary_labels:
                        tf.contrib.summary.histogram(name=name, tensor=variable)
            else:
                self.all_variables[name] = variable
                if kwargs.get('trainable', True):
                    self.variables[name] = variable
                    if 'variables' in self.summary_labels:
                        tf.contrib.summary.histogram(name=name, tensor=variable)
            return variable

        self.apply = tf.make_template(
            name_=(scope + '/apply'),
            func_=self.tf_apply,
            custom_getter_=custom_getter
        )
        self.regularization_loss = tf.make_template(
            name_=(scope + '/regularization-loss'),
            func_=self.tf_regularization_loss,
            custom_getter_=custom_getter
        )

    def tf_apply(self, x, update):
        

        raise NotImplementedError

    def tf_regularization_loss(self):
        

        return None

    def internals_spec(self):
        

        return dict()

    def get_variables(self, include_nontrainable=False):
        

        if include_nontrainable:
            return [self.all_variables[key] for key in sorted(self.all_variables)]
        else:
            return [self.variables[key] for key in sorted(self.variables)]

    @staticmethod
    def from_spec(spec, kwargs=None):
        

        layer = util.get_object(
            obj=spec,
            predefined_objects=tensorforce.core.networks.layers,
            kwargs=kwargs
        )
        assert isinstance(layer, Layer)
        return layer


class Input(Layer):
    


    def __init__(
        self,
        names,
        aggregation_type='concat',
        axis=1,
        named_tensors=None,
        scope='input',
        summary_labels=()
    ):
        

        self.names = names
        self.aggregation_type = aggregation_type
        self.axis = axis
        super(Input, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        if isinstance(self.names, str):
            if self.names == '*' or self.names == 'previous':
                
                return x
            elif self.names in self.named_tensors:
                return self.named_tensors[self.names]
            else:
                keys = sorted(self.named_tensors)
                raise TensorForceError(
                    'Input ""{}"" doesn\'t exist, Available inputs: {}'.format(self.names, keys)
                )

        inputs = list()
        max_shape = ()
        for name in self.names:
            if name == '*' or name == 'previous':
                
                tensor = x
            elif name in self.named_tensors:
                tensor = self.named_tensors[name]
            else:
                keys = sorted(self.named_tensors)
                raise TensorForceError(
                    'Input ""{}"" doesn\'t exist, Available inputs: {}'.format(name, keys)
                )
            inputs.append(tensor)
            shape = util.shape(x=tensor)
            if len(shape) > len(max_shape):
                max_shape = shape

        for n, tensor in enumerate(inputs):
            shape = util.shape(x=tensor)
            if len(shape) < len(max_shape):
                
                for i in range(len(shape), len(max_shape)):
                    
                    tensor = tf.expand_dims(input=tensor, axis=i)
                inputs[n] = tensor
            
            

        if self.aggregation_type == 'concat':
            x = tf.concat(values=inputs, axis=self.axis)
        elif self.aggregation_type == 'stack':
            x = tf.stack(values=inputs, axis=self.axis)
        elif self.aggregation_type == 'sum':
            x = tf.stack(values=inputs, axis=self.axis)
            x = tf.reduce_sum(input_tensor=x, axis=self.axis)
        elif self.aggregation_type == 'product':
            x = tf.stack(values=inputs, axis=self.axis)
            x = tf.reduce_prod(input_tensor=x, axis=self.axis)
        else:
            raise NotImplementedError

        return x


class Output(Layer):
    


    def __init__(
        self,
        name,
        named_tensors=None,
        scope='output',
        summary_labels=()
    ):
        

        self.name = name
        super(Output, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        self.named_tensors[self.name] = x
        return x


class TFLayer(Layer):
    


    tf_layers = dict(
        average_pooling1d=tf.layers.AveragePooling1D,
        average_pooling2d=tf.layers.AveragePooling2D,
        average_pooling3d=tf.layers.AveragePooling3D,
        batch_normalization=tf.layers.BatchNormalization,
        conv1d=tf.layers.Conv1D,
        conv2d=tf.layers.Conv2D,
        conv2d_transpose=tf.layers.Conv2DTranspose,
        conv3d=tf.layers.Conv3D,
        conv3d_transpose=tf.layers.Conv3DTranspose,
        dense=tf.layers.Dense,
        dropout=tf.layers.Dropout,
        flatten=tf.layers.Flatten,
        max_pooling1d=tf.layers.MaxPooling1D,
        max_pooling2d=tf.layers.MaxPooling2D,
        max_pooling3d=tf.layers.MaxPooling3D,
        separable_conv2d=tf.layers.SeparableConv2D
    )

    def __init__(self, layer, named_tensors=None, scope='tf-layer', summary_labels=(), **kwargs):
        

        self.layer_spec = layer
        self.layer = util.get_object(obj=layer, predefined_objects=TFLayer.tf_layers, kwargs=kwargs)
        self.first_scope = None

        super(TFLayer, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        if self.first_scope is None:
            
            self.first_scope = tf.contrib.framework.get_name_scope()
        if isinstance(self.layer, (tf.layers.BatchNormalization, tf.layers.Dropout)):
            return self.layer(inputs=x, training=update)
        else:
            return self.layer(inputs=x)

    def tf_regularization_loss(self):
        regularization_losses = tf.get_collection(
            key=tf.GraphKeys.REGULARIZATION_LOSSES,
            scope=self.first_scope
        )
        if len(regularization_losses) > 0:
            return tf.add_n(inputs=regularization_losses)
        else:
            return None


class Nonlinearity(Layer):
    


    def __init__(self,
        name='relu',
        alpha=None,
        beta=1.0,
        max=None,
        min=None,
        named_tensors=None,
        scope='nonlinearity',
        summary_labels=()
    ):
        

        self.name = name
        self.alpha = None
        self.max = None
        self.min = None
        self.beta_learn = False
        super(Nonlinearity, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

        if max is not None:
            self.max = float(max)

        if min is not None:
            self.min = float(min)

        if alpha is not None:
            self.alpha = float(alpha)

        if beta == 'learn':
            self.beta_learn = True
            self.beta = None
        else:
            self.beta = tf.constant(float(beta), dtype=util.tf_dtype('float'))

    def tf_apply(self, x, update):
        if self.beta_learn:
            self.beta = tf.get_variable(
                name='beta',
                shape=(),
                dtype=tf.float32,
                initializer=tf.ones_initializer()
            )

        if self.max is not None:
            x = tf.minimum(x=(self.beta * x), y=self.max)

        if self.min is not None:
            x = tf.maximum(x=(self.beta * x), y=self.min)

        if self.name == 'elu':
            x = tf.nn.elu(features=(self.beta * x))

        elif self.name == 'none':
            x = tf.identity(input=(self.beta * x))

        elif self.name == 'relu':
            x = tf.nn.relu(features=(self.beta * x))
            if 'relu' in self.summary_labels:
                non_zero = tf.cast(x=tf.count_nonzero(input_tensor=x), dtype=tf.float32)
                size = tf.cast(x=tf.reduce_prod(input_tensor=tf.shape(input=x)), dtype=tf.float32)
                tf.contrib.summary.scalar(name='relu', tensor=(non_zero / size))

        elif self.name == 'selu':
            
            x = tf.nn.selu(features=(self.beta * x))

        elif self.name == 'sigmoid':
            x = tf.sigmoid(x=(self.beta * x))

        elif self.name == 'swish':
            
            x = tf.sigmoid(x=(self.beta * x)) * x

        elif self.name == 'lrelu' or self.name == 'leaky_relu':
            if self.alpha is None:
                
                self.alpha = 0.2
            x = tf.nn.leaky_relu(features=(self.beta * x), alpha=self.alpha)

        elif self.name == 'crelu':
            x = tf.nn.crelu(features=(self.beta * x))

        elif self.name == 'softmax':
            x = tf.nn.softmax(logits=(self.beta * x))

        elif self.name == 'softplus':
            x = tf.nn.softplus(features=(self.beta * x))

        elif self.name == 'softsign':
            x = tf.nn.softsign(features=(self.beta * x))

        elif self.name == 'tanh':
            x = tf.nn.tanh(x=(self.beta * x))

        else:
            raise TensorForceError('Invalid non-linearity: {}'.format(self.name))

        if 'beta' in self.summary_labels:
            tf.contrib.summary.scalar(name='beta', tensor=self.beta)

        return x


class Dropout(Layer):
    


    def __init__(self, rate=0.0, named_tensors=None, scope='dropout', summary_labels=()):
        self.rate = rate
        super(Dropout, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        dropout = tf.nn.dropout(x=x, keep_prob=(1.0 - self.rate))
        return tf.where(condition=update, x=dropout, y=x)


class Flatten(Layer):
    


    def __init__(self, named_tensors=None, scope='flatten', summary_labels=()):
        super(Flatten, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        return tf.reshape(tensor=x, shape=(-1, util.prod(util.shape(x)[1:])))


class Pool2d(Layer):
    


    def __init__(
        self,
        pooling_type='max',
        window=2,
        stride=2,
        padding='SAME',
        named_tensors=None,
        scope='pool2d',
        summary_labels=()
    ):
        

        self.pooling_type = pooling_type
        if isinstance(window, int):
            self.window = (1, window, window, 1)
        elif len(window) == 2:
            self.window = (1, window[0], window[1], 1)
        else:
            raise TensorForceError('Invalid window {} for pool2d layer, must be of size 2'.format(window))
        if isinstance(stride, int):
            self.stride = (1, stride, stride, 1)
        elif len(window) == 2:
            self.stride = (1, stride[0], stride[1], 1)
        else:
            raise TensorForceError('Invalid stride {} for pool2d layer, must be of size 2'.format(stride))
        self.padding = padding
        super(Pool2d, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        if self.pooling_type == 'average':
            x = tf.nn.avg_pool(value=x, ksize=self.window, strides=self.stride, padding=self.padding)

        elif self.pooling_type == 'max':
            x = tf.nn.max_pool(value=x, ksize=self.window, strides=self.stride, padding=self.padding)

        else:
            raise TensorForceError('Invalid pooling type: {}'.format(self.name))

        return x


class Embedding(Layer):
    


    def __init__(
        self,
        indices,
        size,
        l2_regularization=0.0,
        l1_regularization=0.0,
        named_tensors=None,
        scope='embedding',
        summary_labels=()
    ):
        

        self.indices = indices
        self.size = size
        self.l2_regularization = l2_regularization
        self.l1_regularization = l1_regularization
        super(Embedding, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        stddev = min(0.1, sqrt(1.0 / self.size))
        weights_init = tf.random_normal_initializer(mean=0.0, stddev=stddev, dtype=tf.float32)
        self.weights = tf.get_variable(
            name='embeddings',
            shape=(self.indices, self.size),
            dtype=tf.float32,
            initializer=weights_init
        )
        return tf.nn.embedding_lookup(params=self.weights, ids=x)

    def tf_regularization_loss(self):
        regularization_loss = super(Embedding, self).tf_regularization_loss()
        if regularization_loss is None:
            losses = list()
        else:
            losses = [regularization_loss]

        if self.l2_regularization > 0.0:
            losses.append(self.l2_regularization * tf.nn.l2_loss(t=self.weights))

        if self.l1_regularization > 0.0:
            losses.append(self.l1_regularization * tf.reduce_sum(input_tensor=tf.abs(x=self.weights)))

        if len(losses) > 0:
            return tf.add_n(inputs=losses)
        else:
            return None


class Linear(Layer):
    


    def __init__(
        self,
        size,
        weights=None,
        bias=True,
        l2_regularization=0.0,
        l1_regularization=0.0,
        trainable=True,
        named_tensors=None,
        scope='linear',
        summary_labels=()
    ):
        

        self.size = size
        self.weights_init = weights
        self.bias_init = bias
        self.l2_regularization = l2_regularization
        self.l1_regularization = l1_regularization
        self.trainable = trainable
        super(Linear, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update=False):
        if util.rank(x) != 2:
            raise TensorForceError(
                'Invalid input rank for linear layer: {}, must be 2.'.format(util.rank(x))
            )

        if self.size is None:  
            self.size = x.shape[1].value

        weights_shape = (x.shape[1].value, self.size)

        if self.weights_init is None:
            stddev = min(0.1, sqrt(2.0 / (x.shape[1].value + self.size)))
            self.weights_init = tf.random_normal_initializer(mean=0.0, stddev=stddev, dtype=tf.float32)

        elif isinstance(self.weights_init, dict):
            if 'name' in self.weights_init:
                if self.weights_init['name'] == 'msra':
                    slope = 0.25
                    if 'slope' in self.weights_init:
                        slope = self.weights_init['slope']
                    magnitude = 2.0 / (1.0 + slope ** 2)
                    stddev = sqrt(magnitude * 2.0 / (x.shape[1].value + self.size))
                    self.weights_init = tf.random_normal_initializer(mean=0.0, stddev=stddev, dtype=tf.float32)
            else:
                raise TensorForceError(
                    'Linear weights init with dict does not has name attribute, weight_init={}'.format(self.weights_init)
                )

        elif isinstance(self.weights_init, float):
            if self.weights_init == 0.0:
                self.weights_init = tf.zeros_initializer(dtype=tf.float32)
            else:
                self.weights_init = tf.constant_initializer(value=self.weights_init, dtype=tf.float32)

        elif isinstance(self.weights_init, list):
            self.weights_init = np.asarray(self.weights_init, dtype=np.float32)
            if self.weights_init.shape != weights_shape:
                raise TensorForceError(
                    'Weights shape {} does not match expected shape {} '.format(self.weights_init.shape, weights_shape)
                )
            self.weights_init = tf.constant_initializer(value=self.weights_init, dtype=tf.float32)

        elif isinstance(self.weights_init, np.ndarray):
            if self.weights_init.shape != weights_shape:
                raise TensorForceError(
                    'Weights shape {} does not match expected shape {} '.format(self.weights_init.shape, weights_shape)
                )
            self.weights_init = tf.constant_initializer(value=self.weights_init, dtype=tf.float32)

        elif isinstance(self.weights_init, tf.Tensor):
            if util.shape(self.weights_init) != weights_shape:
                raise TensorForceError(
                    'Weights shape {} does not match expected shape {} '.format(self.weights_init.shape, weights_shape)
                )

        bias_shape = (self.size,)

        if isinstance(self.bias_init, bool):
            if self.bias_init:
                self.bias_init = tf.zeros_initializer(dtype=tf.float32)
            else:
                self.bias_init = None

        elif isinstance(self.bias_init, float):
            if self.bias_init == 0.0:
                self.bias_init = tf.zeros_initializer(dtype=tf.float32)
            else:
                self.bias_init = tf.constant_initializer(value=self.bias_init, dtype=tf.float32)

        elif isinstance(self.bias_init, list):
            self.bias_init = np.asarray(self.bias_init, dtype=np.float32)
            if self.bias_init.shape != bias_shape:
                raise TensorForceError(
                    'Bias shape {} does not match expected shape {} '.format(self.bias_init.shape, bias_shape)
                )
            self.bias_init = tf.constant_initializer(value=self.bias_init, dtype=tf.float32)

        elif isinstance(self.bias_init, np.ndarray):
            if self.bias_init.shape != bias_shape:
                raise TensorForceError(
                    'Bias shape {} does not match expected shape {} '.format(self.bias_init.shape, bias_shape)
                )
            self.bias_init = tf.constant_initializer(value=self.bias_init, dtype=tf.float32)

        elif isinstance(self.bias_init, tf.Tensor):
            if util.shape(self.bias_init) != bias_shape:
                raise TensorForceError(
                    'Bias shape {} does not match expected shape {} '.format(self.bias_init.shape, bias_shape)
                )

        if isinstance(self.weights_init, tf.Tensor):
            self.weights = self.weights_init
        else:
            self.weights = tf.get_variable(
                name='W',
                shape=weights_shape,
                dtype=tf.float32,
                initializer=self.weights_init,
                trainable=self.trainable
            )

        x = tf.matmul(a=x, b=self.weights)

        if self.bias_init is None:
            self.bias = None

        else:
            if isinstance(self.bias_init, tf.Tensor):
                self.bias = self.bias_init
            else:
                self.bias = tf.get_variable(
                    name='b',
                    shape=bias_shape,
                    dtype=tf.float32,
                    initializer=self.bias_init,
                    trainable=self.trainable)

            x = tf.nn.bias_add(value=x, bias=self.bias)

        return x

    def tf_regularization_loss(self):
        regularization_loss = super(Linear, self).tf_regularization_loss()
        if regularization_loss is None:
            losses = list()
        else:
            losses = [regularization_loss]

        if self.l2_regularization > 0.0:
            losses.append(self.l2_regularization * tf.nn.l2_loss(t=self.weights))
            if self.bias is not None:
                losses.append(self.l2_regularization * tf.nn.l2_loss(t=self.bias))

        if self.l1_regularization > 0.0:
            losses.append(self.l1_regularization * tf.reduce_sum(input_tensor=tf.abs(x=self.weights)))
            if self.bias is not None:
                losses.append(self.l1_regularization * tf.reduce_sum(input_tensor=tf.abs(x=self.bias)))

        if len(losses) > 0:
            return tf.add_n(inputs=losses)
        else:
            return None


class Dense(Layer):
    


    def __init__(
        self,
        size=None,
        weights=None,
        bias=True,
        activation='relu',
        l2_regularization=0.0,
        l1_regularization=0.0,
        skip=False,
        trainable=True,
        named_tensors=None,
        scope='dense',
        summary_labels=(),
    ):
        

        self.skip = skip
        if self.skip and size is not None:
            raise TensorForceError(
                'Dense Layer SKIP connection needs Size=None, uses input shape '
                'sizes to create skip connection network, please delete ""size"" parameter'
            )

        self.linear = Linear(
            size=size,
            weights=weights,
            bias=bias,
            l2_regularization=l2_regularization,
            l1_regularization=l1_regularization,
            summary_labels=summary_labels,
            trainable=trainable
        )
        if self.skip:
            self.linear_skip = Linear(
                size=size,
                bias=bias,
                l2_regularization=l2_regularization,
                l1_regularization=l1_regularization,
                summary_labels=summary_labels,
                trainable=trainable
            )
        
        
        self.nonlinearity = Nonlinearity(summary_labels=summary_labels, **util.prepare_kwargs(activation))
        super(Dense, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        xl1 = self.linear.apply(x=x, update=update)
        xl1 = self.nonlinearity.apply(x=xl1, update=update)
        if self.skip:
            xl2 = self.linear_skip.apply(x=xl1, update=update)
            xl2 = self.nonlinearity.apply(x=(xl2 + x), update=update)  
        else:
            xl2 = xl1

        if 'activations' in self.summary_labels:
            tf.contrib.summary.histogram(name='activations', tensor=xl2)

        return xl2

    def tf_regularization_loss(self):
        regularization_loss = super(Dense, self).tf_regularization_loss()
        if regularization_loss is None:
            losses = list()
        else:
            losses = [regularization_loss]

        regularization_loss = self.linear.regularization_loss()
        if regularization_loss is not None:
            losses.append(regularization_loss)

        regularization_loss = self.nonlinearity.regularization_loss()
        if regularization_loss is not None:
            losses.append(regularization_loss)

        if self.skip:
            regularization_loss = self.linear_skip.regularization_loss()
            if regularization_loss is not None:
                losses.append(regularization_loss)

        if len(losses) > 0:
            return tf.add_n(inputs=losses)
        else:
            return None

    def get_variables(self, include_nontrainable=False):
        layer_variables = super(Dense, self).get_variables(include_nontrainable=include_nontrainable)
        linear_variables = self.linear.get_variables(include_nontrainable=include_nontrainable)
        if self.skip:
            linear_variables = linear_variables \
                               + self.linear_skip.get_variables(include_nontrainable=include_nontrainable)
        nonlinearity_variables = self.nonlinearity.get_variables(include_nontrainable=include_nontrainable)

        return layer_variables + linear_variables + nonlinearity_variables


class Dueling(Layer):
    


    def __init__(
        self,
        size,
        bias=False,
        activation='none',
        l2_regularization=0.0,
        l1_regularization=0.0,
        output=None,
        named_tensors=None,
        scope='dueling',
        summary_labels=()
    ):
        

        
        self.expectation_layer = Linear(
            size=1, bias=bias,
            l2_regularization=l2_regularization,
            l1_regularization=l1_regularization,
            summary_labels=summary_labels,
        )
        self.advantage_layer = Linear(
            size=size,
            bias=bias,
            l2_regularization=l2_regularization,
            l1_regularization=l1_regularization,
            summary_labels=summary_labels,
        )
        self.output = output
        self.nonlinearity = Nonlinearity(summary_labels=summary_labels, **util.prepare_kwargs(activation))
        super(Dueling, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        expectation = self.expectation_layer.apply(x=x, update=update)
        advantage = self.advantage_layer.apply(x=x, update=update)
        mean_advantage = tf.reduce_mean(input_tensor=advantage, axis=1, keep_dims=True)

        
        if type(self.output) is tuple and len(self.output) == 3:
            if self.named_tensors is not None:
                self.named_tensors[self.output[0]] = expectation
                self.named_tensors[self.output[1]] = advantage - mean_advantage
                self.named_tensors[self.output[2]] = mean_advantage
            if 'activations' in self.summary_labels:
                tf.contrib.summary.histogram(name=self.output[0], tensor=expectation)
                tf.contrib.summary.histogram(name=self.output[1], tensor=advantage - mean_advantage)
                tf.contrib.summary.histogram(name=self.output[2], tensor=mean_advantage)

        x = expectation + advantage - mean_advantage

        x = self.nonlinearity.apply(x=x, update=update)

        if 'activations' in self.summary_labels:
            tf.contrib.summary.histogram(name='activations', tensor=x)

        return x

    def tf_regularization_loss(self):
        regularization_loss = super(Dueling, self).tf_regularization_loss()
        if regularization_loss is None:
            losses = list()
        else:
            losses = [regularization_loss]

        regularization_loss = self.expectation_layer.regularization_loss()
        if regularization_loss is not None:
            losses.append(regularization_loss)

        regularization_loss = self.advantage_layer.regularization_loss()
        if regularization_loss is not None:
            losses.append(regularization_loss)

        if len(losses) > 0:
            return tf.add_n(inputs=losses)
        else:
            return None

    def get_variables(self, include_nontrainable=False):
        layer_variables = super(Dueling, self).get_variables(include_nontrainable=include_nontrainable)
        expectation_layer_variables = self.expectation_layer.get_variables(include_nontrainable=include_nontrainable)
        advantage_layer_variables = self.advantage_layer.get_variables(include_nontrainable=include_nontrainable)
        nonlinearity_variables = self.nonlinearity.get_variables(include_nontrainable=include_nontrainable)

        return layer_variables + expectation_layer_variables + advantage_layer_variables + nonlinearity_variables


class Conv1d(Layer):
    


    def __init__(
        self,
        size,
        window=3,
        stride=1,
        padding='SAME',
        bias=True,
        activation='relu',
        l2_regularization=0.0,
        l1_regularization=0.0,
        named_tensors=None,
        scope='conv1d',
        summary_labels=()
    ):
        

        self.size = size
        self.window = window
        self.stride = stride
        self.padding = padding
        self.bias = bias
        self.l2_regularization = l2_regularization
        self.l1_regularization = l1_regularization
        self.nonlinearity = Nonlinearity(summary_labels=summary_labels, **util.prepare_kwargs(activation))
        super(Conv1d, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        if util.rank(x) != 3:
            raise TensorForceError('Invalid input rank for conv1d layer: {}, must be 3'.format(util.rank(x)))

        filters_shape = (self.window, x.shape[2].value, self.size)
        stddev = min(0.1, sqrt(2.0 / self.size))
        filters_init = tf.random_normal_initializer(mean=0.0, stddev=stddev, dtype=tf.float32)
        self.filters = tf.get_variable(name='W', shape=filters_shape, dtype=tf.float32, initializer=filters_init)
        x = tf.nn.conv1d(value=x, filters=self.filters, stride=self.stride, padding=self.padding)

        if self.bias:
            bias_shape = (self.size,)
            bias_init = tf.zeros_initializer(dtype=tf.float32)
            self.bias = tf.get_variable(name='b', shape=bias_shape, dtype=tf.float32, initializer=bias_init)
            x = tf.nn.bias_add(value=x, bias=self.bias)

        x = self.nonlinearity.apply(x=x, update=update)

        if 'activations' in self.summary_labels:
            tf.contrib.summary.histogram(name='activations', tensor=x)

        return x

    def tf_regularization_loss(self):
        regularization_loss = super(Conv1d, self).tf_regularization_loss()
        if regularization_loss is None:
            losses = list()
        else:
            losses = [regularization_loss]

        if self.l2_regularization > 0.0:
            losses.append(self.l2_regularization * tf.nn.l2_loss(t=self.filters))
            if self.bias is not None:
                losses.append(self.l2_regularization * tf.nn.l2_loss(t=self.bias))

        if self.l1_regularization > 0.0:
            losses.append(self.l1_regularization * tf.reduce_sum(input_tensor=tf.abs(x=self.filters)))
            if self.bias is not None:
                losses.append(self.l1_regularization * tf.reduce_sum(input_tensor=tf.abs(x=self.bias)))

        regularization_loss = self.nonlinearity.regularization_loss()
        if regularization_loss is not None:
            losses.append(regularization_loss)

        if len(losses) > 0:
            return tf.add_n(inputs=losses)
        else:
            return None

    def get_variables(self, include_nontrainable=False):
        layer_variables = super(Conv1d, self).get_variables(include_nontrainable=include_nontrainable)
        nonlinearity_variables = self.nonlinearity.get_variables(include_nontrainable=include_nontrainable)

        return layer_variables + nonlinearity_variables


class Conv2d(Layer):
    


    def __init__(
        self,
        size,
        window=3,
        stride=1,
        padding='SAME',
        bias=True,
        activation='relu',
        l2_regularization=0.0,
        l1_regularization=0.0,
        named_tensors=None,
        scope='conv2d',
        summary_labels=()
    ):
        

        self.size = size
        if isinstance(window, int):
            self.window = (window, window)
        elif len(window) == 2:
            self.window = tuple(window)
        else:
            raise TensorForceError('Invalid window {} for conv2d layer, must be of size 2'.format(window))
        self.stride = stride
        self.padding = padding
        self.bias = bias
        self.l2_regularization = l2_regularization
        self.l1_regularization = l1_regularization
        self.nonlinearity = Nonlinearity(summary_labels=summary_labels, **util.prepare_kwargs(activation))
        super(Conv2d, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update):
        if util.rank(x) != 4:
            raise TensorForceError('Invalid input rank for conv2d layer: {}, must be 4'.format(util.rank(x)))

        filters_shape = self.window + (x.shape[3].value, self.size)
        stddev = min(0.1, sqrt(2.0 / self.size))
        filters_init = tf.random_normal_initializer(mean=0.0, stddev=stddev, dtype=tf.float32)
        self.filters = tf.get_variable(name='W', shape=filters_shape, dtype=tf.float32, initializer=filters_init)
        stride_h, stride_w = self.stride if type(self.stride) is tuple else (self.stride, self.stride)
        x = tf.nn.conv2d(input=x, filter=self.filters, strides=(1, stride_h, stride_w, 1), padding=self.padding)

        if self.bias:
            bias_shape = (self.size,)
            bias_init = tf.zeros_initializer(dtype=tf.float32)
            self.bias = tf.get_variable(name='b', shape=bias_shape, dtype=tf.float32, initializer=bias_init)
            x = tf.nn.bias_add(value=x, bias=self.bias)

        x = self.nonlinearity.apply(x=x, update=update)

        if 'activations' in self.summary_labels:
            tf.contrib.summary.histogram(name='activations', tensor=x)

        return x

    def tf_regularization_loss(self):
        regularization_loss = super(Conv2d, self).tf_regularization_loss()
        if regularization_loss is None:
            losses = list()
        else:
            losses = [regularization_loss]

        if self.l2_regularization > 0.0:
            losses.append(self.l2_regularization * tf.nn.l2_loss(t=self.filters))
            if self.bias is not None:
                losses.append(self.l2_regularization * tf.nn.l2_loss(t=self.bias))

        if self.l1_regularization > 0.0:
            losses.append(self.l1_regularization * tf.reduce_sum(input_tensor=tf.abs(x=self.filters)))
            if self.bias is not None:
                losses.append(self.l1_regularization * tf.reduce_sum(input_tensor=tf.abs(x=self.bias)))

        regularization_loss = self.nonlinearity.regularization_loss()
        if regularization_loss is not None:
            losses.append(regularization_loss)

        if len(losses) > 0:
            return tf.add_n(inputs=losses)
        else:
            return None

    def get_variables(self, include_nontrainable=False):
        layer_variables = super(Conv2d, self).get_variables(include_nontrainable=include_nontrainable)
        nonlinearity_variables = self.nonlinearity.get_variables(include_nontrainable=include_nontrainable)

        return layer_variables + nonlinearity_variables


class InternalLstm(Layer):
    


    def __init__(self, size, dropout=None, lstmcell_args={}, named_tensors=None, scope='internal_lstm', summary_labels=()):
        

        self.size = size
        self.dropout = dropout
        self.lstmcell_args = lstmcell_args
        super(InternalLstm, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update, state):
        if util.rank(x) != 2:
            raise TensorForceError(
                'Invalid input rank for internal lstm layer: {}, must be 2.'.format(util.rank(x))
            )

        state = tf.contrib.rnn.LSTMStateTuple(c=state[:, 0, :], h=state[:, 1, :])

        self.lstm_cell = tf.contrib.rnn.LSTMCell(num_units=self.size, **self.lstmcell_args)

        if self.dropout is not None:
            keep_prob = tf.cond(pred=update, true_fn=(lambda: 1.0 - self.dropout), false_fn=(lambda: 1.0))
            self.lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=self.lstm_cell, output_keep_prob=keep_prob)

        x, state = self.lstm_cell(inputs=x, state=state)

        state = tf.stack(values=(state.c, state.h), axis=1)

        if 'activations' in self.summary_labels:
            tf.contrib.summary.histogram(name='activations', tensor=x)

        return x, dict(state=state)

    def internals_spec(self):
        return dict(state=dict(
            type='float',
            shape=(2, self.size),
            initialization='zeros'
        ))


class Lstm(Layer):

    def __init__(self, size, dropout=None, named_tensors=None, scope='lstm', summary_labels=(), return_final_state=True):
        

        self.size = size
        self.dropout = dropout
        self.return_final_state = return_final_state
        super(Lstm, self).__init__(named_tensors=named_tensors, scope=scope, summary_labels=summary_labels)

    def tf_apply(self, x, update, sequence_length=None):
        if util.rank(x) != 3:
            raise TensorForceError('Invalid input rank for lstm layer: {}, must be 3.'.format(util.rank(x)))

        lstm_cell = tf.contrib.rnn.LSTMCell(num_units=self.size)
        if 'activations' in self.summary_labels:
            tf.contrib.summary.histogram(name='activations', tensor=x)

        x, state = tf.nn.dynamic_rnn(
            cell=lstm_cell,
            inputs=x,
            sequence_length=sequence_length,
            dtype=tf.float32
        )

        
        if self.return_final_state:
            return tf.concat(values=(state.c, state.h), axis=1)
        else:
            return x
","


import asyncio
import qtm


def on_packet(packet):
    

    print(""Framenumber: {}"".format(packet.framenumber))
    header, markers = packet.get_3d_markers()
    print(""Component info: {}"".format(header))
    for marker in markers:
        print(""\t"", marker)


async def setup():
    

    connection = await qtm.connect(""127.0.0.1"")
    if connection is None:
        return

    await connection.stream_frames(components=[""3d""], on_packet=on_packet)


if __name__ == ""__main__"":
    asyncio.ensure_future(setup())
    asyncio.get_event_loop().run_forever()
","
















import threading

import weetest


def Arithmetic(n):
  return n * 3 + 2


def Fib(n):
  if n < 2:
    return 1
  return Fib(n - 1) + Fib(n - 2)


_WORKLOADS = [
    (Arithmetic, 1001),
    (Fib, 10),
]


def _MakeParallelBenchmark(p, work_func, *args):
  

  def Benchmark(b):  
    e = threading.Event()
    def Target():
      e.wait()
      for _ in xrange(b.N / p):
        work_func(*args)
    threads = []
    for _ in xrange(p):
      t = threading.Thread(target=Target)
      t.start()
      threads.append(t)
    b.ResetTimer()
    e.set()
    for t in threads:
      t.join()
  return Benchmark


def _RegisterBenchmarks():
  for p in xrange(1, 13):
    for work_func, arg in _WORKLOADS:
      name = 'Benchmark' + work_func.__name__
      if p > 1:
        name += 'Parallel%s' % p
      globals()[name] = _MakeParallelBenchmark(p, work_func, arg)
_RegisterBenchmarks()


if __name__ == '__main__':
  weetest.RunBenchmarks()
","








from openstax_accounts.interfaces import IOpenstaxAccountsAuthenticationPolicy
from pyramid import security
from pyramid.authorization import ACLAuthorizationPolicy
from pyramid.interfaces import IAuthenticationPolicy
from pyramid_multiauth import MultiAuthenticationPolicy
from zope.interface import implementer

from cnxpublishing.db import db_connect
from cnxpublishing.cache import cache_manager


ALL_KEY_INFO_SQL_STMT = ""SELECT id, key, name, groups FROM api_keys""


@cache_manager.cache(expire=60 * 60 * 24)  
def lookup_api_key_info():
    

    info = {}
    with db_connect() as conn:
        with conn.cursor() as cursor:
            cursor.execute(ALL_KEY_INFO_SQL_STMT)
            for row in cursor.fetchall():
                id, key, name, groups = row
                user_id = ""api_key:{}"".format(id)
                info[key] = dict(id=id, user_id=user_id,
                                 name=name, groups=groups)
    return info


@implementer(IAuthenticationPolicy)
class APIKeyAuthenticationPolicy(object):
    


    @property
    def user_info_by_key(self):
        return lookup_api_key_info()

    def _discover_requesting_party(self, request):
        

        user_id = None
        api_key = request.headers.get('x-api-key', None)
        try:
            principal_info = self.user_info_by_key[api_key]
        except KeyError:
            principal_info = None
        if principal_info is not None:
            user_id = principal_info['user_id']
        return api_key, user_id, principal_info

    def authenticated_userid(self, request):
        api_key, user_id, _ = self._discover_requesting_party(request)
        return user_id

    
    
    unauthenticated_userid = authenticated_userid

    def effective_principals(self, request):
        

        api_key, user_id, info = self._discover_requesting_party(request)
        if api_key is None or user_id is None:
            return []
        try:
            principals = list(info['groups'])
        except TypeError:
            principals = []
        principals.append(security.Everyone)
        principals.append(security.Authenticated)
        return principals

    def remember(self, request, principal, **kw):
        return []  

    def forget(self, request):
        return []  


def includeme(config):
    

    api_key_authn_policy = APIKeyAuthenticationPolicy()
    config.include('openstax_accounts')
    openstax_authn_policy = config.registry.getUtility(
        IOpenstaxAccountsAuthenticationPolicy)

    
    policies = [api_key_authn_policy, openstax_authn_policy]
    authn_policy = MultiAuthenticationPolicy(policies)
    config.set_authentication_policy(authn_policy)

    
    authz_policy = ACLAuthorizationPolicy()
    config.set_authorization_policy(authz_policy)


__all__ = (
    'APIKeyAuthenticationPolicy',
    'lookup_api_key_info',
)
"
